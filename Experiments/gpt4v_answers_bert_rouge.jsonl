{"id": 0, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by a colored bar in the chart that indicates a higher training speed than the pink bar but lower than the blue bar for an input length of 1024, what is the main advantage of this method over traditional absolute sinusoidal positional embeddings in terms of generalization?\nText Context: \nTransformers have been shown to be highly effective on problems involving sequential modeling, such as machine translation (MT)~\\cite{vaswani2017attention} and natural language processing (NLP)~\\cite{devlin2019bert,roller2020recipes,brown2020language}. Following its success on these tasks, the Transformer architecture raised immediate interest in other domains: automatic speech recognition (ASR)~\\cite{dong2018speech,gulati2020conformer}, music generation~\\cite{huang2018music}, object detection~\\cite{carion2020end}, and finally image recognition~\\cite{dosovitskiy2020image,touvron2020training} and video understanding~\\cite{bertasius2021space}.  Two major components of the Transformer are the attention mechanism~\\cite{bahdanau2014neural,vaswani2017attention} and the positional encoding~\\cite{vaswani2017attention,shaw2018self,huang2018music,dai2019transformer}. Without the latter, vanilla attention Transformers are invariant with respect to input tokens permutations (making ``cat eats fish'' and ``fish eats cat'' identical to the model). In the original Transformer publication, sinusoidal positional encoding was introduced~\\cite{vaswani2017attention}. Token positions were encoded in an absolute manner, which was sufficient to achieve state-of-the-art performance in numerous tasks. However, performance issues were later observed when dealing with sequences of length not seen at training time~\\cite{huang2018music,dai2019transformer,zhou2019improving,mohamed2019transformers,rosendahl2019analysis,dufter2021position}. For most applications relative positions between tokens are more relevant than absolute ones. A number of approaches were thus proposed to encode relative positions in an explicit form~\\cite{shaw2018self,dai2019transformer,huang2018music,rosendahl2019analysis,huang2020improve}, leading to improvements in modeling long sequences. However, all these approaches focus on modifying the attention mechanism and suffer from additional computational and memory costs~\\cite{zhang2020pushing}. Relative positional encoding is also notably hard to implement efficiently for multidimensional case, and recent advances in Transformer models for computer vision~\\cite{carion2020end,dosovitskiy2020image,touvron2020training} still rely on learnable absolute positional encoding.  Instead of changing the Transformer attention mechanism, we propose to improve absolute sinusoidal positional encodings in two ways: a) instead of discrete positions, rely on continuous ones, which better match the continuous nature of image, sound or video data; b) preserve some information about relative token positions, via a specific augmentation approach for positional embeddings during training. We empirically evaluate our approach, dubbed continuous augmented positional embedding (CAPE), with recent state-of-the-art models in several application domains. We study generalization properties and introduce unique features unlocked by CAPE. The main contributions of this work are: \\begin{itemize} \\item new augmented continuous positional embedding (CAPE), which encodes some relative position information in a computationally efficient way, and improves generalization performance compared to other positional embeddings across a variety of domains: machine translation, image and speech recognition; \\item a single vision Transformer (UniViT) trained with CAPE on the mix of different resolutions: it outperforms each single-resolution baseline, generalizes better to unseen resolutions and can naturally process images of any size; \\item new CAPE-based adaptive training scheme for ASR that eliminates need for padding. \\end{itemize}  ", "image_folder": 0, "answer": "The orange bar represents CAPE, which encodes some relative position information and improves generalization performance compared to absolute sinusoidal positional embeddings, without additional computational costs.", "bert_scores": [0.8705054521560669, 0.8865202069282532, 0.8784397840499878], "rouge_scores": [{"rouge-1": {"r": 0.35, "p": 0.28, "f": 0.31111110617283955}, "rouge-2": {"r": 0.21052631578947367, "p": 0.16666666666666666, "f": 0.18604650669551123}, "rouge-l": {"r": 0.3, "p": 0.24, "f": 0.2666666617283951}}]}
{"id": 1, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by the color bar that indicates a higher training speed than pink but lower than blue at an input length of 1024, what are the three key ingredients identified in the ablation study that contribute to its success when trained without any positional embeddings (nopos)?\nText Context: \n As demonstrated above in Figure~\\ref{fig:speech:tl}, \\textit{nopos} performs similar to different positional embeddings on both public and RV data while having reliable generalization to the long audio fragments. We figured out that the key components of this phenomenon and \\textit{nopos} success are i) enough training data; ii) sufficient model capacity and iii) CTC loss.  For the first point we saw that \\textit{nopos} model trained on WSJ, a 5x smaller dataset than TL, performs poorly having 45-50\\% WER even on in-domain data. For the second point we perform an additional ablation on WSJ data by decreasing dropout and layer drop in each Transformer layer from 0.4 to 0.1: with increased model capacity \\textit{nopos} reduces the WER by 30\\% and gets closer to other positional embeddings, Figure~\\ref{fig:abl:speech:asr_nopos}. For the third point we perform another ablation by comparing with sequence-to-sequence (seq2seq) training: we use exactly the same encoder $\\mathbf{H}^{L_e}$ (with various positional embeddings) but replace last linear layer and CTC loss with the decoder, encoder-decoder attention, and cross-entropy loss where the probability distribution of the transcription is factorized as \\begin{equation*} p(y_1, ..., y_n) = \\prod_{i=1}^n p(y_i \\ | \\ y_0, ..., y_{i-1}, \\mathbf{H}^{L_e}) \\end{equation*} where $y_0$ is a special symbol indicating the beginning of the transcription. The decoder is a stack of 6 Transformer layers with encoding dimension 256, learnable relative positional embedding with 9.6s left-only context and 4 attention heads. Dropout and layer drop in the decoder layers are set to 0.2.  \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_nopos.pdf} \\caption{ Word error rate comparison for models trained on WSJ data with different positional embeddings. Baseline models, \\textit{nopos} and CAPE, use 0.4 dropout and 0.4 layer drop in every Transformer layer, while \\textit{nopos, dropout=0.1} uses 0.1 for both values.\\label{fig:abl:speech:asr_nopos}} \\end{figure}   \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/tl_s2s.pdf} \\caption{ Word error comparison for CTC and seq2seq models trained on TED-LIUM v3 data without any positional embedding in the encoder (\\textit{nopos}) or with learnable relative positional embedding in every encoder-Transformer layer (\\textit{relpos}).\\label{fig:abl:speech:asr_s2s}} \\end{figure}  In Figure~\\ref{fig:abl:speech:asr_s2s} we show comparison between CTC and seq2seq models trained on TL with either \\textit{nopos} or \\textit{relpos} in the encoder.\\footnote{Encoder remains the same for both CTC and seq2seq models; for seq2seq models decoder is also identical.} Seq2seq \\textit{nopos} performs significantly worse than seq2seq \\textit{relpos} and moreover has higher WER variation on validation data. This result is opposite to the \\textit{nopos} CTC-based training, suggesting that CTC loss is able to train with enough data or model capacity and no positions provided.  These observations only partially overlap with known results: for seq2seq training it was shown recently that relative positions can be modeled via a deep stack of convolutional layers in the encoder~\\cite{mohamed2019transformers} or via convolutions inserted directly in each encoder's Transformer layer~\\cite{zhang2020pushing}. In contrast to the listed works, our encoder has vanilla Transformer layers and only one convolutional layer at the beginning. Thus, \\textit{nopos} model has very limited context to model relative positions, which affects seq2seq training (it has to ``locate'' corresponding timepoint in audio with attention) more than CTC-based, which uses explicit time ordering. In line with this interpretation, for the hybrid ASR systems dropping positional information does not drive to significant deterioration of quality~\\cite{wang2020transformer}.   ", "image_folder": 1, "answer": "The method represented by the orange bar, \\textit{nopos}, achieved success due to three key ingredients: 1) sufficient training data, 2) enough model capacity, and 3) use of CTC loss.", "bert_scores": [0.8312262296676636, 0.938875675201416, 0.8817775845527649], "rouge_scores": [{"rouge-1": {"r": 0.8888888888888888, "p": 0.27586206896551724, "f": 0.42105262796398896}, "rouge-2": {"r": 0.5, "p": 0.14285714285714285, "f": 0.22222221876543213}, "rouge-l": {"r": 0.7777777777777778, "p": 0.2413793103448276, "f": 0.3684210490166206}}]}
{"id": 2, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the positional encoding method represented by the color bar in the bar chart that has a higher training speed than the pink bar but lower than the blue bar at an input length of 1024, what are the two main factors that contribute to the choice of the frequency base in the exponentiation for this particular method's positional encoding in sound?\nText Context: \n Originally absolute positional $K$-dimensional embeddings were introduced in~\\cite{vaswani2017attention} as \\begin{equation} E_{2k}(n)     = \\cos{\\omega_k n} \\qquad E_{2k + 1}(n) = \\sin{\\omega_k n} \\qquad \\omega_k = 10000^{-2k/K} \\qquad n\\in\\mathbb{Z}^+ \\label{eq:vashwani} \\end{equation}  with $k=1,2,\\dots, K/2$ enumerating components for a token at position $n$. For simplicity of analysis, we rewrite Eq.~(\\ref{eq:vashwani}) as a complex-valued embeddings\\footnote{Work~\\cite{Wang2020Encoding} introduces general complex-valued embeddings as continuous word functions over a position.} with half the number of components: $$ \\{\\mathbf{E}(n)\\}_k = E_{k}(n) = e^{i \\omega_k n} $$  This definition can be rewritten in a recursive manner, by introducing a unitary operator $S$: \\begin{equation} \\label{eq-unitary-operator} \\mathbf{E}(n+1) = S\\,\\mathbf{E}(n)\\,, \\qquad \\textrm{with} \\qquad \\{S\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k} \\end{equation}  Therefore, the embedding at position $n$ contains sufficient information to compute the embedding of the next or previous positions, as applying $S^m$ ($m \\in \\mathbb{Z}$) performs a relative shift: $\\mathbf{E}(n+m) = S^m\\,\\mathbf{E}(n)$. Variation in $\\omega_i$ ensures that no positions $<10^4$ are assigned similar embeddings. Before introducing augmentations of positional embeddings, we revisit positions parametrizations for different modalities.  \\paragraph{Positional encoding for text} For natural language processing it is common to split text into words, letters, syllables and other sub-units. Original absolute sinusoidal positional embeddings enumerate these sub-units by their ordinal number $n$, a common choice that we follow.  \\paragraph{Positional encoding for images} In a framework where patch and image sizes may vary, we find that enumerating patches is not appropriate, as positional embeddings may greatly differ for different scales of an image, leading to generalization issues. In that perspective, we consider scaled coordinates $x$ and $y$ that span interval $[-1, +1]$.\\footnote{Authors of~\\cite{carion2020end} also perform coordinates scaling but it spans interval $[0, 1]$.} While previous works~\\cite{dosovitskiy2020image,touvron2020training} relied on learnable absolute positional embedding, we introduce the following $K$-dimensional absolute sinusoidal positional embedding defined for each position $(x, y) \\in \\mathbb{R}^2$: \\begin{equation}\\label{eq:sin2d1} E_{2k}(x, y)   = \\cos \\pi (w_{k,x} x + w_{k,y} y) \\qquad E_{2k+1}(x, y) = \\sin \\pi (w_{k,x} x + w_{k,y} y) \\end{equation} \\begin{equation}\\label{eq:sin2d2} w_{k,x} = 10^{2k/K} \\cos{k} \\qquad w_{k,y} = 10^{2k/K} \\sin{k} \\end{equation} Following Eq.~(\\ref{eq-unitary-operator}), this corresponds to introducing two commuting unitary operators $S_x$ and $S_y$, for each unit shift in either direction on the plane. The choice of $w_{k,x}$ and $w_{k,y}$ is kept simple and deterministic, while giving no preference to any direction on the plane and providing angle of ``hatching'' (all components have different angle and angles uniformly cover possible directions), see Figure~\\ref{fig:emb_visualization}. Furthermore, embeddings defined by Eq.~(\\ref{eq:sin2d1}-\\ref{eq:sin2d2}) allow attending to a specific small region around a point on the image without emphasizing points with only same $x$ or only same~$y$ while we expect this artifact to contribute in case of concatenation of 1D embeddings for each axis~\\cite{chu2021conditional,dosovitskiy2020image,bello2019attention,parmar2018image}.  \\paragraph{Positional encoding for sound} We propose to tie positional embeddings to timestamps in seconds. The embedding for a frame centered at $t$ seconds is given by: $$ E_{2k}(t) = \\cos{\\omega_k t} \\qquad E_{2k + 1}(t) = \\sin{\\omega_k t} \\qquad \\omega_k = 30 \\times 10000^{-2k/K} $$ The choice of $\\omega_k$ corresponds to the scaled version of Eq.~(\\ref{eq:vashwani}) and ensures that queries with 30ms specificity are possible even with minutes-long audio fragments.  Two main factors contribute to the choice of base in the exponentiation of $w_k$ and $w_{k,x}$, $w_{k,y}$ frequencies: i) how precise the location should be for encoding (e.g. there is no need to encode position in audio much more precisely than one syllable takes); ii) longest reasonable ``length'' that is expected in the data. CAPE could be parametrized by precision and ``length'' parameters, but we believe that for all practical cases provided choices are reasonable.   ", "image_folder": 2, "answer": "The two main factors for the choice of frequency base in exponentiation for sound positional encoding are precision in location encoding and the longest expected data length.", "bert_scores": [0.8828139901161194, 0.8905731439590454, 0.886676549911499], "rouge_scores": [{"rouge-1": {"r": 0.47058823529411764, "p": 0.34782608695652173, "f": 0.3999999951125}, "rouge-2": {"r": 0.1111111111111111, "p": 0.07692307692307693, "f": 0.09090908607438043}, "rouge-l": {"r": 0.35294117647058826, "p": 0.2608695652173913, "f": 0.29999999511250003}}]}
{"id": 3, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Referring to the bar chart that depicts training speeds at various input lengths, for the method indicated by the color bar that surpasses the pink bar in training speed yet falls behind the blue bar for an input length of 1024, what is the main computational cost associated with its relative positional embeddings in machine translation models?\nText Context: \n For machine translation experiments we estimate the forward and backward time slowdown when learnable relative positional embedding is used in Transformer layer compared to vanilla Transformer layer. Besides we evaluate slowdown for entire encoder and decoder. Forward and backward time benchmarking is done in Fairseq (PyTorch) for different input sequences length (10, 100, 1000), for different context size of relative positional embedding (100 and 1000 tokens to left/right) in full-precision (fp32) and half-precision (fp16) floating-point computations. We use Volta V-100 32GB GPU and report slowdown for average time of forward and backward passes measured across 100 updates with 10 updates of warmup, see Table~\\ref{tab:speed_relpos}. The batch size is set to 50, attention heads are set to 8 and embedding dimension is 768 (typical use case), there are 6 Transformer layers in encoder and decoder. Results in Table~\\ref{tab:speed_relpos} demonstrate that relative positional embedding indeed has additional computational cost which is even more pronounced with mixed-precision computations.   \\end{document}    ", "image_folder": 3, "answer": "The orange bar represents a method using learnable relative positional embedding. Its computational cost is the additional slowdown compared to the vanilla Transformer, especially pronounced during mixed-precision computations.", "bert_scores": [0.8930950164794922, 0.9521903395652771, 0.9216963648796082], "rouge_scores": [{"rouge-1": {"r": 0.6666666666666666, "p": 0.2962962962962963, "f": 0.41025640599605523}, "rouge-2": {"r": 0.2727272727272727, "p": 0.1111111111111111, "f": 0.15789473272853197}, "rouge-l": {"r": 0.5833333333333334, "p": 0.25925925925925924, "f": 0.358974354714004}}]}
{"id": 4, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by the color bar that has a higher training speed than the pink bar but lower than the blue bar for an input length of 1024, how does its CAPE mechanism handle positional embeddings during training and inference?\nText Context: \n Absolute sinusoidal positional embeddings lack regularization, leading to ``in-domain'' generalization issues as the model may start learning spurious correlations. For applications where inference input sizes may be significantly different than the training ones, ``out-of-domain'' issues may also arise, as positions rarely observed at training may lead to improper inference predictions. To ensure that the model does not learn spurious correlations between content and position, we introduce the following augmentations for absolute sinusoidal positional embeddings at training time, see Figure~\\ref{fig:cape}.   \\begin{figure}[t!] \\centering \\includegraphics[width=0.75\\textwidth]{figures/cape_scheme.png} \\caption{Example of CAPE's transformations for an image: patches positions are scaled to $[-1, 1]\\times [-1, 1]$; random global shift, local shift, and global scaling are then applied to the grid of positions. \\label{fig:cape} } \\end{figure}  \\paragraph{Global shift} Transform every embedding in a sequence using $S^\\Delta$ operator with a global random shift from uniform zero-mean distribution $\\Delta \\sim \\mathcal{U}(-\\Delta_{max}, \\Delta_{max})$: $$ \\mathbf{E}'(n) = S^\\Delta \\mathbf{E}(n) \\qquad \\{S^\\Delta\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k \\Delta } \\qquad \\Delta \\in \\mathbb{R} $$ This modification hides the absolute positional information, but relative relations, see Eq.~(\\ref{eq-unitary-operator}), between embeddings still hold. This transformation can be rewritten as augmenting positions by a random shift before encoding with $\\sin$ and $\\cos$: \\begin{equation} n'_i \\leftarrow n_i + \\Delta \\qquad \\quad x'_i \\leftarrow x_i + \\Delta_x, \\;\\; y'_i \\leftarrow y_i + \\Delta_y \\qquad \\qquad t'_i \\leftarrow t_i + \\Delta \\label{cape-global-shift} \\end{equation}  \\paragraph{Local shift} To further increase augmentation and prevent capturing spontaneous correlations, we additionally introduce local shifts from uniform zero-mean distribution $\\epsilon_i \\sim \\mathcal{U}(-\\epsilon_{max}, \\epsilon_{max})$ \\begin{equation} n'_i \\leftarrow n_i + \\epsilon_i        \\qquad \\quad x'_i \\leftarrow x_i + \\epsilon_{x,i},   \\;\\; y'_i \\leftarrow y_i + \\epsilon_{y,i}    \\qquad \\qquad t'_i \\leftarrow t_i + \\epsilon_i \\label{cape-local-shift} \\end{equation} \\paragraph{Global scaling} To prevent distances memorization, we also introduce random global scale $\\lambda$ from $\\log \\lambda \\sim \\mathcal{U}(- \\log \\lambda_{max}, \\, \\log \\lambda_{max})$ \\begin{equation} n'_i \\leftarrow \\lambda n_i     \\qquad \\qquad \\quad x'_i \\leftarrow \\lambda x_i,    \\;\\;          \\quad y'_i \\leftarrow \\lambda y_i     \\qquad \\qquad \\quad \\quad t'_i \\leftarrow \\lambda t_i                   \\quad \\label{cape-scale} \\end{equation}  At training time, computing our continuous augmented positional embedding is performed through four steps: i)~mean-normalization of positions (extract mean of sequence positions), ii)~global shift Eq.~(\\ref{cape-global-shift}), iii)~local shift Eq.~(\\ref{cape-local-shift}), and iv)~global scaling Eq.~(\\ref{cape-scale}). At inference time, only mean-normalization of positions is performed. The reference implementation can be found in Appendix~\\ref{app:impl}.  ", "image_folder": 4, "answer": "The CAPE mechanism, represented by the orange bar, handles positional embeddings by applying mean-normalization, global shift, local shift, and global scaling during training. During inference, it only performs mean-normalization of positions.", "bert_scores": [0.9243866205215454, 0.9522697925567627, 0.9381210803985596], "rouge_scores": [{"rouge-1": {"r": 0.631578947368421, "p": 0.42857142857142855, "f": 0.5106382930556813}, "rouge-2": {"r": 0.391304347826087, "p": 0.3, "f": 0.33962263659665365}, "rouge-l": {"r": 0.5789473684210527, "p": 0.39285714285714285, "f": 0.46808510156631966}}]}
{"id": 5, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that maintains a consistently low perplexity across all inference input token lengths as shown in the graph of models trained on 512 tokens, what is its main advantage over traditional absolute sinusoidal positional embeddings in terms of generalization?\nText Context: \nTransformers have been shown to be highly effective on problems involving sequential modeling, such as machine translation (MT)~\\cite{vaswani2017attention} and natural language processing (NLP)~\\cite{devlin2019bert,roller2020recipes,brown2020language}. Following its success on these tasks, the Transformer architecture raised immediate interest in other domains: automatic speech recognition (ASR)~\\cite{dong2018speech,gulati2020conformer}, music generation~\\cite{huang2018music}, object detection~\\cite{carion2020end}, and finally image recognition~\\cite{dosovitskiy2020image,touvron2020training} and video understanding~\\cite{bertasius2021space}.  Two major components of the Transformer are the attention mechanism~\\cite{bahdanau2014neural,vaswani2017attention} and the positional encoding~\\cite{vaswani2017attention,shaw2018self,huang2018music,dai2019transformer}. Without the latter, vanilla attention Transformers are invariant with respect to input tokens permutations (making ``cat eats fish'' and ``fish eats cat'' identical to the model). In the original Transformer publication, sinusoidal positional encoding was introduced~\\cite{vaswani2017attention}. Token positions were encoded in an absolute manner, which was sufficient to achieve state-of-the-art performance in numerous tasks. However, performance issues were later observed when dealing with sequences of length not seen at training time~\\cite{huang2018music,dai2019transformer,zhou2019improving,mohamed2019transformers,rosendahl2019analysis,dufter2021position}. For most applications relative positions between tokens are more relevant than absolute ones. A number of approaches were thus proposed to encode relative positions in an explicit form~\\cite{shaw2018self,dai2019transformer,huang2018music,rosendahl2019analysis,huang2020improve}, leading to improvements in modeling long sequences. However, all these approaches focus on modifying the attention mechanism and suffer from additional computational and memory costs~\\cite{zhang2020pushing}. Relative positional encoding is also notably hard to implement efficiently for multidimensional case, and recent advances in Transformer models for computer vision~\\cite{carion2020end,dosovitskiy2020image,touvron2020training} still rely on learnable absolute positional encoding.  Instead of changing the Transformer attention mechanism, we propose to improve absolute sinusoidal positional encodings in two ways: a) instead of discrete positions, rely on continuous ones, which better match the continuous nature of image, sound or video data; b) preserve some information about relative token positions, via a specific augmentation approach for positional embeddings during training. We empirically evaluate our approach, dubbed continuous augmented positional embedding (CAPE), with recent state-of-the-art models in several application domains. We study generalization properties and introduce unique features unlocked by CAPE. The main contributions of this work are: \\begin{itemize} \\item new augmented continuous positional embedding (CAPE), which encodes some relative position information in a computationally efficient way, and improves generalization performance compared to other positional embeddings across a variety of domains: machine translation, image and speech recognition; \\item a single vision Transformer (UniViT) trained with CAPE on the mix of different resolutions: it outperforms each single-resolution baseline, generalizes better to unseen resolutions and can naturally process images of any size; \\item new CAPE-based adaptive training scheme for ASR that eliminates need for padding. \\end{itemize}  ", "image_folder": 5, "answer": "The method with low perplexity, likely CAPE, generalizes better to sequences of varying lengths not seen during training compared to traditional sinusoidal embeddings.", "bert_scores": [0.8551393747329712, 0.8704385757446289, 0.8627212047576904], "rouge_scores": [{"rouge-1": {"r": 0.2, "p": 0.18181818181818182, "f": 0.1904761854875285}, "rouge-2": {"r": 0.05263157894736842, "p": 0.045454545454545456, "f": 0.04878048283164834}, "rouge-l": {"r": 0.15, "p": 0.13636363636363635, "f": 0.14285713786848087}}]}
{"id": 6, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that maintains a consistently low perplexity across all inference input token lengths as shown in the graph for models trained on 512 tokens, what are the three key ingredients identified in the ablation study that contribute to the success of this model trained without any positional embeddings (nopos)?\nText Context: \n As demonstrated above in Figure~\\ref{fig:speech:tl}, \\textit{nopos} performs similar to different positional embeddings on both public and RV data while having reliable generalization to the long audio fragments. We figured out that the key components of this phenomenon and \\textit{nopos} success are i) enough training data; ii) sufficient model capacity and iii) CTC loss.  For the first point we saw that \\textit{nopos} model trained on WSJ, a 5x smaller dataset than TL, performs poorly having 45-50\\% WER even on in-domain data. For the second point we perform an additional ablation on WSJ data by decreasing dropout and layer drop in each Transformer layer from 0.4 to 0.1: with increased model capacity \\textit{nopos} reduces the WER by 30\\% and gets closer to other positional embeddings, Figure~\\ref{fig:abl:speech:asr_nopos}. For the third point we perform another ablation by comparing with sequence-to-sequence (seq2seq) training: we use exactly the same encoder $\\mathbf{H}^{L_e}$ (with various positional embeddings) but replace last linear layer and CTC loss with the decoder, encoder-decoder attention, and cross-entropy loss where the probability distribution of the transcription is factorized as \\begin{equation*} p(y_1, ..., y_n) = \\prod_{i=1}^n p(y_i \\ | \\ y_0, ..., y_{i-1}, \\mathbf{H}^{L_e}) \\end{equation*} where $y_0$ is a special symbol indicating the beginning of the transcription. The decoder is a stack of 6 Transformer layers with encoding dimension 256, learnable relative positional embedding with 9.6s left-only context and 4 attention heads. Dropout and layer drop in the decoder layers are set to 0.2.  \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_nopos.pdf} \\caption{ Word error rate comparison for models trained on WSJ data with different positional embeddings. Baseline models, \\textit{nopos} and CAPE, use 0.4 dropout and 0.4 layer drop in every Transformer layer, while \\textit{nopos, dropout=0.1} uses 0.1 for both values.\\label{fig:abl:speech:asr_nopos}} \\end{figure}   \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/tl_s2s.pdf} \\caption{ Word error comparison for CTC and seq2seq models trained on TED-LIUM v3 data without any positional embedding in the encoder (\\textit{nopos}) or with learnable relative positional embedding in every encoder-Transformer layer (\\textit{relpos}).\\label{fig:abl:speech:asr_s2s}} \\end{figure}  In Figure~\\ref{fig:abl:speech:asr_s2s} we show comparison between CTC and seq2seq models trained on TL with either \\textit{nopos} or \\textit{relpos} in the encoder.\\footnote{Encoder remains the same for both CTC and seq2seq models; for seq2seq models decoder is also identical.} Seq2seq \\textit{nopos} performs significantly worse than seq2seq \\textit{relpos} and moreover has higher WER variation on validation data. This result is opposite to the \\textit{nopos} CTC-based training, suggesting that CTC loss is able to train with enough data or model capacity and no positions provided.  These observations only partially overlap with known results: for seq2seq training it was shown recently that relative positions can be modeled via a deep stack of convolutional layers in the encoder~\\cite{mohamed2019transformers} or via convolutions inserted directly in each encoder's Transformer layer~\\cite{zhang2020pushing}. In contrast to the listed works, our encoder has vanilla Transformer layers and only one convolutional layer at the beginning. Thus, \\textit{nopos} model has very limited context to model relative positions, which affects seq2seq training (it has to ``locate'' corresponding timepoint in audio with attention) more than CTC-based, which uses explicit time ordering. In line with this interpretation, for the hybrid ASR systems dropping positional information does not drive to significant deterioration of quality~\\cite{wang2020transformer}.   ", "image_folder": 6, "answer": "The success of the \\textit{nopos} model is attributed to having sufficient training data, enough model capacity, and the use of CTC loss.", "bert_scores": [0.8727861046791077, 0.9453027248382568, 0.9075982570648193], "rouge_scores": [{"rouge-1": {"r": 0.8888888888888888, "p": 0.42105263157894735, "f": 0.5714285670663266}, "rouge-2": {"r": 0.5, "p": 0.19047619047619047, "f": 0.2758620649702735}, "rouge-l": {"r": 0.7777777777777778, "p": 0.3684210526315789, "f": 0.49999999563775516}}]}
{"id": 7, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that maintains a consistently low perplexity across all inference input token lengths when trained on 512 tokens, what are the two main factors that contribute to the choice of the frequency base in its exponentiation for positional encoding in sound?\nText Context: \n Originally absolute positional $K$-dimensional embeddings were introduced in~\\cite{vaswani2017attention} as \\begin{equation} E_{2k}(n)     = \\cos{\\omega_k n} \\qquad E_{2k + 1}(n) = \\sin{\\omega_k n} \\qquad \\omega_k = 10000^{-2k/K} \\qquad n\\in\\mathbb{Z}^+ \\label{eq:vashwani} \\end{equation}  with $k=1,2,\\dots, K/2$ enumerating components for a token at position $n$. For simplicity of analysis, we rewrite Eq.~(\\ref{eq:vashwani}) as a complex-valued embeddings\\footnote{Work~\\cite{Wang2020Encoding} introduces general complex-valued embeddings as continuous word functions over a position.} with half the number of components: $$ \\{\\mathbf{E}(n)\\}_k = E_{k}(n) = e^{i \\omega_k n} $$  This definition can be rewritten in a recursive manner, by introducing a unitary operator $S$: \\begin{equation} \\label{eq-unitary-operator} \\mathbf{E}(n+1) = S\\,\\mathbf{E}(n)\\,, \\qquad \\textrm{with} \\qquad \\{S\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k} \\end{equation}  Therefore, the embedding at position $n$ contains sufficient information to compute the embedding of the next or previous positions, as applying $S^m$ ($m \\in \\mathbb{Z}$) performs a relative shift: $\\mathbf{E}(n+m) = S^m\\,\\mathbf{E}(n)$. Variation in $\\omega_i$ ensures that no positions $<10^4$ are assigned similar embeddings. Before introducing augmentations of positional embeddings, we revisit positions parametrizations for different modalities.  \\paragraph{Positional encoding for text} For natural language processing it is common to split text into words, letters, syllables and other sub-units. Original absolute sinusoidal positional embeddings enumerate these sub-units by their ordinal number $n$, a common choice that we follow.  \\paragraph{Positional encoding for images} In a framework where patch and image sizes may vary, we find that enumerating patches is not appropriate, as positional embeddings may greatly differ for different scales of an image, leading to generalization issues. In that perspective, we consider scaled coordinates $x$ and $y$ that span interval $[-1, +1]$.\\footnote{Authors of~\\cite{carion2020end} also perform coordinates scaling but it spans interval $[0, 1]$.} While previous works~\\cite{dosovitskiy2020image,touvron2020training} relied on learnable absolute positional embedding, we introduce the following $K$-dimensional absolute sinusoidal positional embedding defined for each position $(x, y) \\in \\mathbb{R}^2$: \\begin{equation}\\label{eq:sin2d1} E_{2k}(x, y)   = \\cos \\pi (w_{k,x} x + w_{k,y} y) \\qquad E_{2k+1}(x, y) = \\sin \\pi (w_{k,x} x + w_{k,y} y) \\end{equation} \\begin{equation}\\label{eq:sin2d2} w_{k,x} = 10^{2k/K} \\cos{k} \\qquad w_{k,y} = 10^{2k/K} \\sin{k} \\end{equation} Following Eq.~(\\ref{eq-unitary-operator}), this corresponds to introducing two commuting unitary operators $S_x$ and $S_y$, for each unit shift in either direction on the plane. The choice of $w_{k,x}$ and $w_{k,y}$ is kept simple and deterministic, while giving no preference to any direction on the plane and providing angle of ``hatching'' (all components have different angle and angles uniformly cover possible directions), see Figure~\\ref{fig:emb_visualization}. Furthermore, embeddings defined by Eq.~(\\ref{eq:sin2d1}-\\ref{eq:sin2d2}) allow attending to a specific small region around a point on the image without emphasizing points with only same $x$ or only same~$y$ while we expect this artifact to contribute in case of concatenation of 1D embeddings for each axis~\\cite{chu2021conditional,dosovitskiy2020image,bello2019attention,parmar2018image}.  \\paragraph{Positional encoding for sound} We propose to tie positional embeddings to timestamps in seconds. The embedding for a frame centered at $t$ seconds is given by: $$ E_{2k}(t) = \\cos{\\omega_k t} \\qquad E_{2k + 1}(t) = \\sin{\\omega_k t} \\qquad \\omega_k = 30 \\times 10000^{-2k/K} $$ The choice of $\\omega_k$ corresponds to the scaled version of Eq.~(\\ref{eq:vashwani}) and ensures that queries with 30ms specificity are possible even with minutes-long audio fragments.  Two main factors contribute to the choice of base in the exponentiation of $w_k$ and $w_{k,x}$, $w_{k,y}$ frequencies: i) how precise the location should be for encoding (e.g. there is no need to encode position in audio much more precisely than one syllable takes); ii) longest reasonable ``length'' that is expected in the data. CAPE could be parametrized by precision and ``length'' parameters, but we believe that for all practical cases provided choices are reasonable.   ", "image_folder": 7, "answer": "The two main factors are the precision needed for encoding location in sound and the longest audio fragment length expected in the data.", "bert_scores": [0.8993363380432129, 0.9068504571914673, 0.9030777812004089], "rouge_scores": [{"rouge-1": {"r": 0.47058823529411764, "p": 0.4, "f": 0.43243242746530314}, "rouge-2": {"r": 0.2777777777777778, "p": 0.22727272727272727, "f": 0.24999999505000006}, "rouge-l": {"r": 0.4117647058823529, "p": 0.35, "f": 0.37837837341124914}}]}
{"id": 8, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Considering the method that maintains a consistently low perplexity across all inference input token lengths when models are trained on 512 tokens as shown in the graph, what is the main computational cost associated with its use of relative positional embeddings in machine translation models?\nText Context: \n For machine translation experiments we estimate the forward and backward time slowdown when learnable relative positional embedding is used in Transformer layer compared to vanilla Transformer layer. Besides we evaluate slowdown for entire encoder and decoder. Forward and backward time benchmarking is done in Fairseq (PyTorch) for different input sequences length (10, 100, 1000), for different context size of relative positional embedding (100 and 1000 tokens to left/right) in full-precision (fp32) and half-precision (fp16) floating-point computations. We use Volta V-100 32GB GPU and report slowdown for average time of forward and backward passes measured across 100 updates with 10 updates of warmup, see Table~\\ref{tab:speed_relpos}. The batch size is set to 50, attention heads are set to 8 and embedding dimension is 768 (typical use case), there are 6 Transformer layers in encoder and decoder. Results in Table~\\ref{tab:speed_relpos} demonstrate that relative positional embedding indeed has additional computational cost which is even more pronounced with mixed-precision computations.   \\end{document}    ", "image_folder": 8, "answer": "The main computational cost is the additional time required for forward and backward passes due to the complexity of learnable relative positional embedding compared to vanilla Transformer layers.", "bert_scores": [0.8694990873336792, 0.9020521640777588, 0.8854765295982361], "rouge_scores": [{"rouge-1": {"r": 0.3333333333333333, "p": 0.15384615384615385, "f": 0.21052631146814413}, "rouge-2": {"r": 0.09090909090909091, "p": 0.037037037037037035, "f": 0.05263157483379533}, "rouge-l": {"r": 0.16666666666666666, "p": 0.07692307692307693, "f": 0.10526315357340738}}]}
{"id": 9, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that maintains a consistently low perplexity across all inference input token lengths as illustrated in the graph for models trained on 512 tokens, how does its CAPE mechanism handle positional embeddings during training and inference?\nText Context: \n Absolute sinusoidal positional embeddings lack regularization, leading to ``in-domain'' generalization issues as the model may start learning spurious correlations. For applications where inference input sizes may be significantly different than the training ones, ``out-of-domain'' issues may also arise, as positions rarely observed at training may lead to improper inference predictions. To ensure that the model does not learn spurious correlations between content and position, we introduce the following augmentations for absolute sinusoidal positional embeddings at training time, see Figure~\\ref{fig:cape}.   \\begin{figure}[t!] \\centering \\includegraphics[width=0.75\\textwidth]{figures/cape_scheme.png} \\caption{Example of CAPE's transformations for an image: patches positions are scaled to $[-1, 1]\\times [-1, 1]$; random global shift, local shift, and global scaling are then applied to the grid of positions. \\label{fig:cape} } \\end{figure}  \\paragraph{Global shift} Transform every embedding in a sequence using $S^\\Delta$ operator with a global random shift from uniform zero-mean distribution $\\Delta \\sim \\mathcal{U}(-\\Delta_{max}, \\Delta_{max})$: $$ \\mathbf{E}'(n) = S^\\Delta \\mathbf{E}(n) \\qquad \\{S^\\Delta\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k \\Delta } \\qquad \\Delta \\in \\mathbb{R} $$ This modification hides the absolute positional information, but relative relations, see Eq.~(\\ref{eq-unitary-operator}), between embeddings still hold. This transformation can be rewritten as augmenting positions by a random shift before encoding with $\\sin$ and $\\cos$: \\begin{equation} n'_i \\leftarrow n_i + \\Delta \\qquad \\quad x'_i \\leftarrow x_i + \\Delta_x, \\;\\; y'_i \\leftarrow y_i + \\Delta_y \\qquad \\qquad t'_i \\leftarrow t_i + \\Delta \\label{cape-global-shift} \\end{equation}  \\paragraph{Local shift} To further increase augmentation and prevent capturing spontaneous correlations, we additionally introduce local shifts from uniform zero-mean distribution $\\epsilon_i \\sim \\mathcal{U}(-\\epsilon_{max}, \\epsilon_{max})$ \\begin{equation} n'_i \\leftarrow n_i + \\epsilon_i        \\qquad \\quad x'_i \\leftarrow x_i + \\epsilon_{x,i},   \\;\\; y'_i \\leftarrow y_i + \\epsilon_{y,i}    \\qquad \\qquad t'_i \\leftarrow t_i + \\epsilon_i \\label{cape-local-shift} \\end{equation} \\paragraph{Global scaling} To prevent distances memorization, we also introduce random global scale $\\lambda$ from $\\log \\lambda \\sim \\mathcal{U}(- \\log \\lambda_{max}, \\, \\log \\lambda_{max})$ \\begin{equation} n'_i \\leftarrow \\lambda n_i     \\qquad \\qquad \\quad x'_i \\leftarrow \\lambda x_i,    \\;\\;          \\quad y'_i \\leftarrow \\lambda y_i     \\qquad \\qquad \\quad \\quad t'_i \\leftarrow \\lambda t_i                   \\quad \\label{cape-scale} \\end{equation}  At training time, computing our continuous augmented positional embedding is performed through four steps: i)~mean-normalization of positions (extract mean of sequence positions), ii)~global shift Eq.~(\\ref{cape-global-shift}), iii)~local shift Eq.~(\\ref{cape-local-shift}), and iv)~global scaling Eq.~(\\ref{cape-scale}). At inference time, only mean-normalization of positions is performed. The reference implementation can be found in Appendix~\\ref{app:impl}.  ", "image_folder": 9, "answer": "During training, CAPE applies global shift, local shift, and global scaling to the sinusoidal positional embeddings to prevent learning spurious correlations. At inference, it only performs mean-normalization of positions.", "bert_scores": [0.9168229103088379, 0.9439867734909058, 0.9302065372467041], "rouge_scores": [{"rouge-1": {"r": 0.631578947368421, "p": 0.46153846153846156, "f": 0.533333328454321}, "rouge-2": {"r": 0.391304347826087, "p": 0.32142857142857145, "f": 0.35294117151864673}, "rouge-l": {"r": 0.631578947368421, "p": 0.46153846153846156, "f": 0.533333328454321}}]}
{"id": 10, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that maintains a consistently lower perplexity across all inference input token ranges compared to the Sinusoidal, Rotary, and T5 Bias methods as depicted in the graph of models trained on 1024 tokens, what is the main advantage of this method over traditional absolute sinusoidal positional embeddings in terms of generalization?\nText Context: \nTransformers have been shown to be highly effective on problems involving sequential modeling, such as machine translation (MT)~\\cite{vaswani2017attention} and natural language processing (NLP)~\\cite{devlin2019bert,roller2020recipes,brown2020language}. Following its success on these tasks, the Transformer architecture raised immediate interest in other domains: automatic speech recognition (ASR)~\\cite{dong2018speech,gulati2020conformer}, music generation~\\cite{huang2018music}, object detection~\\cite{carion2020end}, and finally image recognition~\\cite{dosovitskiy2020image,touvron2020training} and video understanding~\\cite{bertasius2021space}.  Two major components of the Transformer are the attention mechanism~\\cite{bahdanau2014neural,vaswani2017attention} and the positional encoding~\\cite{vaswani2017attention,shaw2018self,huang2018music,dai2019transformer}. Without the latter, vanilla attention Transformers are invariant with respect to input tokens permutations (making ``cat eats fish'' and ``fish eats cat'' identical to the model). In the original Transformer publication, sinusoidal positional encoding was introduced~\\cite{vaswani2017attention}. Token positions were encoded in an absolute manner, which was sufficient to achieve state-of-the-art performance in numerous tasks. However, performance issues were later observed when dealing with sequences of length not seen at training time~\\cite{huang2018music,dai2019transformer,zhou2019improving,mohamed2019transformers,rosendahl2019analysis,dufter2021position}. For most applications relative positions between tokens are more relevant than absolute ones. A number of approaches were thus proposed to encode relative positions in an explicit form~\\cite{shaw2018self,dai2019transformer,huang2018music,rosendahl2019analysis,huang2020improve}, leading to improvements in modeling long sequences. However, all these approaches focus on modifying the attention mechanism and suffer from additional computational and memory costs~\\cite{zhang2020pushing}. Relative positional encoding is also notably hard to implement efficiently for multidimensional case, and recent advances in Transformer models for computer vision~\\cite{carion2020end,dosovitskiy2020image,touvron2020training} still rely on learnable absolute positional encoding.  Instead of changing the Transformer attention mechanism, we propose to improve absolute sinusoidal positional encodings in two ways: a) instead of discrete positions, rely on continuous ones, which better match the continuous nature of image, sound or video data; b) preserve some information about relative token positions, via a specific augmentation approach for positional embeddings during training. We empirically evaluate our approach, dubbed continuous augmented positional embedding (CAPE), with recent state-of-the-art models in several application domains. We study generalization properties and introduce unique features unlocked by CAPE. The main contributions of this work are: \\begin{itemize} \\item new augmented continuous positional embedding (CAPE), which encodes some relative position information in a computationally efficient way, and improves generalization performance compared to other positional embeddings across a variety of domains: machine translation, image and speech recognition; \\item a single vision Transformer (UniViT) trained with CAPE on the mix of different resolutions: it outperforms each single-resolution baseline, generalizes better to unseen resolutions and can naturally process images of any size; \\item new CAPE-based adaptive training scheme for ASR that eliminates need for padding. \\end{itemize}  ", "image_folder": 10, "answer": "The ALiBi method exhibits better generalization performance compared to traditional absolute sinusoidal embeddings due to its lower perplexity across various input token ranges.", "bert_scores": [0.8677980899810791, 0.8823273777961731, 0.8750025033950806], "rouge_scores": [{"rouge-1": {"r": 0.3, "p": 0.2727272727272727, "f": 0.2857142807256236}, "rouge-2": {"r": 0.15789473684210525, "p": 0.13636363636363635, "f": 0.1463414584414041}, "rouge-l": {"r": 0.3, "p": 0.2727272727272727, "f": 0.2857142807256236}}]}
{"id": 11, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Regarding the method depicted in the graph that maintains a consistently lower perplexity across all inference input token ranges in comparison to the Sinusoidal, Rotary, and T5 Bias methods when trained on 1024 tokens, what are the three key ingredients identified in the ablation study that contribute to the success of this particular model trained without any positional embeddings (nopos)?\nText Context: \n As demonstrated above in Figure~\\ref{fig:speech:tl}, \\textit{nopos} performs similar to different positional embeddings on both public and RV data while having reliable generalization to the long audio fragments. We figured out that the key components of this phenomenon and \\textit{nopos} success are i) enough training data; ii) sufficient model capacity and iii) CTC loss.  For the first point we saw that \\textit{nopos} model trained on WSJ, a 5x smaller dataset than TL, performs poorly having 45-50\\% WER even on in-domain data. For the second point we perform an additional ablation on WSJ data by decreasing dropout and layer drop in each Transformer layer from 0.4 to 0.1: with increased model capacity \\textit{nopos} reduces the WER by 30\\% and gets closer to other positional embeddings, Figure~\\ref{fig:abl:speech:asr_nopos}. For the third point we perform another ablation by comparing with sequence-to-sequence (seq2seq) training: we use exactly the same encoder $\\mathbf{H}^{L_e}$ (with various positional embeddings) but replace last linear layer and CTC loss with the decoder, encoder-decoder attention, and cross-entropy loss where the probability distribution of the transcription is factorized as \\begin{equation*} p(y_1, ..., y_n) = \\prod_{i=1}^n p(y_i \\ | \\ y_0, ..., y_{i-1}, \\mathbf{H}^{L_e}) \\end{equation*} where $y_0$ is a special symbol indicating the beginning of the transcription. The decoder is a stack of 6 Transformer layers with encoding dimension 256, learnable relative positional embedding with 9.6s left-only context and 4 attention heads. Dropout and layer drop in the decoder layers are set to 0.2.  \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_nopos.pdf} \\caption{ Word error rate comparison for models trained on WSJ data with different positional embeddings. Baseline models, \\textit{nopos} and CAPE, use 0.4 dropout and 0.4 layer drop in every Transformer layer, while \\textit{nopos, dropout=0.1} uses 0.1 for both values.\\label{fig:abl:speech:asr_nopos}} \\end{figure}   \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/tl_s2s.pdf} \\caption{ Word error comparison for CTC and seq2seq models trained on TED-LIUM v3 data without any positional embedding in the encoder (\\textit{nopos}) or with learnable relative positional embedding in every encoder-Transformer layer (\\textit{relpos}).\\label{fig:abl:speech:asr_s2s}} \\end{figure}  In Figure~\\ref{fig:abl:speech:asr_s2s} we show comparison between CTC and seq2seq models trained on TL with either \\textit{nopos} or \\textit{relpos} in the encoder.\\footnote{Encoder remains the same for both CTC and seq2seq models; for seq2seq models decoder is also identical.} Seq2seq \\textit{nopos} performs significantly worse than seq2seq \\textit{relpos} and moreover has higher WER variation on validation data. This result is opposite to the \\textit{nopos} CTC-based training, suggesting that CTC loss is able to train with enough data or model capacity and no positions provided.  These observations only partially overlap with known results: for seq2seq training it was shown recently that relative positions can be modeled via a deep stack of convolutional layers in the encoder~\\cite{mohamed2019transformers} or via convolutions inserted directly in each encoder's Transformer layer~\\cite{zhang2020pushing}. In contrast to the listed works, our encoder has vanilla Transformer layers and only one convolutional layer at the beginning. Thus, \\textit{nopos} model has very limited context to model relative positions, which affects seq2seq training (it has to ``locate'' corresponding timepoint in audio with attention) more than CTC-based, which uses explicit time ordering. In line with this interpretation, for the hybrid ASR systems dropping positional information does not drive to significant deterioration of quality~\\cite{wang2020transformer}.   ", "image_folder": 11, "answer": "The three key ingredients contributing to \\textit{nopos} success are: enough training data, sufficient model capacity, and CTC loss.", "bert_scores": [0.8607304096221924, 0.9547143578529358, 0.9052896499633789], "rouge_scores": [{"rouge-1": {"r": 0.8888888888888888, "p": 0.4444444444444444, "f": 0.5925925881481482}, "rouge-2": {"r": 0.875, "p": 0.4117647058823529, "f": 0.5599999956480001}, "rouge-l": {"r": 0.8888888888888888, "p": 0.4444444444444444, "f": 0.5925925881481482}}]}
{"id": 12, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the positional encoding method that maintains a consistently lower perplexity across all inference input token ranges when compared to the Sinusoidal, Rotary, and T5 Bias methods, as depicted in the graph for models trained on 1024 tokens, what are the two main factors that contribute to the choice of the frequency base in its exponentiation for positional encoding in sound?\nText Context: \n Originally absolute positional $K$-dimensional embeddings were introduced in~\\cite{vaswani2017attention} as \\begin{equation} E_{2k}(n)     = \\cos{\\omega_k n} \\qquad E_{2k + 1}(n) = \\sin{\\omega_k n} \\qquad \\omega_k = 10000^{-2k/K} \\qquad n\\in\\mathbb{Z}^+ \\label{eq:vashwani} \\end{equation}  with $k=1,2,\\dots, K/2$ enumerating components for a token at position $n$. For simplicity of analysis, we rewrite Eq.~(\\ref{eq:vashwani}) as a complex-valued embeddings\\footnote{Work~\\cite{Wang2020Encoding} introduces general complex-valued embeddings as continuous word functions over a position.} with half the number of components: $$ \\{\\mathbf{E}(n)\\}_k = E_{k}(n) = e^{i \\omega_k n} $$  This definition can be rewritten in a recursive manner, by introducing a unitary operator $S$: \\begin{equation} \\label{eq-unitary-operator} \\mathbf{E}(n+1) = S\\,\\mathbf{E}(n)\\,, \\qquad \\textrm{with} \\qquad \\{S\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k} \\end{equation}  Therefore, the embedding at position $n$ contains sufficient information to compute the embedding of the next or previous positions, as applying $S^m$ ($m \\in \\mathbb{Z}$) performs a relative shift: $\\mathbf{E}(n+m) = S^m\\,\\mathbf{E}(n)$. Variation in $\\omega_i$ ensures that no positions $<10^4$ are assigned similar embeddings. Before introducing augmentations of positional embeddings, we revisit positions parametrizations for different modalities.  \\paragraph{Positional encoding for text} For natural language processing it is common to split text into words, letters, syllables and other sub-units. Original absolute sinusoidal positional embeddings enumerate these sub-units by their ordinal number $n$, a common choice that we follow.  \\paragraph{Positional encoding for images} In a framework where patch and image sizes may vary, we find that enumerating patches is not appropriate, as positional embeddings may greatly differ for different scales of an image, leading to generalization issues. In that perspective, we consider scaled coordinates $x$ and $y$ that span interval $[-1, +1]$.\\footnote{Authors of~\\cite{carion2020end} also perform coordinates scaling but it spans interval $[0, 1]$.} While previous works~\\cite{dosovitskiy2020image,touvron2020training} relied on learnable absolute positional embedding, we introduce the following $K$-dimensional absolute sinusoidal positional embedding defined for each position $(x, y) \\in \\mathbb{R}^2$: \\begin{equation}\\label{eq:sin2d1} E_{2k}(x, y)   = \\cos \\pi (w_{k,x} x + w_{k,y} y) \\qquad E_{2k+1}(x, y) = \\sin \\pi (w_{k,x} x + w_{k,y} y) \\end{equation} \\begin{equation}\\label{eq:sin2d2} w_{k,x} = 10^{2k/K} \\cos{k} \\qquad w_{k,y} = 10^{2k/K} \\sin{k} \\end{equation} Following Eq.~(\\ref{eq-unitary-operator}), this corresponds to introducing two commuting unitary operators $S_x$ and $S_y$, for each unit shift in either direction on the plane. The choice of $w_{k,x}$ and $w_{k,y}$ is kept simple and deterministic, while giving no preference to any direction on the plane and providing angle of ``hatching'' (all components have different angle and angles uniformly cover possible directions), see Figure~\\ref{fig:emb_visualization}. Furthermore, embeddings defined by Eq.~(\\ref{eq:sin2d1}-\\ref{eq:sin2d2}) allow attending to a specific small region around a point on the image without emphasizing points with only same $x$ or only same~$y$ while we expect this artifact to contribute in case of concatenation of 1D embeddings for each axis~\\cite{chu2021conditional,dosovitskiy2020image,bello2019attention,parmar2018image}.  \\paragraph{Positional encoding for sound} We propose to tie positional embeddings to timestamps in seconds. The embedding for a frame centered at $t$ seconds is given by: $$ E_{2k}(t) = \\cos{\\omega_k t} \\qquad E_{2k + 1}(t) = \\sin{\\omega_k t} \\qquad \\omega_k = 30 \\times 10000^{-2k/K} $$ The choice of $\\omega_k$ corresponds to the scaled version of Eq.~(\\ref{eq:vashwani}) and ensures that queries with 30ms specificity are possible even with minutes-long audio fragments.  Two main factors contribute to the choice of base in the exponentiation of $w_k$ and $w_{k,x}$, $w_{k,y}$ frequencies: i) how precise the location should be for encoding (e.g. there is no need to encode position in audio much more precisely than one syllable takes); ii) longest reasonable ``length'' that is expected in the data. CAPE could be parametrized by precision and ``length'' parameters, but we believe that for all practical cases provided choices are reasonable.   ", "image_folder": 12, "answer": "The two main factors are the precision needed for encoding location in audio and the longest expected \"length\" of data.", "bert_scores": [0.906868577003479, 0.9159733057022095, 0.9113982319831848], "rouge_scores": [{"rouge-1": {"r": 0.47058823529411764, "p": 0.42105263157894735, "f": 0.44444443945987655}, "rouge-2": {"r": 0.1111111111111111, "p": 0.10526315789473684, "f": 0.10810810311176064}, "rouge-l": {"r": 0.35294117647058826, "p": 0.3157894736842105, "f": 0.3333333283487655}}]}
{"id": 13, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by the line that maintains a consistently lower perplexity across all inference input token ranges when compared to the Sinusoidal, Rotary, and T5 Bias methods in the graph depicting extrapolation performance of models trained on 1024 tokens, what is the main computational cost associated with its relative positional embeddings in machine translation models?\nText Context: \n For machine translation experiments we estimate the forward and backward time slowdown when learnable relative positional embedding is used in Transformer layer compared to vanilla Transformer layer. Besides we evaluate slowdown for entire encoder and decoder. Forward and backward time benchmarking is done in Fairseq (PyTorch) for different input sequences length (10, 100, 1000), for different context size of relative positional embedding (100 and 1000 tokens to left/right) in full-precision (fp32) and half-precision (fp16) floating-point computations. We use Volta V-100 32GB GPU and report slowdown for average time of forward and backward passes measured across 100 updates with 10 updates of warmup, see Table~\\ref{tab:speed_relpos}. The batch size is set to 50, attention heads are set to 8 and embedding dimension is 768 (typical use case), there are 6 Transformer layers in encoder and decoder. Results in Table~\\ref{tab:speed_relpos} demonstrate that relative positional embedding indeed has additional computational cost which is even more pronounced with mixed-precision computations.   \\end{document}    ", "image_folder": 13, "answer": "The main computational cost associated with the ALiBi method's relative positional embeddings is the additional time required for forward and backward passes during the translation process, which is further increased with mixed-precision computations.", "bert_scores": [0.8788130879402161, 0.9326117038726807, 0.9049134850502014], "rouge_scores": [{"rouge-1": {"r": 0.5, "p": 0.20689655172413793, "f": 0.2926829226888757}, "rouge-2": {"r": 0.18181818181818182, "p": 0.0625, "f": 0.09302325200649016}, "rouge-l": {"r": 0.4166666666666667, "p": 0.1724137931034483, "f": 0.2439024348839977}}]}
{"id": 14, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: In reference to the graph showing extrapolation performance of models trained on 1024 tokens, for the method that maintains a consistently lower perplexity across all inference input token ranges compared to the Sinusoidal, Rotary, and T5 Bias methods, how does this specific method's CAPE mechanism handle positional embeddings during training and inference?\nText Context: \n Absolute sinusoidal positional embeddings lack regularization, leading to ``in-domain'' generalization issues as the model may start learning spurious correlations. For applications where inference input sizes may be significantly different than the training ones, ``out-of-domain'' issues may also arise, as positions rarely observed at training may lead to improper inference predictions. To ensure that the model does not learn spurious correlations between content and position, we introduce the following augmentations for absolute sinusoidal positional embeddings at training time, see Figure~\\ref{fig:cape}.   \\begin{figure}[t!] \\centering \\includegraphics[width=0.75\\textwidth]{figures/cape_scheme.png} \\caption{Example of CAPE's transformations for an image: patches positions are scaled to $[-1, 1]\\times [-1, 1]$; random global shift, local shift, and global scaling are then applied to the grid of positions. \\label{fig:cape} } \\end{figure}  \\paragraph{Global shift} Transform every embedding in a sequence using $S^\\Delta$ operator with a global random shift from uniform zero-mean distribution $\\Delta \\sim \\mathcal{U}(-\\Delta_{max}, \\Delta_{max})$: $$ \\mathbf{E}'(n) = S^\\Delta \\mathbf{E}(n) \\qquad \\{S^\\Delta\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k \\Delta } \\qquad \\Delta \\in \\mathbb{R} $$ This modification hides the absolute positional information, but relative relations, see Eq.~(\\ref{eq-unitary-operator}), between embeddings still hold. This transformation can be rewritten as augmenting positions by a random shift before encoding with $\\sin$ and $\\cos$: \\begin{equation} n'_i \\leftarrow n_i + \\Delta \\qquad \\quad x'_i \\leftarrow x_i + \\Delta_x, \\;\\; y'_i \\leftarrow y_i + \\Delta_y \\qquad \\qquad t'_i \\leftarrow t_i + \\Delta \\label{cape-global-shift} \\end{equation}  \\paragraph{Local shift} To further increase augmentation and prevent capturing spontaneous correlations, we additionally introduce local shifts from uniform zero-mean distribution $\\epsilon_i \\sim \\mathcal{U}(-\\epsilon_{max}, \\epsilon_{max})$ \\begin{equation} n'_i \\leftarrow n_i + \\epsilon_i        \\qquad \\quad x'_i \\leftarrow x_i + \\epsilon_{x,i},   \\;\\; y'_i \\leftarrow y_i + \\epsilon_{y,i}    \\qquad \\qquad t'_i \\leftarrow t_i + \\epsilon_i \\label{cape-local-shift} \\end{equation} \\paragraph{Global scaling} To prevent distances memorization, we also introduce random global scale $\\lambda$ from $\\log \\lambda \\sim \\mathcal{U}(- \\log \\lambda_{max}, \\, \\log \\lambda_{max})$ \\begin{equation} n'_i \\leftarrow \\lambda n_i     \\qquad \\qquad \\quad x'_i \\leftarrow \\lambda x_i,    \\;\\;          \\quad y'_i \\leftarrow \\lambda y_i     \\qquad \\qquad \\quad \\quad t'_i \\leftarrow \\lambda t_i                   \\quad \\label{cape-scale} \\end{equation}  At training time, computing our continuous augmented positional embedding is performed through four steps: i)~mean-normalization of positions (extract mean of sequence positions), ii)~global shift Eq.~(\\ref{cape-global-shift}), iii)~local shift Eq.~(\\ref{cape-local-shift}), and iv)~global scaling Eq.~(\\ref{cape-scale}). At inference time, only mean-normalization of positions is performed. The reference implementation can be found in Appendix~\\ref{app:impl}.  ", "image_folder": 14, "answer": "The CAPE mechanism applies random global shifts, local shifts, and global scalings to positional embeddings during training. At inference, only mean-normalization of positions is performed.", "bert_scores": [0.9271032810211182, 0.9377384185791016, 0.9323905110359192], "rouge_scores": [{"rouge-1": {"r": 0.631578947368421, "p": 0.5217391304347826, "f": 0.571428566473923}, "rouge-2": {"r": 0.2608695652173913, "p": 0.25, "f": 0.25531914393843375}, "rouge-l": {"r": 0.5789473684210527, "p": 0.4782608695652174, "f": 0.5238095188548754}}]}
{"id": 15, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the language model represented by the bar that indicates a training speed higher than the smallest bar yet lower than the tallest bar for each input length category, what is the magnitude of that model based on the number of trainable parameters?\nText Context: \nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "image_folder": 15, "answer": "The model has 175 billion parameters.", "bert_scores": [0.9478360414505005, 0.8568758368492126, 0.900063693523407], "rouge_scores": [{"rouge-1": {"r": 0.36363636363636365, "p": 0.6666666666666666, "f": 0.4705882307266437}, "rouge-2": {"r": 0.1, "p": 0.2, "f": 0.13333332888888905}, "rouge-l": {"r": 0.36363636363636365, "p": 0.6666666666666666, "f": 0.4705882307266437}}]}
{"id": 16, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model depicted in the bar chart with a training speed that is higher than the smallest bar but lower than the tallest bar for each input length category, how do its results in the few-shot learning setting compare to the state-of-the-art fine-tuned models?\nText Context: \n\\label{section:Introduction} Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer.  First, single-layer representations were learned using word vectors \\cite{mikolov2013efficient, pennington2014glove} and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations \\cite{dai2015semi, mccann2017learned, peters2018dissecting} (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models \\cite{vaswani2017attention}  have been directly fine-tuned, entirely removing the need for task-specific architectures \\cite{radford2018gpt1, devlin2018bert, howard2018universal}.   This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms \\cite{raffel2019t5, liu2019roberta, yang2019xlnet, lan2019albert}. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.  Removing this limitation would be desirable, for several reasons.  First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.  There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.  For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.  Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance \\cite{hendrycks2020pretrained} observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it \\cite{yogatama2019learning, mccoy2019right}. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task \\cite{gururangan2018annotation, niven2019probing}.  Third, humans do not require large supervised datasets to learn most language tasks -- a brief directive in natural language (e.g. ``please tell me if this sentence describes something happy or something sad'') or at most a tiny number of demonstrations (e.g. ``here are two examples of people acting brave; please give a third example of bravery'') is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence.  Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -- it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.  To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.  One potential route towards addressing these issues is meta-learning\\footnote{In the context of language models this has sometimes been called ``zero-shot transfer'', but this term is potentially ambiguous: the method is ``zero-shot'' in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples.  To avoid this confusion, we use the term ``meta-learning'' to capture the inner-loop / outer-loop structure of the general method, and the term ``in context-learning\" to refer to the inner loop of meta-learning. We further specialize the description to ``zero-shot\", ``one-shot\", or ``few-shot\" depending on how many demonstrations are provided at inference time.  These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training -- this is an important issue which we discuss later in the paper, but ``meta-learning'' is intended to encompass both possibilities, and simply describes the inner-outer loop structure.} -- which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure \\ref{figure:metalearning}). Recent work \\cite{radford2019language} attempts to do this via what we call ``in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.   While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -- for example \\cite{radford2019language} achieves only 4\\% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art.  Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.  Another recent trend in language modeling may offer a way forward.  In recent years the capacity of transformer language models has increased substantially, from 100 million parameters \\cite{radford2018gpt1}, to 300 million parameters \\cite{devlin2018bert}, to 1.5 billion parameters \\cite{radford2019language}, to 8 billion parameters \\cite{shoeybi2019megatronlm}, 11 billion parameters \\cite{raffel2019t5}, and finally 17 billion parameters \\cite{turing_17m}.  Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale \\cite{kaplan2020scaling}.  Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.   \\begin{figure} \\begin{center} \\includegraphics[width=0.7\\linewidth]{graphs/aggregate_performance.png} \\end{center} \\caption{\\textbf{Aggregate performance for all 42 accuracy-denominated benchmarks}~~~While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure \\ref{graph:superglue_analysis} for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.} \\label{figure:aggregate_performance} \\end{figure}  In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities.  Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.  For each task, we evaluate GPT-3 under 3 conditions: (a) ``few-shot learning'', or in-context learning where we allow as many demonstrations as will fit into the model\u2019s context window (typically 10 to 100), (b) ``one-shot learning'', where we allow only one demonstration, and (c) ``zero-shot'' learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.  GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.  Figure \\ref{graph:scramble_prompted} illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, $K$. Few-shot learning also improves dramatically with model size.  Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study.  We emphasize that these ``learning'' curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.  Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models).  For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting.  Similarly, GPT-3 achieves 64.3\\% accuracy on TriviaQA in the zero-shot setting, 68.0\\% in the one-shot setting, and 71.2\\% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.  GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once.  We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.  At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3.  This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.  By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure \\ref{figure:aggregate_performance}, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).  We also undertake a systematic study of ``data contamination'' -- a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects.  Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.  In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings.  Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.  Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.  The remainder of this paper is organized as follows.  In Section \\ref{section:Approach}, we describe our approach and methods for training GPT-3 and evaluating it.  Section \\ref{section:Results} presents results on the full range of tasks in the zero-, one- and few-shot settings.  Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} addresses questions of data contamination (train-test overlap).  Section \\ref{section:Limitations} discusses limitations of GPT-3.  Section \\ref{section:Broader_Impacts} discusses broader impacts.  Section \\ref{section:Related Work} reviews related work and Section \\ref{section:Conclusion} concludes.       ", "image_folder": 16, "answer": "The model's few-shot learning results are sometimes competitive with or even surpass state-of-the-art fine-tuned models.", "bert_scores": [0.9414834380149841, 0.9289301633834839, 0.9351646900177002], "rouge_scores": [{"rouge-1": {"r": 0.5625, "p": 0.6, "f": 0.5806451562955255}, "rouge-2": {"r": 0.4, "p": 0.42857142857142855, "f": 0.41379309845422124}, "rouge-l": {"r": 0.5625, "p": 0.6, "f": 0.5806451562955255}}]}
{"id": 17, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that represents a training speed higher than the smallest bar but lower than the tallest bar for each input length category in the bar chart on training speed in words per second (WPS), what architectural pattern does it employ in its transformer layers?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 17, "answer": "The models represented by the middle bars in the bar chart for each input length category employ an alternating dense and locally banded sparse attention pattern in their transformer layers.", "bert_scores": [0.8858296871185303, 0.9303117990493774, 0.9075260162353516], "rouge_scores": [{"rouge-1": {"r": 0.7333333333333333, "p": 0.39285714285714285, "f": 0.511627902433748}, "rouge-2": {"r": 0.4666666666666667, "p": 0.2413793103448276, "f": 0.31818181368801657}, "rouge-l": {"r": 0.6, "p": 0.32142857142857145, "f": 0.4186046466197945}}]}
{"id": 18, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the bar that represents a training speed higher than the smallest bar for each input length category yet lower than the tallest bar, what is the context window size used for all models in the study, and how are documents handled within a sequence during training?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 18, "answer": "The context window size used for all models in the study is $n_{\\mathrm{ctx}}=2048$ tokens. During training, documents are handled within a sequence by drawing $K$ random examples from the task's training set as conditioning", "bert_scores": [0.885847806930542, 0.8934030532836914, 0.8896093964576721], "rouge_scores": [{"rouge-1": {"r": 0.3333333333333333, "p": 0.24242424242424243, "f": 0.2807017495106187}, "rouge-2": {"r": 0.07407407407407407, "p": 0.06060606060606061, "f": 0.06666666171666703}, "rouge-l": {"r": 0.2916666666666667, "p": 0.21212121212121213, "f": 0.24561403021237313}}]}
{"id": 19, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the training speed that is higher than the smallest bar but lower than the tallest bar for each input length category in the bar chart, what computational optimizations were applied for training the corresponding large language models?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 19, "answer": "The computational optimizations include alternating dense and locally banded sparse attention patterns in the transformer layers, and model parallelism within each matrix multiply and across the network layers to handle larger batch sizes and minimize memory usage.", "bert_scores": [0.8773776292800903, 0.9152162671089172, 0.8958975672721863], "rouge_scores": [{"rouge-1": {"r": 0.631578947368421, "p": 0.36363636363636365, "f": 0.46153845690088763}, "rouge-2": {"r": 0.4090909090909091, "p": 0.25, "f": 0.3103448228775268}, "rouge-l": {"r": 0.5789473684210527, "p": 0.3333333333333333, "f": 0.42307691843934914}}]}
{"id": 20, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that maintains the lowest perplexity across all inference input token ranges when trained on 1024 tokens, what is the magnitude of its language model based on the number of trainable parameters?\nText Context: \nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "image_folder": 20, "answer": "The model with the lowest perplexity across all inference input token ranges when trained on 1024 tokens is GPT-3, which has 175 billion parameters.", "bert_scores": [0.8817996382713318, 0.8943207263946533, 0.8880160450935364], "rouge_scores": [{"rouge-1": {"r": 0.5454545454545454, "p": 0.25, "f": 0.3428571385469388}, "rouge-2": {"r": 0.2, "p": 0.08695652173913043, "f": 0.12121211698806258}, "rouge-l": {"r": 0.45454545454545453, "p": 0.20833333333333334, "f": 0.28571428140408167}}]}
{"id": 21, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that maintains the lowest perplexity across all inference input token ranges when trained on 1024 tokens, how do its results in the few-shot learning setting compare to the state-of-the-art fine-tuned models?\nText Context: \n\\label{section:Introduction} Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer.  First, single-layer representations were learned using word vectors \\cite{mikolov2013efficient, pennington2014glove} and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations \\cite{dai2015semi, mccann2017learned, peters2018dissecting} (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models \\cite{vaswani2017attention}  have been directly fine-tuned, entirely removing the need for task-specific architectures \\cite{radford2018gpt1, devlin2018bert, howard2018universal}.   This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms \\cite{raffel2019t5, liu2019roberta, yang2019xlnet, lan2019albert}. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.  Removing this limitation would be desirable, for several reasons.  First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.  There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.  For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.  Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance \\cite{hendrycks2020pretrained} observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it \\cite{yogatama2019learning, mccoy2019right}. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task \\cite{gururangan2018annotation, niven2019probing}.  Third, humans do not require large supervised datasets to learn most language tasks -- a brief directive in natural language (e.g. ``please tell me if this sentence describes something happy or something sad'') or at most a tiny number of demonstrations (e.g. ``here are two examples of people acting brave; please give a third example of bravery'') is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence.  Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -- it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.  To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.  One potential route towards addressing these issues is meta-learning\\footnote{In the context of language models this has sometimes been called ``zero-shot transfer'', but this term is potentially ambiguous: the method is ``zero-shot'' in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples.  To avoid this confusion, we use the term ``meta-learning'' to capture the inner-loop / outer-loop structure of the general method, and the term ``in context-learning\" to refer to the inner loop of meta-learning. We further specialize the description to ``zero-shot\", ``one-shot\", or ``few-shot\" depending on how many demonstrations are provided at inference time.  These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training -- this is an important issue which we discuss later in the paper, but ``meta-learning'' is intended to encompass both possibilities, and simply describes the inner-outer loop structure.} -- which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure \\ref{figure:metalearning}). Recent work \\cite{radford2019language} attempts to do this via what we call ``in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.   While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -- for example \\cite{radford2019language} achieves only 4\\% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art.  Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.  Another recent trend in language modeling may offer a way forward.  In recent years the capacity of transformer language models has increased substantially, from 100 million parameters \\cite{radford2018gpt1}, to 300 million parameters \\cite{devlin2018bert}, to 1.5 billion parameters \\cite{radford2019language}, to 8 billion parameters \\cite{shoeybi2019megatronlm}, 11 billion parameters \\cite{raffel2019t5}, and finally 17 billion parameters \\cite{turing_17m}.  Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale \\cite{kaplan2020scaling}.  Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.   \\begin{figure} \\begin{center} \\includegraphics[width=0.7\\linewidth]{graphs/aggregate_performance.png} \\end{center} \\caption{\\textbf{Aggregate performance for all 42 accuracy-denominated benchmarks}~~~While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure \\ref{graph:superglue_analysis} for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.} \\label{figure:aggregate_performance} \\end{figure}  In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities.  Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.  For each task, we evaluate GPT-3 under 3 conditions: (a) ``few-shot learning'', or in-context learning where we allow as many demonstrations as will fit into the model\u2019s context window (typically 10 to 100), (b) ``one-shot learning'', where we allow only one demonstration, and (c) ``zero-shot'' learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.  GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.  Figure \\ref{graph:scramble_prompted} illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, $K$. Few-shot learning also improves dramatically with model size.  Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study.  We emphasize that these ``learning'' curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.  Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models).  For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting.  Similarly, GPT-3 achieves 64.3\\% accuracy on TriviaQA in the zero-shot setting, 68.0\\% in the one-shot setting, and 71.2\\% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.  GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once.  We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.  At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3.  This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.  By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure \\ref{figure:aggregate_performance}, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).  We also undertake a systematic study of ``data contamination'' -- a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects.  Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.  In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings.  Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.  Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.  The remainder of this paper is organized as follows.  In Section \\ref{section:Approach}, we describe our approach and methods for training GPT-3 and evaluating it.  Section \\ref{section:Results} presents results on the full range of tasks in the zero-, one- and few-shot settings.  Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} addresses questions of data contamination (train-test overlap).  Section \\ref{section:Limitations} discusses limitations of GPT-3.  Section \\ref{section:Broader_Impacts} discusses broader impacts.  Section \\ref{section:Related Work} reviews related work and Section \\ref{section:Conclusion} concludes.       ", "image_folder": 21, "answer": "The model with the lowest perplexity across all inference input token ranges, ALiBi, performs competitively or even surpasses fine-tuned models in few-shot learning settings on various NLP tasks.", "bert_scores": [0.8704696893692017, 0.9048672914505005, 0.8873352408409119], "rouge_scores": [{"rouge-1": {"r": 0.5, "p": 0.2857142857142857, "f": 0.36363635900826446}, "rouge-2": {"r": 0.13333333333333333, "p": 0.07407407407407407, "f": 0.09523809064625872}, "rouge-l": {"r": 0.375, "p": 0.21428571428571427, "f": 0.2727272680991736}}]}
{"id": 22, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that maintains the lowest perplexity across all inference input token ranges when trained on 1024 tokens, what architectural pattern does it employ in its transformer layers?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 22, "answer": "The model employs alternating dense and locally banded sparse attention patterns in its transformer layers.", "bert_scores": [0.9646963477134705, 0.9440078735351562, 0.9542399644851685], "rouge_scores": [{"rouge-1": {"r": 0.7333333333333333, "p": 0.7333333333333333, "f": 0.7333333283333333}, "rouge-2": {"r": 0.5333333333333333, "p": 0.5714285714285714, "f": 0.5517241329369797}, "rouge-l": {"r": 0.6666666666666666, "p": 0.6666666666666666, "f": 0.6666666616666668}}]}
{"id": 23, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that maintains the lowest perplexity across all inference input token ranges when extrapolated from training on 1024 tokens, what is the context window size used for all models in the study, and how are documents handled within a sequence during training?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 23, "answer": "The context window size for all models in the study is $n_{\\mathrm{ctx}}=2048$ tokens. During training, documents are handled within a sequence as delimited by 1 or 2 newlines depending on the task.", "bert_scores": [0.8946371674537659, 0.9028244018554688, 0.898712158203125], "rouge_scores": [{"rouge-1": {"r": 0.3333333333333333, "p": 0.25806451612903225, "f": 0.2909090859900827}, "rouge-2": {"r": 0.07407407407407407, "p": 0.06451612903225806, "f": 0.06896551226516089}, "rouge-l": {"r": 0.2916666666666667, "p": 0.22580645161290322, "f": 0.2545454496264464}}]}
{"id": 24, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the model that maintains the lowest perplexity across all inference input token ranges when extrapolated from training on 1024 tokens, what computational optimizations were applied for its training?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 24, "answer": "The model utilizing ALiBi (Attention with Linear Biases) maintains the lowest perplexity across all inference input token ranges when extrapolated from training on 1024 tokens. This optimization involves alternating dense and locally banded sparse attention patterns in the transformer", "bert_scores": [0.8242119550704956, 0.863703727722168, 0.8434959053993225], "rouge_scores": [{"rouge-1": {"r": 0.2631578947368421, "p": 0.13157894736842105, "f": 0.17543859204678372}, "rouge-2": {"r": 0.0, "p": 0.0, "f": 0.0}, "rouge-l": {"r": 0.15789473684210525, "p": 0.07894736842105263, "f": 0.10526315345029258}}]}
{"id": 25, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Considering the perplexity analysis presented in the figure for various lengths, particularly noting the technique that showcases lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, what is the magnitude of the corresponding language model's trainable parameters?\nText Context: \nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "image_folder": 25, "answer": "The language model in question has 175 billion trainable parameters.", "bert_scores": [0.9444047212600708, 0.893616795539856, 0.9183090329170227], "rouge_scores": [{"rouge-1": {"r": 0.5454545454545454, "p": 0.6, "f": 0.5714285664399092}, "rouge-2": {"r": 0.4, "p": 0.4444444444444444, "f": 0.42105262659279785}, "rouge-l": {"r": 0.5454545454545454, "p": 0.6, "f": 0.5714285664399092}}]}
{"id": 26, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the technique that shows lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, how do its few-shot learning results compare to those of state-of-the-art fine-tuned models?\nText Context: \n\\label{section:Introduction} Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer.  First, single-layer representations were learned using word vectors \\cite{mikolov2013efficient, pennington2014glove} and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations \\cite{dai2015semi, mccann2017learned, peters2018dissecting} (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models \\cite{vaswani2017attention}  have been directly fine-tuned, entirely removing the need for task-specific architectures \\cite{radford2018gpt1, devlin2018bert, howard2018universal}.   This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms \\cite{raffel2019t5, liu2019roberta, yang2019xlnet, lan2019albert}. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.  Removing this limitation would be desirable, for several reasons.  First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.  There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.  For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.  Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance \\cite{hendrycks2020pretrained} observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it \\cite{yogatama2019learning, mccoy2019right}. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task \\cite{gururangan2018annotation, niven2019probing}.  Third, humans do not require large supervised datasets to learn most language tasks -- a brief directive in natural language (e.g. ``please tell me if this sentence describes something happy or something sad'') or at most a tiny number of demonstrations (e.g. ``here are two examples of people acting brave; please give a third example of bravery'') is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence.  Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -- it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.  To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.  One potential route towards addressing these issues is meta-learning\\footnote{In the context of language models this has sometimes been called ``zero-shot transfer'', but this term is potentially ambiguous: the method is ``zero-shot'' in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples.  To avoid this confusion, we use the term ``meta-learning'' to capture the inner-loop / outer-loop structure of the general method, and the term ``in context-learning\" to refer to the inner loop of meta-learning. We further specialize the description to ``zero-shot\", ``one-shot\", or ``few-shot\" depending on how many demonstrations are provided at inference time.  These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training -- this is an important issue which we discuss later in the paper, but ``meta-learning'' is intended to encompass both possibilities, and simply describes the inner-outer loop structure.} -- which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure \\ref{figure:metalearning}). Recent work \\cite{radford2019language} attempts to do this via what we call ``in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.   While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -- for example \\cite{radford2019language} achieves only 4\\% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art.  Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.  Another recent trend in language modeling may offer a way forward.  In recent years the capacity of transformer language models has increased substantially, from 100 million parameters \\cite{radford2018gpt1}, to 300 million parameters \\cite{devlin2018bert}, to 1.5 billion parameters \\cite{radford2019language}, to 8 billion parameters \\cite{shoeybi2019megatronlm}, 11 billion parameters \\cite{raffel2019t5}, and finally 17 billion parameters \\cite{turing_17m}.  Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale \\cite{kaplan2020scaling}.  Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.   \\begin{figure} \\begin{center} \\includegraphics[width=0.7\\linewidth]{graphs/aggregate_performance.png} \\end{center} \\caption{\\textbf{Aggregate performance for all 42 accuracy-denominated benchmarks}~~~While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure \\ref{graph:superglue_analysis} for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.} \\label{figure:aggregate_performance} \\end{figure}  In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities.  Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.  For each task, we evaluate GPT-3 under 3 conditions: (a) ``few-shot learning'', or in-context learning where we allow as many demonstrations as will fit into the model\u2019s context window (typically 10 to 100), (b) ``one-shot learning'', where we allow only one demonstration, and (c) ``zero-shot'' learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.  GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.  Figure \\ref{graph:scramble_prompted} illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, $K$. Few-shot learning also improves dramatically with model size.  Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study.  We emphasize that these ``learning'' curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.  Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models).  For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting.  Similarly, GPT-3 achieves 64.3\\% accuracy on TriviaQA in the zero-shot setting, 68.0\\% in the one-shot setting, and 71.2\\% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.  GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once.  We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.  At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3.  This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.  By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure \\ref{figure:aggregate_performance}, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).  We also undertake a systematic study of ``data contamination'' -- a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects.  Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.  In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings.  Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.  Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.  The remainder of this paper is organized as follows.  In Section \\ref{section:Approach}, we describe our approach and methods for training GPT-3 and evaluating it.  Section \\ref{section:Results} presents results on the full range of tasks in the zero-, one- and few-shot settings.  Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} addresses questions of data contamination (train-test overlap).  Section \\ref{section:Limitations} discusses limitations of GPT-3.  Section \\ref{section:Broader_Impacts} discusses broader impacts.  Section \\ref{section:Related Work} reviews related work and Section \\ref{section:Conclusion} concludes.       ", "image_folder": 26, "answer": "For the technique with lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, its few-shot learning results sometimes surpass state-of-the-art fine-tuned models.", "bert_scores": [0.8514618873596191, 0.9056174159049988, 0.8777050971984863], "rouge_scores": [{"rouge-1": {"r": 0.4375, "p": 0.2916666666666667, "f": 0.3499999952000001}, "rouge-2": {"r": 0.13333333333333333, "p": 0.07692307692307693, "f": 0.09756097096966114}, "rouge-l": {"r": 0.375, "p": 0.25, "f": 0.29999999520000004}}]}
{"id": 27, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the technique that shows a lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, what architectural pattern does its corresponding model employ in its transformer layers?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 27, "answer": "The model employing the alternating dense and locally banded sparse attention patterns in its transformer layers corresponds to the technique with lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=204", "bert_scores": [0.8464615345001221, 0.9242788553237915, 0.8836603164672852], "rouge_scores": [{"rouge-1": {"r": 0.8, "p": 0.4, "f": 0.533333328888889}, "rouge-2": {"r": 0.5333333333333333, "p": 0.23529411764705882, "f": 0.3265306079966681}, "rouge-l": {"r": 0.7333333333333333, "p": 0.36666666666666664, "f": 0.4888888844444445}}]}
{"id": 28, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Considering the technique that shows lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, what is the context window size used for this particular model in the study, and how are documents managed within a sequence during its training?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 28, "answer": "The context window size used for the model in the study is $n_{\\mathrm{ctx}}=2048$ tokens. During training, documents are managed within a sequence by performing fuzzy deduplication at the document level, within and across datasets", "bert_scores": [0.8881241083145142, 0.8970735669136047, 0.8925763368606567], "rouge_scores": [{"rouge-1": {"r": 0.3333333333333333, "p": 0.25806451612903225, "f": 0.2909090859900827}, "rouge-2": {"r": 0.07407407407407407, "p": 0.06060606060606061, "f": 0.06666666171666703}, "rouge-l": {"r": 0.2916666666666667, "p": 0.22580645161290322, "f": 0.2545454496264464}}]}
{"id": 29, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the technique that demonstrates lower perplexity than the Sinusoidal method at L=3072 and higher perplexity than the ALiBi method at L=2048, what computational optimizations were applied for training such large language models?\nText Context: \n\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      ", "image_folder": 29, "answer": "The computational optimizations applied for training large language models include scaling up model size, dataset size and diversity, length of training, alternating attention patterns, partitioning the model across GPUs for efficiency, and using gradient noise scale to guide batch size choice.", "bert_scores": [0.8510082364082336, 0.8761951327323914, 0.8634180426597595], "rouge_scores": [{"rouge-1": {"r": 0.42105263157894735, "p": 0.21621621621621623, "f": 0.2857142812308674}, "rouge-2": {"r": 0.0, "p": 0.0, "f": 0.0}, "rouge-l": {"r": 0.3157894736842105, "p": 0.16216216216216217, "f": 0.214285709802296}}]}
{"id": 30, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by the color bar that uses less memory than the orange and pink bars but more memory than the yellow bar for an input length of 1024, as shown in the bar chart, what is the motivation behind using adaptive input embeddings in neural language models?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 30, "answer": "The blue bar represents the method that uses adaptive input embeddings in neural language models for an input length of 1024. The motivation is to reduce the number of parameters, freeing up capacity for other parts of the model, by partitioning the vocabulary based", "bert_scores": [0.8989579677581787, 0.9481026530265808, 0.9228765368461609], "rouge_scores": [{"rouge-1": {"r": 0.7619047619047619, "p": 0.4444444444444444, "f": 0.5614035041181903}, "rouge-2": {"r": 0.6086956521739131, "p": 0.32558139534883723, "f": 0.4242424197015611}, "rouge-l": {"r": 0.7619047619047619, "p": 0.4444444444444444, "f": 0.5614035041181903}}]}
{"id": 31, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by the color bar that uses less memory than the orange and pink bars but more than the yellow bar at an input length of 1024, how are words assigned to clusters, and what is the impact of this cluster assignment on the dimensionality of word embeddings as described in the paper?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 31, "answer": "The blue bar represents the method. Words are assigned to clusters based on frequency, reducing representation size for less frequent words. This lowers memory usage compared to the orange and pink methods. The dimensionality is standardized to $d$ through projections, enabling efficient", "bert_scores": [0.8547671437263489, 0.8662177324295044, 0.8604543209075928], "rouge_scores": [{"rouge-1": {"r": 0.375, "p": 0.3157894736842105, "f": 0.34285713789387756}, "rouge-2": {"r": 0.16216216216216217, "p": 0.14634146341463414, "f": 0.15384614885930323}, "rouge-l": {"r": 0.375, "p": 0.3157894736842105, "f": 0.34285713789387756}}]}
{"id": 32, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the color bar in the training memory usage chart that uses less memory than the orange and pink bars but more than the yellow bar for an input length of 1024, what approach is used for weight sharing between input and output layers in the context of adaptive embeddings, and under what conditions does this approach improve performance?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 32, "answer": "For the blue bar, weight sharing between input and output layers involves sharing both word embeddings and projections, except for the head projection. This approach improves performance when the output layer is an adaptive softmax with the same vocabulary partition, dimension, and reduction factor (", "bert_scores": [0.8918179869651794, 0.9191100597381592, 0.9052583575248718], "rouge_scores": [{"rouge-1": {"r": 0.5806451612903226, "p": 0.47368421052631576, "f": 0.5217391254862423}, "rouge-2": {"r": 0.32432432432432434, "p": 0.2727272727272727, "f": 0.2962962913336382}, "rouge-l": {"r": 0.5161290322580645, "p": 0.42105263157894735, "f": 0.46376811099348875}}]}
{"id": 33, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the color bar in the training memory usage chart that signifies a method using less memory than the orange and pink bars but more than the yellow bar at an input length of 1024, why did the authors choose to apply dropout to the tail projection within this method's adaptive softmax, and which dataset saw performance improvements due to this regularization technique?\nText Context: \n We also found that adaptive softmax can benefit from additional regularization of rare words. Adaptive softmax first projects the model output to the dimension of a particular cluster and then computes a dot product with the respective word embeddings. We add dropout to the output of the first projection for all clusters, except for the head. This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on \\wiki{} (Table~\\ref{tab:adaptive}).  However, we found that adding dropout in this way is not helpful for larger datasets such as \\gbw{}. Unfortunately, a standard softmax over 800K words is not tractable and we were unable to make a comparison. It may be possible to achieve better results by tuning dropout for each band of the tail and we leave this for future work.  \\begin{table} \\centering \\begin{tabular}{crr} \\toprule & \\thead{Tail\\\\ dropout} & \\thead{Validation\\\\ perplexity} \\\\ \\midrule Softmax (\\sm{}) & N/A & 23.87 \\\\ Adaptive (\\adp{}) & 0.0 & 24.74 \\\\ Adaptive (\\adp{}) & 0.2 & 21.23 \\\\ \\bottomrule \\end{tabular} \\caption{Perplexity on \\wiki{} when regularizing rare words in adaptive softmax.} \\label{tab:adaptive} \\end{table}   ", "image_folder": 33, "answer": "The authors applied dropout to the tail projection in the adaptive softmax to regularize rare words, which improved performance on the \\wiki{} dataset. The blue color bar represents this method with lesser memory usage than the orange (Softmax) and pink (Ad", "bert_scores": [0.8960198163986206, 0.9533830881118774, 0.9238117933273315], "rouge_scores": [{"rouge-1": {"r": 0.7727272727272727, "p": 0.4722222222222222, "f": 0.5862068918430441}, "rouge-2": {"r": 0.7083333333333334, "p": 0.425, "f": 0.5312499953125001}, "rouge-l": {"r": 0.7727272727272727, "p": 0.4722222222222222, "f": 0.5862068918430441}}]}
{"id": 34, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the color bar in the chart that indicates a method using less memory than the orange and pink bars, but more than the yellow bar at an input length of 1024, what are the test perplexity results achieved by this specific model on the Billion Word benchmark and the WikiText-103 benchmark compared to previously published work?\nText Context: \n Language modeling is a basic task in natural language processing, with many applications such as speech recognition \\citep{arisoy:2012:wfnlm} and statistical machine translation \\citep{schwenk:2012:wfnlm,vaswani:2013:emnlp,baltescu2014pragmatic}. Recently, much progress has been made by neural methods \\citep{bengio:2003:jmlr,mikolov:2010:interspeech} based on LSTMs \\citep{jozefowicz2016lm}, gated convolutional networks \\citep{dauphin2017convlm} and self-attentional networks \\citep{alrfou2018chartrans}.  There are different choices for the basic unit we wish to model, including full words \\citep{bengio:2003:jmlr}, characters for the input \\citep{kim2016character}, or also the output \\citep{merity2018lm} as well as sub-words \\citep{buckman2018taacl,mielke2018spell}. Word-based models are particularly challenging since computing probabilities for all 800K words of the \\gbw{} benchmark is still a substantial part of the overall computation \\citep{chen2016strategies}.  A popular approach to lower the computational burden is to structure the output vocabulary so that not all probabilities need to be computed. The hierarchical softmax does this by introducing latent variables or clusters to simplify normalization \\citep{goodman:2001:icassp,morin:2005:aistats,mikolov:2011:icassp}. This has been further improved by the \\emph{adaptive softmax} which introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words \\citep{grave2017icml}.  In this paper, we introduce \\emph{adaptive input embeddings} which extend the adaptive softmax to input word representations. This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words. For a competitive setup on the \\gbw{} benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23\\% while achieving higher accuracy over fixed size embeddings. When the adaptive input representations are tied with an adaptive softmax in the output, then the number of parameters is reduced by a total of 61\\%.  Our experiments compare models based on word inputs, character inputs, as well as sub-word units using a self-attention architecture \\citep{vaswani2017transformer}. We show that models with adaptive word representations can outperform very strong character-based models while training more than twice as fast. We also substantially improve adaptive softmax by introducing additional dropout regularization in the tail projection. On the \\wiki{} benchmark we achieve a perplexity of 18.7, a reduction of 10.5 perplexity over the previously best published result. On the larger \\gbw{} benchmark our best model with adaptive input embeddings achieves 23.02 perplexity, a reduction of nearly 5 perplexity over the next best previously published result.  ", "image_folder": 34, "answer": "At input length 1024, the blue bar indicates a method with less memory than orange and pink but more than yellow. According to the text, this model with adaptive input embeddings achieved a perplexity of 18.7 on WikiText-103", "bert_scores": [0.8714727759361267, 0.8869373798370361, 0.8791370391845703], "rouge_scores": [{"rouge-1": {"r": 0.4482758620689655, "p": 0.38235294117647056, "f": 0.41269840772990685}, "rouge-2": {"r": 0.15625, "p": 0.13157894736842105, "f": 0.14285713789387772}, "rouge-l": {"r": 0.3793103448275862, "p": 0.3235294117647059, "f": 0.3492063442378433}}]}
{"id": 35, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the representation that shows a consistent decrease in perplexity as training progresses and achieves lower perplexity than the Sinusoidal representation with L=1024 after 6000 GPU hours of training, what is the motivation behind using adaptive input embeddings as discussed in the context of neural language models?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 35, "answer": "The motivation is to reduce the number of parameters, thereby freeing up capacity for other model elements, by assigning words to clusters based on frequency and utilizing varying representation sizes accordingly.", "bert_scores": [0.9001418948173523, 0.9237350225448608, 0.911785900592804], "rouge_scores": [{"rouge-1": {"r": 0.6190476190476191, "p": 0.43333333333333335, "f": 0.5098039167243368}, "rouge-2": {"r": 0.391304347826087, "p": 0.3, "f": 0.33962263659665365}, "rouge-l": {"r": 0.6190476190476191, "p": 0.43333333333333335, "f": 0.5098039167243368}}]}
{"id": 36, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the representation that shows a consistent decrease in perplexity as training time increases and exhibits lower perplexity than the Sinusoidal representation with L=1024 after 6000 GPU hours of training, how are words assigned to clusters in this model, and what is the effect of this cluster assignment on the dimensionality of word embeddings?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 36, "answer": "In the model, words are assigned to clusters based on frequency, reducing dimensionality variably with lower-frequency words having smaller dimensions. This results in fewer parameters and maintains consistent embedding dimensions through linear projection. The effect is an efficient and effective representation, lowering", "bert_scores": [0.8535354733467102, 0.8483479619026184, 0.850933849811554], "rouge_scores": [{"rouge-1": {"r": 0.34375, "p": 0.28205128205128205, "f": 0.309859149978179}, "rouge-2": {"r": 0.13513513513513514, "p": 0.12195121951219512, "f": 0.12820512321827762}, "rouge-l": {"r": 0.34375, "p": 0.28205128205128205, "f": 0.309859149978179}}]}
{"id": 37, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: In the context of the representation that shows a consistent decrease in perplexity as training time increases and has lower perplexity than the Sinusoidal representation with L=1024 after 6000 GPU hours of training, what approach is used for weight sharing between input and output layers in the context of adaptive embeddings, and under what conditions does this approach improve performance?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 37, "answer": "The approach used is weight sharing between input and output layers where clusters partition the vocabulary based on word frequency. This approach improves performance when the output layer is an adaptive softmax with the same vocabulary partition, dimension, and factor as the input layer.", "bert_scores": [0.9196169972419739, 0.9255223870277405, 0.9225602746009827], "rouge_scores": [{"rouge-1": {"r": 0.6451612903225806, "p": 0.5882352941176471, "f": 0.6153846103952664}, "rouge-2": {"r": 0.40540540540540543, "p": 0.3488372093023256, "f": 0.37499999502812503}, "rouge-l": {"r": 0.6129032258064516, "p": 0.5588235294117647, "f": 0.5846153796260355}}]}
{"id": 38, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: In considering the representation that shows a consistent decrease in perplexity with extended training and maintains lower perplexity than the Sinusoidal representation with L=1024 after 6000 GPU hours, why did the authors implement dropout on the tail projection within the adaptive softmax, and on which dataset did this approach enhance performance?\nText Context: \n We also found that adaptive softmax can benefit from additional regularization of rare words. Adaptive softmax first projects the model output to the dimension of a particular cluster and then computes a dot product with the respective word embeddings. We add dropout to the output of the first projection for all clusters, except for the head. This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on \\wiki{} (Table~\\ref{tab:adaptive}).  However, we found that adding dropout in this way is not helpful for larger datasets such as \\gbw{}. Unfortunately, a standard softmax over 800K words is not tractable and we were unable to make a comparison. It may be possible to achieve better results by tuning dropout for each band of the tail and we leave this for future work.  \\begin{table} \\centering \\begin{tabular}{crr} \\toprule & \\thead{Tail\\\\ dropout} & \\thead{Validation\\\\ perplexity} \\\\ \\midrule Softmax (\\sm{}) & N/A & 23.87 \\\\ Adaptive (\\adp{}) & 0.0 & 24.74 \\\\ Adaptive (\\adp{}) & 0.2 & 21.23 \\\\ \\bottomrule \\end{tabular} \\caption{Perplexity on \\wiki{} when regularizing rare words in adaptive softmax.} \\label{tab:adaptive} \\end{table}   ", "image_folder": 38, "answer": "The authors implemented dropout on the tail projection within the adaptive softmax to provide additional regularization for rare words. This approach enhanced performance on the \\wiki{} dataset, as shown in Table~\\ref{tab:adaptive}, leading to lower validation perplexity.", "bert_scores": [0.8865788578987122, 0.9514840841293335, 0.917885422706604], "rouge_scores": [{"rouge-1": {"r": 0.7272727272727273, "p": 0.5, "f": 0.5925925877640604}, "rouge-2": {"r": 0.4166666666666667, "p": 0.29411764705882354, "f": 0.3448275813555292}, "rouge-l": {"r": 0.6818181818181818, "p": 0.46875, "f": 0.5555555507270235}}]}
{"id": 39, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Referring to the representation that shows a consistent decrease in perplexity as training time increases and achieves lower perplexity than the Sinusoidal representation with L=1024 after 6000 GPU hours, what are the test perplexity results achieved by this model on the Billion Word benchmark and the WikiText-103 benchmark compared to previously published work?\nText Context: \n Language modeling is a basic task in natural language processing, with many applications such as speech recognition \\citep{arisoy:2012:wfnlm} and statistical machine translation \\citep{schwenk:2012:wfnlm,vaswani:2013:emnlp,baltescu2014pragmatic}. Recently, much progress has been made by neural methods \\citep{bengio:2003:jmlr,mikolov:2010:interspeech} based on LSTMs \\citep{jozefowicz2016lm}, gated convolutional networks \\citep{dauphin2017convlm} and self-attentional networks \\citep{alrfou2018chartrans}.  There are different choices for the basic unit we wish to model, including full words \\citep{bengio:2003:jmlr}, characters for the input \\citep{kim2016character}, or also the output \\citep{merity2018lm} as well as sub-words \\citep{buckman2018taacl,mielke2018spell}. Word-based models are particularly challenging since computing probabilities for all 800K words of the \\gbw{} benchmark is still a substantial part of the overall computation \\citep{chen2016strategies}.  A popular approach to lower the computational burden is to structure the output vocabulary so that not all probabilities need to be computed. The hierarchical softmax does this by introducing latent variables or clusters to simplify normalization \\citep{goodman:2001:icassp,morin:2005:aistats,mikolov:2011:icassp}. This has been further improved by the \\emph{adaptive softmax} which introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words \\citep{grave2017icml}.  In this paper, we introduce \\emph{adaptive input embeddings} which extend the adaptive softmax to input word representations. This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words. For a competitive setup on the \\gbw{} benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23\\% while achieving higher accuracy over fixed size embeddings. When the adaptive input representations are tied with an adaptive softmax in the output, then the number of parameters is reduced by a total of 61\\%.  Our experiments compare models based on word inputs, character inputs, as well as sub-word units using a self-attention architecture \\citep{vaswani2017transformer}. We show that models with adaptive word representations can outperform very strong character-based models while training more than twice as fast. We also substantially improve adaptive softmax by introducing additional dropout regularization in the tail projection. On the \\wiki{} benchmark we achieve a perplexity of 18.7, a reduction of 10.5 perplexity over the previously best published result. On the larger \\gbw{} benchmark our best model with adaptive input embeddings achieves 23.02 perplexity, a reduction of nearly 5 perplexity over the next best previously published result.  ", "image_folder": 39, "answer": "The model achieved a test perplexity of 18.7 on WikiText-103 and 23.02 on the Billion Word benchmark, outperforming previous work by 10.5 and nearly 5 perplexity points respectively.", "bert_scores": [0.9284290075302124, 0.9359068274497986, 0.9321529865264893], "rouge_scores": [{"rouge-1": {"r": 0.5862068965517241, "p": 0.6296296296296297, "f": 0.6071428521492348}, "rouge-2": {"r": 0.34375, "p": 0.36666666666666664, "f": 0.3548387046826223}, "rouge-l": {"r": 0.5862068965517241, "p": 0.6296296296296297, "f": 0.6071428521492348}}]}
{"id": 40, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Referring to the graph depicting Validation Perplexity Through Training, for the line representing the method with lower perplexity than Sinusoidal with L=2048 across all measured training times, what is the motivation behind using adaptive input embeddings in this specific neural language model?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 40, "answer": "The motivation behind using adaptive input embeddings is to reduce the number of parameters, which allows for capacity to be reallocated for other model components, thus enhancing performance as reflected by the lower perplexity of ALiBi L=1024 compared to Sinus", "bert_scores": [0.8845328092575073, 0.9491811394691467, 0.9157173037528992], "rouge_scores": [{"rouge-1": {"r": 0.7619047619047619, "p": 0.45714285714285713, "f": 0.5714285667410715}, "rouge-2": {"r": 0.43478260869565216, "p": 0.2564102564102564, "f": 0.3225806404942768}, "rouge-l": {"r": 0.7619047619047619, "p": 0.45714285714285713, "f": 0.5714285667410715}}]}
{"id": 41, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: Referring to the graph depicting Validation Perplexity Through Training, which line represents a method with lower perplexity than Sinusoidal when L=2048 across all measured training times, and how does this model assign words to clusters, influencing the dimensionality of word embeddings as described in the paper?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 41, "answer": "The ALiBi line represents a method with lower perplexity than Sinusoidal when L=2048. This model, as described in the paper, assigns words to clusters based on frequency, reducing the dimensionality of less frequent word embeddings while projecting", "bert_scores": [0.8432753682136536, 0.8512483835220337, 0.847243070602417], "rouge_scores": [{"rouge-1": {"r": 0.28125, "p": 0.25, "f": 0.26470587737024226}, "rouge-2": {"r": 0.10810810810810811, "p": 0.1111111111111111, "f": 0.1095890360968289}, "rouge-l": {"r": 0.25, "p": 0.2222222222222222, "f": 0.23529411266435996}}]}
{"id": 42, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by a line on the graph that shows a lower perplexity than Sinusoidal with L=2048 across all measured training times, what approach is used for weight sharing between input and output layers in the context of adaptive embeddings, and under what conditions does this approach improve performance?\nText Context: \n \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   ", "image_folder": 42, "answer": "The method with lower perplexity than Sinusoidal with L=2048 is Adaptive Input (ALiBi) with L=1024. This approach uses weight sharing between the input and output layers, sharing both the adaptive embeddings' parameters and projections", "bert_scores": [0.8401454091072083, 0.8687905073165894, 0.8542279005050659], "rouge_scores": [{"rouge-1": {"r": 0.3225806451612903, "p": 0.3448275862068966, "f": 0.33333332833888896}, "rouge-2": {"r": 0.02702702702702703, "p": 0.030303030303030304, "f": 0.02857142358775597}, "rouge-l": {"r": 0.25806451612903225, "p": 0.27586206896551724, "f": 0.2666666616722223}}]}
{"id": 43, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method that is represented by a line with lower perplexity than Sinusoidal with L=2048 across all measured training times, why did the authors choose to apply dropout to the tail projection in the adaptive softmax, and on which dataset did this regularization technique improve performance?\nText Context: \n We also found that adaptive softmax can benefit from additional regularization of rare words. Adaptive softmax first projects the model output to the dimension of a particular cluster and then computes a dot product with the respective word embeddings. We add dropout to the output of the first projection for all clusters, except for the head. This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on \\wiki{} (Table~\\ref{tab:adaptive}).  However, we found that adding dropout in this way is not helpful for larger datasets such as \\gbw{}. Unfortunately, a standard softmax over 800K words is not tractable and we were unable to make a comparison. It may be possible to achieve better results by tuning dropout for each band of the tail and we leave this for future work.  \\begin{table} \\centering \\begin{tabular}{crr} \\toprule & \\thead{Tail\\\\ dropout} & \\thead{Validation\\\\ perplexity} \\\\ \\midrule Softmax (\\sm{}) & N/A & 23.87 \\\\ Adaptive (\\adp{}) & 0.0 & 24.74 \\\\ Adaptive (\\adp{}) & 0.2 & 21.23 \\\\ \\bottomrule \\end{tabular} \\caption{Perplexity on \\wiki{} when regularizing rare words in adaptive softmax.} \\label{tab:adaptive} \\end{table}   ", "image_folder": 43, "answer": "The authors applied dropout to the tail projection in the adaptive softmax to regularize rare words, improving performance on the \\wiki{} dataset, as shown by the lower perplexity in the table and blue line in the graph.", "bert_scores": [0.9095377922058105, 0.9588516354560852, 0.9335439801216125], "rouge_scores": [{"rouge-1": {"r": 0.6818181818181818, "p": 0.5172413793103449, "f": 0.5882352892118416}, "rouge-2": {"r": 0.6666666666666666, "p": 0.47058823529411764, "f": 0.5517241330796672}, "rouge-l": {"r": 0.6818181818181818, "p": 0.5172413793103449, "f": 0.5882352892118416}}]}
{"id": 44, "prompt": "Answer the following question using both the given text and image context. Remember to limit your response within 50 words and do not use any knowledge not given in the context.\nQuestion: For the method represented by a line with lower perplexity than Sinusoidal with L=2048 across all measured training times in the Validation Perplexity Through Training graph, what are the test perplexity results achieved by this method on the Billion Word benchmark and the WikiText-103 benchmark compared to previously published work?\nText Context: \n Language modeling is a basic task in natural language processing, with many applications such as speech recognition \\citep{arisoy:2012:wfnlm} and statistical machine translation \\citep{schwenk:2012:wfnlm,vaswani:2013:emnlp,baltescu2014pragmatic}. Recently, much progress has been made by neural methods \\citep{bengio:2003:jmlr,mikolov:2010:interspeech} based on LSTMs \\citep{jozefowicz2016lm}, gated convolutional networks \\citep{dauphin2017convlm} and self-attentional networks \\citep{alrfou2018chartrans}.  There are different choices for the basic unit we wish to model, including full words \\citep{bengio:2003:jmlr}, characters for the input \\citep{kim2016character}, or also the output \\citep{merity2018lm} as well as sub-words \\citep{buckman2018taacl,mielke2018spell}. Word-based models are particularly challenging since computing probabilities for all 800K words of the \\gbw{} benchmark is still a substantial part of the overall computation \\citep{chen2016strategies}.  A popular approach to lower the computational burden is to structure the output vocabulary so that not all probabilities need to be computed. The hierarchical softmax does this by introducing latent variables or clusters to simplify normalization \\citep{goodman:2001:icassp,morin:2005:aistats,mikolov:2011:icassp}. This has been further improved by the \\emph{adaptive softmax} which introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words \\citep{grave2017icml}.  In this paper, we introduce \\emph{adaptive input embeddings} which extend the adaptive softmax to input word representations. This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words. For a competitive setup on the \\gbw{} benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23\\% while achieving higher accuracy over fixed size embeddings. When the adaptive input representations are tied with an adaptive softmax in the output, then the number of parameters is reduced by a total of 61\\%.  Our experiments compare models based on word inputs, character inputs, as well as sub-word units using a self-attention architecture \\citep{vaswani2017transformer}. We show that models with adaptive word representations can outperform very strong character-based models while training more than twice as fast. We also substantially improve adaptive softmax by introducing additional dropout regularization in the tail projection. On the \\wiki{} benchmark we achieve a perplexity of 18.7, a reduction of 10.5 perplexity over the previously best published result. On the larger \\gbw{} benchmark our best model with adaptive input embeddings achieves 23.02 perplexity, a reduction of nearly 5 perplexity over the next best previously published result.  ", "image_folder": 44, "answer": "The method represented by the line with lower perplexity than Sinusoidal with L=2048 achieves a perplexity of 18.7 on WikiText-103, reducing 10.5 perplexity over previous work, and 23.02", "bert_scores": [0.8857436180114746, 0.8949385285377502, 0.8903173804283142], "rouge_scores": [{"rouge-1": {"r": 0.3448275862068966, "p": 0.35714285714285715, "f": 0.35087718798399514}, "rouge-2": {"r": 0.09375, "p": 0.1, "f": 0.09677418855359028}, "rouge-l": {"r": 0.3103448275862069, "p": 0.32142857142857145, "f": 0.31578946868574953}}]}
