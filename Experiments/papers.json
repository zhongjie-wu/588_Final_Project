{
    "2106.03143": {
        "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings",
        "abstract": "Without positional information, attention-based Transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed Transformer models with positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences longer than seen at training time. Relative positions are more robust to input length change, but are more complex to implement and yield inferior model throughput due to extra computational and memory costs. In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, image and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters.",
        "tldr": "This paper proposes an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute and relative positions and leads to better generalization performance as well as increased stability with respect to training hyper-parameters.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "Transformers have been shown to be highly effective on problems involving sequential modeling, such as machine translation (MT)~\\cite{vaswani2017attention} and natural language processing (NLP)~\\cite{devlin2019bert,roller2020recipes,brown2020language}. Following its success on these tasks, the Transformer architecture raised immediate interest in other domains: automatic speech recognition (ASR)~\\cite{dong2018speech,gulati2020conformer}, music generation~\\cite{huang2018music}, object detection~\\cite{carion2020end}, and finally image recognition~\\cite{dosovitskiy2020image,touvron2020training} and video understanding~\\cite{bertasius2021space}.  Two major components of the Transformer are the attention mechanism~\\cite{bahdanau2014neural,vaswani2017attention} and the positional encoding~\\cite{vaswani2017attention,shaw2018self,huang2018music,dai2019transformer}. Without the latter, vanilla attention Transformers are invariant with respect to input tokens permutations (making ``cat eats fish'' and ``fish eats cat'' identical to the model). In the original Transformer publication, sinusoidal positional encoding was introduced~\\cite{vaswani2017attention}. Token positions were encoded in an absolute manner, which was sufficient to achieve state-of-the-art performance in numerous tasks. However, performance issues were later observed when dealing with sequences of length not seen at training time~\\cite{huang2018music,dai2019transformer,zhou2019improving,mohamed2019transformers,rosendahl2019analysis,dufter2021position}. For most applications relative positions between tokens are more relevant than absolute ones. A number of approaches were thus proposed to encode relative positions in an explicit form~\\cite{shaw2018self,dai2019transformer,huang2018music,rosendahl2019analysis,huang2020improve}, leading to improvements in modeling long sequences. However, all these approaches focus on modifying the attention mechanism and suffer from additional computational and memory costs~\\cite{zhang2020pushing}. Relative positional encoding is also notably hard to implement efficiently for multidimensional case, and recent advances in Transformer models for computer vision~\\cite{carion2020end,dosovitskiy2020image,touvron2020training} still rely on learnable absolute positional encoding.  Instead of changing the Transformer attention mechanism, we propose to improve absolute sinusoidal positional encodings in two ways: a) instead of discrete positions, rely on continuous ones, which better match the continuous nature of image, sound or video data; b) preserve some information about relative token positions, via a specific augmentation approach for positional embeddings during training. We empirically evaluate our approach, dubbed continuous augmented positional embedding (CAPE), with recent state-of-the-art models in several application domains. We study generalization properties and introduce unique features unlocked by CAPE. The main contributions of this work are: \\begin{itemize} \\item new augmented continuous positional embedding (CAPE), which encodes some relative position information in a computationally efficient way, and improves generalization performance compared to other positional embeddings across a variety of domains: machine translation, image and speech recognition; \\item a single vision Transformer (UniViT) trained with CAPE on the mix of different resolutions: it outperforms each single-resolution baseline, generalizes better to unseen resolutions and can naturally process images of any size; \\item new CAPE-based adaptive training scheme for ASR that eliminates need for padding. \\end{itemize}  "
            },
            {
                "section_name": "Related Works_2",
                "paragraphs": "  Since the appearance of Transformers, many works have investigated ways to encode positional information. A detailed analysis of various positional embeddings is available for BERT architecture~\\cite{wang2021position}, where authors empirically relate properties of positional embeddings to performance on downstream NLP tasks. A recent study~\\cite{ke2020rethinking} (also focuses on BERT) highlights the negative impact of spurious correlations between word and positional embeddings, and proposes to explicitly disentangle the contribution of positional and content embeddings in the attention mechanism. In contrast, our approach implicitly enforces this disentanglement by leveraging augmentation.  Systematic studies of positional embeddings in audio domain are scarce. Several ways to encode relative positions for Transformer-based speech recognition are compared in~\\cite{wang2020transformer}. Experiments show that absolute sinusoidal positional embeddings work no better than stacking consecutive frames at each time position (a particular form of convolution). We provide a more thorough evaluation of positional embeddings in ASR, over multiple datasets. We also show that embeddings obtained from a one-layer convolutional frontend benefits from adding positional information.  Transformers for computer vision applications are still in their early days, and most works rely on learnable absolute positional embeddings only \\cite{dosovitskiy2020image,touvron2020training,bertasius2021space,arnab2021vivit}. Several recent works complement the Transformer architecture with convolutional layers to induce a spacial relationship between tokens~\\cite{graham2021levit,chu2021conditional,wu2021cvt}. As we discuss later, this restricts flexibility compared to pure attention-based models. The work~\\cite{graham2021levit} suggests injecting learnable attention biases as an alternative mechanism to positional encoding. Evaluation of several positional encodings and their corresponding generalization has been done in a study \\cite{chu2021conditional} which is in line with our work. Convolutional elements were introduced in the Transformer architecture, leading to better generalization properties. In contrast, our experiments demonstrate that generalization can be achieved without architecture modification or convolutional inductive bias. Concerning video understanding, an evaluation of the impact of positional encoding was done in~\\cite{neimark2021video}: according to the results, positional encoding-free architectures perform best. Other work~\\cite{bertasius2021space} reports that adding absolute positional embeddings improves models performance, but contribution of encoding space and time vary between datasets.  As a summary, many positional embeddings variants were previously introduced, often modality-specific. In our cross-modal study we focus on generalization properties of popular and widely used embeddings, and improve on absolute sinusoidal positional embedding, leading to a flexible Transformer architecture, with great generalization properties across a number of different tasks.  The closest idea to our work is augmentation of positions in Universal Transformer~\\cite{dehghani2018universal} where \\textit{discrete} global shifts are applied to synthetic tasks for encoder-decoder Transformer models. In our work, we introduce \\textit{continuous} augmentations, not \\textit{discrete}, which are more natural for continuous modalities like images and speech where CAPE benefits most and which are not discussed in~\\cite{dehghani2018universal}. In addition to global shifts our augmentations include global scaling and local shifts, and we also synchronize augmentations between encoder and decoder (Section~\\ref{sec:mt}).  "
            },
            {
                "section_name": "Theoretical Analysis of Absolute Sinusoidal Positional Embeddings_3",
                "paragraphs": " Originally absolute positional $K$-dimensional embeddings were introduced in~\\cite{vaswani2017attention} as \\begin{equation} E_{2k}(n)     = \\cos{\\omega_k n} \\qquad E_{2k + 1}(n) = \\sin{\\omega_k n} \\qquad \\omega_k = 10000^{-2k/K} \\qquad n\\in\\mathbb{Z}^+ \\label{eq:vashwani} \\end{equation}  with $k=1,2,\\dots, K/2$ enumerating components for a token at position $n$. For simplicity of analysis, we rewrite Eq.~(\\ref{eq:vashwani}) as a complex-valued embeddings\\footnote{Work~\\cite{Wang2020Encoding} introduces general complex-valued embeddings as continuous word functions over a position.} with half the number of components: $$ \\{\\mathbf{E}(n)\\}_k = E_{k}(n) = e^{i \\omega_k n} $$  This definition can be rewritten in a recursive manner, by introducing a unitary operator $S$: \\begin{equation} \\label{eq-unitary-operator} \\mathbf{E}(n+1) = S\\,\\mathbf{E}(n)\\,, \\qquad \\textrm{with} \\qquad \\{S\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k} \\end{equation}  Therefore, the embedding at position $n$ contains sufficient information to compute the embedding of the next or previous positions, as applying $S^m$ ($m \\in \\mathbb{Z}$) performs a relative shift: $\\mathbf{E}(n+m) = S^m\\,\\mathbf{E}(n)$. Variation in $\\omega_i$ ensures that no positions $<10^4$ are assigned similar embeddings. Before introducing augmentations of positional embeddings, we revisit positions parametrizations for different modalities.  \\paragraph{Positional encoding for text} For natural language processing it is common to split text into words, letters, syllables and other sub-units. Original absolute sinusoidal positional embeddings enumerate these sub-units by their ordinal number $n$, a common choice that we follow.  \\paragraph{Positional encoding for images} In a framework where patch and image sizes may vary, we find that enumerating patches is not appropriate, as positional embeddings may greatly differ for different scales of an image, leading to generalization issues. In that perspective, we consider scaled coordinates $x$ and $y$ that span interval $[-1, +1]$.\\footnote{Authors of~\\cite{carion2020end} also perform coordinates scaling but it spans interval $[0, 1]$.} While previous works~\\cite{dosovitskiy2020image,touvron2020training} relied on learnable absolute positional embedding, we introduce the following $K$-dimensional absolute sinusoidal positional embedding defined for each position $(x, y) \\in \\mathbb{R}^2$: \\begin{equation}\\label{eq:sin2d1} E_{2k}(x, y)   = \\cos \\pi (w_{k,x} x + w_{k,y} y) \\qquad E_{2k+1}(x, y) = \\sin \\pi (w_{k,x} x + w_{k,y} y) \\end{equation} \\begin{equation}\\label{eq:sin2d2} w_{k,x} = 10^{2k/K} \\cos{k} \\qquad w_{k,y} = 10^{2k/K} \\sin{k} \\end{equation} Following Eq.~(\\ref{eq-unitary-operator}), this corresponds to introducing two commuting unitary operators $S_x$ and $S_y$, for each unit shift in either direction on the plane. The choice of $w_{k,x}$ and $w_{k,y}$ is kept simple and deterministic, while giving no preference to any direction on the plane and providing angle of ``hatching'' (all components have different angle and angles uniformly cover possible directions), see Figure~\\ref{fig:emb_visualization}. Furthermore, embeddings defined by Eq.~(\\ref{eq:sin2d1}-\\ref{eq:sin2d2}) allow attending to a specific small region around a point on the image without emphasizing points with only same $x$ or only same~$y$ while we expect this artifact to contribute in case of concatenation of 1D embeddings for each axis~\\cite{chu2021conditional,dosovitskiy2020image,bello2019attention,parmar2018image}.  \\paragraph{Positional encoding for sound} We propose to tie positional embeddings to timestamps in seconds. The embedding for a frame centered at $t$ seconds is given by: $$ E_{2k}(t) = \\cos{\\omega_k t} \\qquad E_{2k + 1}(t) = \\sin{\\omega_k t} \\qquad \\omega_k = 30 \\times 10000^{-2k/K} $$ The choice of $\\omega_k$ corresponds to the scaled version of Eq.~(\\ref{eq:vashwani}) and ensures that queries with 30ms specificity are possible even with minutes-long audio fragments.  Two main factors contribute to the choice of base in the exponentiation of $w_k$ and $w_{k,x}$, $w_{k,y}$ frequencies: i) how precise the location should be for encoding (e.g. there is no need to encode position in audio much more precisely than one syllable takes); ii) longest reasonable ``length'' that is expected in the data. CAPE could be parametrized by precision and ``length'' parameters, but we believe that for all practical cases provided choices are reasonable.   "
            },
            {
                "section_name": "Continuous Augmented Positional Embeddings (CAPE)_4",
                "paragraphs": " Absolute sinusoidal positional embeddings lack regularization, leading to ``in-domain'' generalization issues as the model may start learning spurious correlations. For applications where inference input sizes may be significantly different than the training ones, ``out-of-domain'' issues may also arise, as positions rarely observed at training may lead to improper inference predictions. To ensure that the model does not learn spurious correlations between content and position, we introduce the following augmentations for absolute sinusoidal positional embeddings at training time, see Figure~\\ref{fig:cape}.   \\begin{figure}[t!] \\centering \\includegraphics[width=0.75\\textwidth]{figures/cape_scheme.png} \\caption{Example of CAPE's transformations for an image: patches positions are scaled to $[-1, 1]\\times [-1, 1]$; random global shift, local shift, and global scaling are then applied to the grid of positions. \\label{fig:cape} } \\end{figure}  \\paragraph{Global shift} Transform every embedding in a sequence using $S^\\Delta$ operator with a global random shift from uniform zero-mean distribution $\\Delta \\sim \\mathcal{U}(-\\Delta_{max}, \\Delta_{max})$: $$ \\mathbf{E}'(n) = S^\\Delta \\mathbf{E}(n) \\qquad \\{S^\\Delta\\,\\mathbf{X}\\}_k = X_k e^{i \\omega_k \\Delta } \\qquad \\Delta \\in \\mathbb{R} $$ This modification hides the absolute positional information, but relative relations, see Eq.~(\\ref{eq-unitary-operator}), between embeddings still hold. This transformation can be rewritten as augmenting positions by a random shift before encoding with $\\sin$ and $\\cos$: \\begin{equation} n'_i \\leftarrow n_i + \\Delta \\qquad \\quad x'_i \\leftarrow x_i + \\Delta_x, \\;\\; y'_i \\leftarrow y_i + \\Delta_y \\qquad \\qquad t'_i \\leftarrow t_i + \\Delta \\label{cape-global-shift} \\end{equation}  \\paragraph{Local shift} To further increase augmentation and prevent capturing spontaneous correlations, we additionally introduce local shifts from uniform zero-mean distribution $\\epsilon_i \\sim \\mathcal{U}(-\\epsilon_{max}, \\epsilon_{max})$ \\begin{equation} n'_i \\leftarrow n_i + \\epsilon_i        \\qquad \\quad x'_i \\leftarrow x_i + \\epsilon_{x,i},   \\;\\; y'_i \\leftarrow y_i + \\epsilon_{y,i}    \\qquad \\qquad t'_i \\leftarrow t_i + \\epsilon_i \\label{cape-local-shift} \\end{equation} \\paragraph{Global scaling} To prevent distances memorization, we also introduce random global scale $\\lambda$ from $\\log \\lambda \\sim \\mathcal{U}(- \\log \\lambda_{max}, \\, \\log \\lambda_{max})$ \\begin{equation} n'_i \\leftarrow \\lambda n_i     \\qquad \\qquad \\quad x'_i \\leftarrow \\lambda x_i,    \\;\\;          \\quad y'_i \\leftarrow \\lambda y_i     \\qquad \\qquad \\quad \\quad t'_i \\leftarrow \\lambda t_i                   \\quad \\label{cape-scale} \\end{equation}  At training time, computing our continuous augmented positional embedding is performed through four steps: i)~mean-normalization of positions (extract mean of sequence positions), ii)~global shift Eq.~(\\ref{cape-global-shift}), iii)~local shift Eq.~(\\ref{cape-local-shift}), and iv)~global scaling Eq.~(\\ref{cape-scale}). At inference time, only mean-normalization of positions is performed. The reference implementation can be found in Appendix~\\ref{app:impl}.  "
            },
            {
                "section_name": "Experiments_5",
                "paragraphs": " "
            },
            {
                "section_name": "Image Recognition_1",
                "paragraphs": " We evaluate CAPE embedding empirically with a recently proposed Vision Transformer (ViT)~\\cite{dosovitskiy2020image,touvron2020training} for image recognition. These works rely on learnable absolute positional embedding (\\textit{abspos}) for both class token and patches, and train ViT models on $224^2$ images\\footnote{In the following, we denote the size and resolution $N\\times N$ as $N^2$.} with $16^2$ patches. To further improve model quality, \\cite{touvron2020training} performs fine-tuning on images of higher resolution $384^2$. The grid of positional embeddings is then upsampled.  \\paragraph{Data and ViT Models} All experiments are performed on the ImageNet~\\cite{deng2009imagenet,russakovsky2015imagenet} dataset. We report top-1 and top-5 accuracies on ImageNet validation set and ImageNet-v2-\\{a,b,c\\}~\\cite{recht2019imagenet} test sets. The same convolution-free architecture, identical to the one proposed by~\\cite{dosovitskiy2020image} (ViT-B) and used by~\\cite{touvron2020training} (referred as DeiT-B), is chosen for all experiments. A ViT-B/DeiT-B baseline is trained with \\textit{abspos} on $224^2$ images, carefully following Section~6 from~\\cite{touvron2020training}. The \\textit{exact same training configuration} is used for models with other positional embeddings: \\textit{only} positional embedding is changed. We evaluate both proposed absolute sinusoidal positional embedding (\\textit{sinpos}), Eq.~(\\ref{eq:sin2d1}-\\ref{eq:sin2d2}), and CAPE ($\\Delta_{max}=0.5$, $\\epsilon_{max}=1/N$ and $\\lambda_{max}=1.4$). As a control experiment we also train a model without any positional embedding (\\textit{nopos}), which can be interpreted as a 'bag of \\sout{words} patches', as no patch position information is available. We also train models with different positional embeddings on either $160^2$ or $384^2$ images. The whole training configuration remains the same as for training on $224^2$ images, except for the positional embedding. All models trained on $224^2$ images we additionally fine-tune on images of higher resolution $384^2$, following~\\cite{touvron2020training}.  \\paragraph{Evaluation} To study generalization when image sizes vary, we evaluate all models on different resolutions ($160^2$, $224^2$, $384^2$ and $672^2$) by resizing all images in validation and test sets. When evaluating on resolutions different from the training one, bicubic interpolation is applied to \\textit{abspos} embeddings\\footnote{Interpolation is not applied to the class token embedding.} as was justified in~\\cite{touvron2020training}. In contrast, \\textit{sinpos} and CAPE approaches can ingest any image resolution, thanks to the continuous nature of their positional embeddings.  \\subsubsection{Results}  \\begin{figure}[t!] \\centering \\includegraphics[width=0.95\\textwidth]{figures/vit_base10.pdf} \\caption{Top-1 accuracy on ImageNet and ImageNet-v2 for ViT models trained with different positional embeddings on $224^2$ resolution (solid) and further fine-tuned on $384^2$ (dashed, ``+ft''). Insets focus on higher accuracies. The full list of top-1 and top-5 accuracies can be found in Appendix~\\ref{app:vision:tech}, Tables~\\ref{tab:vision_all_top1} and~\\ref{tab:vision_all_top5}. \\label{fig:vision:vit_main} } \\end{figure}  In Figure~\\ref{fig:vision:vit_main} we compare generalization performance of models trained with different positional embeddings on $224^2$ images (solid). Both proposed \\textit{sinpos} and CAPE approaches perform at least as well, if not better, than the \\textit{abspos} approach on the same-as-train resolution. When performing inference on resolutions different than the training one, CAPE performs best, notably outperforming \\textit{abspos} on high ($672^2$) and low ($160^2$) resolutions up to 25\\% and 2\\%, respectively. On $160^2$ and $384^2$ resolutions CAPE trained on $224^2$ resolution performs similar to \\textit{abspos} trained on corresponding $160^2$ or $384^2$ inference resolutions (the latter results being reported in Figure~\\ref{fig:vision:mix_sizes}). This confirms good generalization properties of CAPE on image resolutions unseen at training time.  \\textit{Abspos} fine-tuned on a higher resolution ($384^2$) improves in accuracy for both $384^2$ and $672^2$ resolutions, while degrading on lower ones (original $224^2$ and lowest $160^2$), as shown in Figure~\\ref{fig:vision:vit_main} (dashed). \\textit{Sinpos} and CAPE fine-tuned on $384^2$ resolution outperform \\textit{abspos} by 0.3-0.4\\%, thanks to a better fine-tuning starting point after resolution change. In that setting, \\textit{sinpos} and CAPE keep better generalization performance on nearby resolutions ($224^2$ and $672^2$). While being seriously impacted on the $160^2$ resolution, CAPE still outperforms others by 12-25\\%. Comparison of ViT models trained with either \\textit{abspos} or CAPE on each particular resolution is shown in Figure~\\ref{fig:vision:mix_sizes}: CAPE outperforms \\textit{abspos} on a specific training resolution, and generalizes better to unseen resolutions.  \\paragraph{No Positional Embedding} With \\textit{nopos} model, we confirm \\cite{dosovitskiy2020image}'s observation that positional embedding does not have a critical importance for ImageNet classification (see Figure~\\ref{fig:vision:vit_main}). \\textit{Nopos} model's simplicity and generalization abilities make it a nice baseline for positional embedding study in computer vision. It has generalization accuracy similar to \\textit{abspos} on low $160^2$ and high $672^2$ resolutions, while CAPE outperforms \\textit{nopos} across the board. It is likely that \\textit{abspos} suffers from the embedding interpolation step on extreme resolutions. In contrast, both \\textit{sinpos} and CAPE have the advantage to naturally support variation in resolutions.  \\subsubsection{UniViT: Training Universal Transformer on Different Resolutions}  \\begin{figure}[t!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/vit_size_best.pdf} \\caption{Top-1 accuracy on ImageNet and ImageNet-v2 for ViT models with either \\textit{abspos} or CAPE trained on each particular resolution and UniViT model trained on the mixture of resolutions.\\label{fig:vision:mix_sizes}} \\end{figure}   \\begin{figure} \\CenterFloatBoxes \\begin{floatrow} \\ffigbox{ \\includegraphics[width=0.4\\textwidth]{figures/vit_speed_final.pdf} }{ \\caption{Accuracy with respect to throughput on ImageNet at inference time under variation of image resolution. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively. \\label{fig:vision:speed}} } \\killfloatstyle \\ttabbox {\\caption{ Test time augmentation (TTA) results on ImageNet when predictions on resolutions $r^2$, $(r+32)^2$ and $(r-32)^2$ are combined. \\textit{Sinpos} and CAPE are trained on $224^2$ resolution.\\label{tab:vision_tta}} }{ \\resizebox{0.9\\linewidth}{!}{ \\begin{tabular}{@{}cccccc@{}} \\toprule \\multirow{2}{*}{Model} & \\multirow{2}{*}{$r$} & \\multicolumn{2}{c}{Top-1 (\\%)} & \\multicolumn{2}{c}{Top-5 (\\%)} \\\\ \\cmidrule(lr){3-4} \\cmidrule(lr){5-6} & & -TTA & +TTA & -TTA & + TTA \\\\ \\midrule  \\\\ \\\\ sinpos & 224 & 81.32 & 81.47 & 95.44 & 95.54 \\\\ CAPE & 224 & 81.01 & 81.34 & 95.18 & 95.49  \\\\ \\midrule \\midrule UniViT, sinpos & 224 & 80.82 & 81.34 & 95.40 & 95.57  \\\\ UniViT, CAPE & 224 & 81.26 & 81.64 & 95.56 & 95.71 \\\\ \\midrule UniViT, sinpos & 384 & 82.31 & 82.44 & 96.04 & 96.14 \\\\ UniViT, CAPE & 384 & 82.55 & 82.72 & 96.18 & 96.22 \\\\ \\bottomrule \\end{tabular} } } \\end{floatrow} \\end{figure}  As CAPE-based models can handle any image resolution, we propose a single universal ViT model, called UniViT, which is trained on images of different resolutions: during training we resize all images in a batch to a randomly chosen size, uniformly sampled in the range $[128,320]^2$ with a step of $32$. For experiments with UniViT training configuration remains the same as for ViT. Because of training on the images of different resolution UniViT training time is only 1.1x longer than ViT trained on $224^2$ resolution. In Figure~\\ref{fig:vision:mix_sizes} we compare UniViT model trained with CAPE ($\\lambda=1$) against different ViT models trained on each particular resolution: UniViT outperforms single-resolution ViT models for any given resolution and, moreover, generalizes to non-training resolutions well.  Image resolution directly impacts throughput: computational complexity of attention is $O(N^4)$ for a $N\\times N$ image. In Figure~\\ref{fig:vision:speed}, we show UniViT with CAPE throughput and accuracy with respect to input image resolution.  On $96^2$ resolution UniViT with CAPE handles throughput and accuracy similar to ``tiny'' vanilla ViT, while ``small'' UniViT with CAPE has 4\\% higher accuracy (with identical throughput) on resolution $160^2$ and 1.4x higher throughput (with identical accuracy) on resolution $128^2$. Thus, UniViT unlocks the possibility of dynamically adjusting throughput of a model in a production regime under heavy loads, a practical alternative to improving model throughput at inference time via decreasing model size.  We further improve UniViT with CAPE accuracy by resizing each image to its optimal resolution at inference, as shown in Appendix~\\ref{app:vision:resize} Table~\\ref{tab:vision:own_size_eval}. We split ImageNet validation images into 8 bins, according to their size. By selecting an optimal resizing strategy in each bin we are able to improve top-1 accuracy to 82.92\\% (in comparison, the model has 81.26\\% on $224^2$ and 82.55\\% on $384^2$).  \\subsubsection{Resizing as Test Time Augmentation (TTA)}  As both \\textit{sinpos}- and CAPE-based models handle well different image resolutions, we propose to perform test time resolution augmentation when evaluating a single model. For TTA we average model's logits evaluated on three resolutions for the same image: $r^2$, $(r-32)^2$ and $(r+32)^2$, where $r$ is either 224 or 384. As shown in Table~\\ref{tab:vision_tta}, ViT and UniViT models trained with either \\textit{sinpos} or CAPE embeddings get 0.2\\%-0.5\\% top-1 accuracy boost with this test time augmentation.  "
            },
            {
                "section_name": "Automatic Speech Recognition (ASR)_2",
                "paragraphs": " Recently it was shown that Transformer~\\cite{vaswani2017attention} architectures are state-of-the-art on different public benchmarks for ASR~\\cite{zeyer2019comparison,mohamed2019transformers,likhomanenko2020rethinking,chan2021speechstew}.  \\paragraph{Data} We evaluate our models on several English speech datasets, both on in-domain data and out-of-domain data. We also analyze our models generalization to long sequences. We consider two standard training benchmarks: Wall Street Journal (WSJ)~\\cite{garofolo1993csr,linguistic1994csr,woodland1994large}, read speech with 81.5h of training data, and TED-LIUM v3 (TL)~\\cite{hernandez2018ted}, oratory speech with 452h of training data. Besides these datasets we use two other sets for evaluation only: i) LibriSpeech (LS)~\\cite{panayotov2015librispeech}, read speech from audiobook recordings (we use only test sets with clean, \\emph{test-clean}, and noisy, \\emph{test-other}, speech); ii) Robust Video (RV), our in-house English video dataset, which is sampled from public social media videos and aggregated and de-identified before transcription; these videos contain a diverse range of speakers, accents, topics, and acoustic conditions making ASR difficult; the test sets are composed of \\emph{clean} and \\emph{noisy} subsets. Details on data and its statistics can be found in Appendix~\\ref{app:asr:data}.  \\paragraph{Evaluation} To evaluate our acoustic models on sequence lengths not seen at training time, we split all evaluation utterances by their duration $T$ into the following bins: $T<10$s, $T\\in[10-15)$s, $T\\in[15, 20)$s and $T>=20$s. Our performance metric is word error rate (WER) (no language model is involved), reported for each sequence length bin and for the entire evaluation dataset. For RV data a hand-crafted segmentation is available, allowing us to evaluate on the exact same data, but segmented in different ways. More precisely, for RV data we prepare 8 sets where audios have the following respective durations: $T=10$, $15$, $20$, $25$, $30$, $35$, $40$, $45$s.  \\paragraph{Acoustic Model (AM) Training} All models are trained with Connectionist Temporal Classification~\\cite{graves2006connectionist}. SpecAugment~\\cite{park2019specaug} is used as data augmentation in training, and the network architecture follows~\\cite{likhomanenko2020rethinking}: the AM encoder is composed of a 1D convolution (kernel 7, stride 3) with a GLU activation and 36 4-heads Transformer layers~\\cite{vaswani2017attention}, finally followed by a linear layer which outputs a score for each target token. Our token set consists of 26 English alphabet letters, augmented with the apostrophe and a word boundary token (further details in Appendix~\\ref{app:asr:am}).  \\paragraph{Positional Embedding} As in vision experiments, we evaluate \\textit{nopos}, \\textit{sinpos}, \\textit{abspos} and CAPE-based models. In addition, we evaluate models trained with relative positional embeddings: in that case, no absolute positions are used, and learnable relative positional embeddings~\\cite{shaw2018self} (\\textit{relpos}) are trained in each Transformer layer. We follow~\\cite{likhomanenko2020rethinking} to train an AM baseline with~\\textit{relpos}. For models with other positional embeddings, the training \\textit{configuration remains identical}. \\textit{Abspos} $\\{\\mathbf{E}(t)\\}_{t=1}^N$ is set to cover 13.8s of context. At training/evaluation time for the longer sequences we define \\textit{abspos} as $\\mathbf{E}(t) \\equiv  \\mathbf{E}{(t \\mod N)}$ for $t > N$. This extrapolation at training time leaves a chance to the acoustic model to generalize to unseen (longer) sequence lengths. \\textit{Relpos} spans a large context, 26.8s to the left/right. CAPE's global shift covers 60s, while a local shift is set to its maximum to preserve the frames order; $\\lambda_{max}=1.1$ and $\\lambda_{max}=2$ for WSJ and TL, respectively.  \\subsubsection{Results}  A model trained on WSJ with CAPE outperforms other positional embeddings on both public and RV data across different audio durations, as shown in Figure~\\ref{fig:speech:wsj}. A model trained on TL with CAPE outperforms \\textit{nopos} and \\textit{sinpos} on all data, outperforms \\textit{abspos} and \\textit{relpos} for audio longer than 20s, and behaves similarly on shorter durations, see Figure~\\ref{fig:speech:tl}. On RV data, CAPE-based models perform uniformly well on different audio durations, including long ones. In contrast, other embeddings-based models are seriously impacted when audio duration increases. Finally, CAPE does not have computational or parameters overheads compared to \\textit{relpos}.  \\paragraph{No Positional Embedding} As expected, \\textit{nopos} models (both WSJ and TL ones) perform similarly in WER across different audio durations. However, \\textit{nopos} TL model performs surprisingly well: it is competitive to positional embeddings-based models on public data. On RV data, \\textit{nopos} TL model outperforms all other models, except CAPE when $T>20$s. We perform ablations in Appendix~\\ref{app:asr:ablation} to show that key ingredients are CTC loss, sufficient model capacity, and large amount of data for this effect to occur.  \\begin{figure}[t!] \\centering \\includegraphics[width=0.86\\textwidth]{figures/wsj.pdf} \\caption{Word error rate for models trained on WSJ with different positional embeddings.\\label{fig:speech:wsj}} \\end{figure}  \\begin{figure}[t!] \\centering \\includegraphics[width=0.86\\textwidth]{figures/tl.pdf} \\caption{Word error rate for models trained on TED-LIUM v3 with different positional embeddings.\\label{fig:speech:tl}} \\end{figure}  \\paragraph{CAPE as Augmentation} CAPE can be viewed as a data augmentation performed on input data (positions), which regularizes the trained model. We demonstrate this by training on TL with either \\textit{sinpos} or CAPE and with/without SpecAugment (no other augmentations are used), see Figure~\\ref{fig:asr_aug}. Baseline \\textit{sinpos} without any augmentation performs the worst with a large gap. Including either CAPE or SpecAugment decreases the WER significantly by 4-5\\%: SpecAugment is more beneficial due to its domain relevance. Combining together CAPE and SpecAugment further decreases the WER by 2.5-3.5\\%, showing that augmentations are complementary to each other.  \\subsubsection{Padding-free ASR with CAPE and Variable STFT Hop Distance}  In ASR, when batching audios of different duration, one often relies on padding tokens. We propose instead to perform time stretching augmentation on all audios in the batch, such that they will have the same number of frames. We perform this augmentation by tuning the short-time Fourier Transform (STFT) hop distance when computing the audio features. Positions remain tied to the original audio timestamps. These models trained either on WSJ or TL show better WER across the board, compared to models trained with a padding approach, as shown in Appendix~\\ref{app:asr:hop} Figures~\\ref{fig:abl:wsj_hop} and~\\ref{fig:abl:tl_hop}, and improvements on RV are quite consistent. As before, CAPE-based models outperform \\textit{sinpos} models.  We found this padding-free approach convenient, as it \\textit{alleviates the implementation of special cases to handle padding tokens} in ASR models: e.g. any normalization should be aware which tokens are padding, otherwise normalization constants would depend on the amount of padding; any attention module should be aware which tokens it should not attend to. Moreover, for efficient computations and reducing padding tokens audio samples are often packed together via sorting by their duration; this reduces variability in the batches drastically. Our results demonstrate that with CAPE and padding-free approach we can mix samples of not-too-different lengths within a batch, providing better randomization and utilize all frames. While UniViT adjusts number of tokens by changing resolution, the STFT hop distance achieves the same for audio. By adjusting the hop distance during inference for padding-free ASR, we can achieve higher throughput with similar recognition quality, see Appendix~\\ref{app:asr:hop} Figure~\\ref{fig:abl:tl_hop_time}. However, very low resolution (e.g. 128x128) in images works well enough while speech recognition is far more sensitive to token sparsification as phoneme can last as little as 30-50ms.  \\begin{figure} \\CenterFloatBoxes \\begin{floatrow} \\ffigbox{ \\includegraphics[width=0.5\\textwidth]{figures/tl_aug.pdf} \\caption{Validation WER for models trained with different augmentations: ``baseline'' is a model with \\textit{sinpos}, ``+CAPE'' adds CAPE's global and local shifts, ``+SpecAug'' adds SpecAugment.\\label{fig:asr_aug}} } \\killfloatstyle \\ttabbox{ \\caption{WMT\u201914 BLEU score (3 runs avg.).\\label{tab:mt}} }{ \\resizebox{0.95\\linewidth}{!}{ \\begin{tabular}{@{}cccc@{}} \\toprule Model \\& Lang. & Embedding & Valid BLEU & Test BLEU \\\\ \\midrule \\multirow{3}{*}{6L-6L} & sinpos & 26.88$\\pm$0.05  & 27.66$\\pm$0.10 \\\\ & abspos & 26.68$\\pm$0.05 & 27.36$\\pm$0.06 \\\\ & relpos & 26.81$\\pm$0.16 & 27.92$\\pm$0.07 \\\\ \\multirow{2}{*}{DE} & CAPE, $\\Delta = 5$ & 26.86$\\pm$0.13 & 27.89$\\pm$0.07 \\\\ & CAPE, $\\Delta = 50$ & 27.09$\\pm$0.03 & 27.77$\\pm$0.16 \\\\ \\midrule \\midrule \\multirow{2}{*}{18L-18L} & sinpos & 27.09$\\pm$0.06 & 28.28$\\pm$0.28 \\\\ & abspos & 27.23$\\pm$0.02 & 28.26$\\pm$0.22 \\\\ DE & CAPE, $\\Delta = 10$ & 27.17$\\pm$0.10 & 28.44$\\pm$0.06 \\\\ \\midrule \\midrule \\multirow{3}{*}{6L-6L} & sinpos & 47.27$\\pm$0.03 & 41.13$\\pm$0.07 \\\\ & abspos & 47.22$\\pm$0.03 & 41.21$\\pm$0.04 \\\\ & relpos & 47.12$\\pm$0.03 & 41.33$\\pm$0.13 \\\\ \\multirow{2}{*}{FR} & CAPE, $\\Delta = 5$ & 47.22$\\pm$0.03 & 41.59$\\pm$0.03 \\\\ & CAPE, $\\Delta = 50$ & 47.14$\\pm$0.02 & 41.48$\\pm$0.10 \\\\ \\bottomrule \\end{tabular} } } \\end{floatrow} \\end{figure}  "
            },
            {
                "section_name": "Machine Translation (MT)_3",
                "paragraphs": "Our MT experiments follow the recent results with Transformers combined with a new initialization scheme (ADMIN)~\\cite{liu2019deep,liu2020very}. This approach allows to train very deep state-of-the-art Transformers for MT. We did not implement back-translation or other domain-specific augmentations.  \\paragraph{Data and Models Training} Experiments are conducted on standard WMT\u201914 English-French (FR)  and  English-German (DE) benchmarks. For both benchmarks we follow~\\cite{liu2019deep,liu2020very}: for FR we use a 40k subword vocabulary, and evaluate on the provided ``valid'' file for validation and newstest14 for test. On DE, we consider a 32K subword vocabulary, newstest2013 for validation, and newstest2014 for test. We reproduce results from~\\cite{liu2019deep,liu2020very} by training a~\\textit{sinpos}-based model with 6L-6L, 18L-18L for DE and 6L-6L for FR encoder-decoder layers with ADMIN. \\textit{Training configuration stays the same} for other positional embedding-based models, other than positional embeddings being either \\textit{abspos} (covers 1k tokens), \\textit{relpos} (learnable \\cite{shaw2018self}, covers max train content: 150 left/right tokens) or CAPE in both encoder and decoder layers. For CAPE to have approximate correspondence in positions of source and target sentences, we first scale positions of source language by a factor $\\alpha = \\frac{\\text{\\# tokens in target corpus}}{\\text{\\# tokens in source corpus}} \\in\\mathbb{R}$ so that positions interval is loosely matched between source and target sentences. For each training sample we then apply the same global shift and scale for source and target positions. Local shifts for source/target positions are independently sampled from $\\mathcal{U}(-0.5, 0.5)$. As no absolute positions are provided anymore, we prepend source sentences with a ``begin of sentence\" token to hint the decoder with a first position during both training and evaluation.  \\paragraph{Evaluation} We select the best checkpoint according to BLEU on the validation set, using a beam size 4 for DE and 5 for FR. Following convention, BLEU is computed by \\url{multi-bleu.perl} via the standardized tokenization of the publicly-accessible dataset. As WMT\u201914 test sets have limited length of sentences being same as train we artificially stack together sentences in test sets (source and target) to have 300+ tokens in each new target sample (3k original pairs of sentences are transformed into 270 pairs): during stacking dots on junctions are replaced with commas and first letters of the next stacked sentence are lower-cased to match one-sentence setup. On these test sets we compute log loss for each position: lower value shows that a model is more likely to predict a correct word at a particular position.  \\paragraph{Results}  \\begin{figure}[t!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/mt_logloss.pdf} \\caption{Average negative log loss across consecutive positions measured on test sets with stacked sentences for models with different positional embeddings: 6L-6L DE (left) and 6L-6L FR (right). \\label{fig:mt}} \\end{figure}  Comparison between models trained with different positional embeddings on WMT'14 benchmarks is shown in Table~\\ref{tab:mt}. CAPE outperforms \\textit{sinpos} and \\textit{abspos} on all settings, is similar to \\textit{relpos} on DE and outperforms it on FR. Figure~\\ref{fig:mt} shows that i) for positions covered in training (<150) all absolute positional embeddings behave similarly and outperform \\textit{relpos}; ii) for positions not seen during training (>200) \\textit{relpos} outperforms others. However, CAPE generalizes better than \\textit{abspos} and \\textit{sinpos} and, moreover, is able to generalize similar to \\textit{relpos} with more data (FR).  "
            },
            {
                "section_name": "Discussion and Conclusion_6",
                "paragraphs": " Encoding positional information is a key component of attention-based models. Poor generalization of absolute sinusoidal positional embeddings led to numerous works investigating ways to encode relevant positional information. Existing solutions are often modality-specific, non-trivial to implement and incur computational overhead. We demonstrated in different domains that existing positional embeddings may generalize poorly in certain conditions. We introduced a simple and efficient continuous augmented positional embedding, CAPE, which preserves some information about relative token positions. Thanks to its continuous nature, CAPE allows augmentations which were previously not possible. CAPE makes models more flexible both at training and inference. It generalizes well to input sizes not seen during training across a variety of domains. We expect emergence of new training and production pipelines that leverage the adjustable throughput property when tuning the input size: CAPE provides a relatively simple way of producing more efficient Transformer by down-sampling input (e.g for image and audio). Going further, CAPE-based architectures are free from baked-in restrictions on patches positions: these could overlap, or be sparse for example -- opportunities impossible for convolution-containing Transformers. In contrast to relative positional embeddings which modify attention mechanism, CAPE is ready to be used by novel attention mechanisms, such as ``linear''  Transformers~\\cite{tay2021long}. Finally, CAPE can be combined with relative positional embeddings like~\\cite{dai2019transformer} and~\\cite{su2021roformer} to limit over-fitting to exact relative positions.  \\paragraph{Limitations} CAPE applies only to attention-based models, and no testing was performed outside described modalities. From a representation perspective we demonstrated that CAPE is capable of providing relative positioning, however, a model should ``learn'' it. Thus, we can expect that relative positional embeddings should be beneficial in settings with small amount of data because of more appropriate inductive bias. Proposed UniViT model was tested only for image recognition task and further exploration of broader UniViT applicability to other tasks is a subject of future work.  "
            },
            {
                "section_name": "Acknowledgments_7",
                "paragraphs": " We would like to thank Mark Tygert and Edouard Grave for relevant references and helpful discussions, Paden Tomasello for English language editing.  \\bibliographystyle{plain} \\bibliography{mybib}   \\newpage \\appendix  "
            },
            {
                "section_name": "CAPE Implementation in Python_8",
                "paragraphs": " \\begin{lstlisting} import numpy as np  def augment_positions_1d( positions_1d: np.ndarray, mean_normalize: bool, augment: bool,    # True during training max_global_shift, # delta max max_local_shift,  # epsilon max max_scale,        # lambda max rng=np.random.RandomState(42) ): \"\"\" Takes original positions, returns modified ones. Can reuse sin/cos embedding from \"Attention is all you need\". Code handles NaNs is positions_1d input as if those correspond to pad tokens \"\"\" assert max_scale >= 1 batch_size, n_tokens = positions_1d.shape if mean_normalize: positions_1d -= np.nanmean(positions_1d, axis=1, keepdims=True) if augment: delta = rng.uniform(-max_global_shift, +max_global_shift, size=[batch_size, 1]) delta_local = rng.uniform(-max_local_shift, +max_local_shift, size=[batch_size, n_tokens]) log_lambdas = rng.uniform(-np.log(max_scale), +np.log(max_scale), size=[batch_size, 1]) new_positions = (positions_1d + delta + delta_local) * np.exp(log_lambdas) return new_positions else: return positions_1d  def CAPE_2d( n_patches: int,   # number of patches, default in ViT is 14 batch_size: int, augment: bool,    # True during training n_channels: int,  # embedding size for one patch max_global_shift, # delta max max_local_shift,  # epsilon max max_scale,        # lambda max rng=np.random.RandomState(42) ): \"\"\"Prepares grid of CAPE embeddings for provided grid size\"\"\" x = np.zeros([batch_size, n_patches, n_patches]) y = np.zeros([batch_size, n_patches, n_patches]) x += np.linspace(-1, 1, n_patches)[None, :, None] y += np.linspace(-1, 1, n_patches)[None, None, :] if augment: # global shift x += rng.uniform(-max_global_shift, +max_global_shift, size=[batch_size, 1, 1]) y += rng.uniform(-max_global_shift, +max_global_shift, size=[batch_size, 1, 1]) # local shift x += rng.uniform(-max_local_shift, +max_local_shift, size=x.shape) y += rng.uniform(-max_local_shift, +max_local_shift, size=y.shape) # scaling lambdas = np.exp(rng.uniform(-np.log(max_scale), + np.log(max_scale), size=[batch_size, 1, 1])) x *= lambdas y *= lambdas  assert n_channels half_channels = n_channels // 2 rho = 10 ** (np.arange(1, half_channels + 1) / half_channels) # recommended simpler approximate implementation # rho = 10 ** np.linspace(0, 1, half_channels) w_x = rho * np.cos(np.arange(half_channels)) w_y = rho * np.sin(np.arange(half_channels))  phase = np.pi * (w_x * x[:, :, :, None] + w_y * y[:, :, :, None]) return np.concatenate([np.cos(phase), np.sin(phase)], axis=-1) \\end{lstlisting}  \\newpage "
            },
            {
                "section_name": "Image Recognition Experiments_9",
                "paragraphs": ""
            },
            {
                "section_name": "Technical Details_4",
                "paragraphs": " \\begin{table}[h!] \\caption{Top-1 accuracy (\\%) for ViT and UniViT models evaluated on ImageNet validation set and ImageNet-v2 test sets with images resized to different resolutions: $160^2$, $224^2$, $384^2$ and $672^2$. Models trained on $224^2$ and further fine-tuned on $384^2$ resolution are marked with ``+ft''. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively.\\label{tab:vision_all_top1}} \\begin{center} \\npdecimalsign{.} \\nprounddigits{2} \\npnoroundexp \\resizebox{\\linewidth}{!}{ \\begin{tabular}{@{}lcn{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}@{}} \\toprule \\multirow{2}{*}{Model} & Train & \\multicolumn{4}{c}{ImageNet} & \\multicolumn{4}{c}{ImageNet-v2-a} & \\multicolumn{4}{c}{ImageNet-v2-b} & \\multicolumn{4}{c}{ImageNet-v2-c} \\\\ \\cmidrule(lr){3-6} \\cmidrule(lr){7-10} \\cmidrule(lr){11-14} \\cmidrule(lr){15-18} & Res. & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} \\\\ \\midrule \\m\\\\ \\\\ \\l\\\\ \\\\ nopos &  \\multirow{5}{*}{$160^2$} & 77.334 & 78.38 & 75.462 & 61.004 & 72.39 & 73.78 & 69.65 & 52.99 & 64.41 & 66.25 & 62.58 & 46.58 & 77.67 & 78.76 & 75.28 & 59.13 \\\\ abspos &  & 78.454 & 79.038 & 73.96 & 57.562 & 74.31 & 74.75 & 68.02 & 49.54 & 66.02 & 66.47 & 60.18 & 42.53 & 79.08 & 79.15 & 73.31 & 55.48 \\\\ sinpos &  & 79.052 & 74.382 & 65.654 & 44.128 & 74.75 & 70.11 & 59.61 & 36.75 & 66.91 & 62.13 & 52.28 & 30.55 & 79.37 & 75.41 & 65.45 & 42.51 \\\\ CAPE, $\\lambda=1$ &  & 78.738 & 79.694 & 75.82 & 61.868 & 74.94 & 75.75 & 70.5 & 54.91 & 66.56 & 68.17 & 62.78 & 46.74 & 79.51 & 80.18 & 75.71 & 60.39 \\\\ CAPE &  & 78.702 & 79.73 & 76.176 & 62.59 & 74.98 & 75.94 & 71.17 & 56.13 & 66.95 & 68.65 & 64.19 & 48.72 & 79.38 & 80.38 & 76.23 & 62 \\\\ \\midrule \\m\\\\ nopos & \\multirow{9}{*}{$224^2$} & 75.296 & 78.966 & 78.678 & 72.922 & 70.52 & 74.79 & 74.15 & 65.99 & 61.59 & 66.93 & 67.01 & 59.41 & 75.94 & 79.5 & 78.81 & 71.06 \\\\ abspos &  & 77.782 & 80.904 & 79.898 & 72.206 & 73.38 & 77.01 & 75.66 & 65.47 & 65.07 & 69.38 & 68.05 & 57.69 & 78.53 & 81.53 & 80.14 & 70.9  \\\\ \\\\ sinpos &  & 77.152 & 81.316 & 79.722 & 70.714 & 72.91 & 77.52 & 75.48 & 63.74 & 64.45 & 70.14 & 67.96 & 55.67 & 78.3 & 82.19 & 80.13 & 69.28 \\\\ CAPE, $\\lambda=1, \\Delta=0$ & & 77.532 & 81.084 & 80.18 & 72.344 & 73.41 & 77.61 & 75.81 & 65.99 & 64.85 & 70.23 & 68.22 & 57.83 & 78.33 & 82.05 & 80.06 & 70.99 \\\\ CAPE, $\\lambda=1, \\epsilon=0$ & & 77.458 & 81.144 & 80.488 & 72.132 & 73.35 & 77.9 & 76.36 & 64.95 & 64.84 & 69.84 & 68.45 & 57.31 & 78.8 & 81.84 & 80.58 & 70.53 \\\\ \\l\\\\ CAPE, $\\lambda=1$ &  & 77.698 & 81.01 & 80.384 & 73.064 & 72.88 & 77.67 & 76.23 & 67.35 & 64.79 & 70.25 & 69.36 & 59.59 & 78.16 & 82.22 & 80.96 & 72.07 \\\\ CAPE, $\\Delta=0$ & & 77.35 & 81.08 & 80.502 & 73.108 & 73.05 & 77.59 & 76.73 & 67.25 & 64.98 & 69.75 & 69.14 & 59.01 & 78.32 & 81.88 & 81 & 72.04 \\\\ CAPE, $\\epsilon=0$ & & 77.706 & 81.296 & 80.572 & 73.354 & 73.51 & 77.71 & 76.72 & 66.58 & 65.01 & 69.93 & 69.59 & 59.31 & 78.02 & 81.93 & 80.86 & 71.73 \\\\ \\\\ CAPE &  & 77.138 & 81.006 & 80.326 & 73.426 & 72.37 & 77.59 & 76.6 & 66.99 & 63.94 & 69.65 & 69.43 & 59.56 & 77.37 & 82 & 81.02 & 72.12 \\\\ \\midrule \\m\\\\ \\\\ \\l\\\\ \\\\ abspos & \\multirow{4}{*}{$384^2$} & 21.826 & 73.57 & 80.682 & 78.87 & 19.99 & 68.43 & 76.66 & 74 & 16.23 & 60.09 & 69.37 & 66.47 & 24.02 & 74.17 & 80.94 & 78.36 \\\\ sinpos &  & 7.708 & 75.55 & 82.416 & 80.734 & 7.16 & 70.87 & 78.82 & 76.21 & 5.26 & 62.29 & 71.32 & 68.82 & 8.72 & 75.57 & 82.82 & 80.47 \\\\ CAPE, $\\lambda=1$ &  & 31.978 & 75.378 & 82.554 & 80.94 & 28.47 & 71 & 78.97 & 75.85 & 23.51 & 62.4 & 71.55 & 69.02 & 34.15 & 76.25 & 82.77 & 80.6 \\\\ CAPE &  & 32.97 & 74.712 & 81.778 & 80.216 & 29.23 & 69.5 & 77.9 & 75.39 & 24.21 & 61.23 & 70.71 & 68.53 & 34.83 & 75.18 & 81.96 & 79.83 \\\\ \\midrule \\m\\s\\\\\\d\\\\\\\\ \\\\ \\\\ \\l\\\\ \\\\ nopos+ft & \\multirow{5}{*}{ \\shortstack{ $224^2$ \\\\ $\\downarrow$ \\\\ $384^2$} } & 46.104 & 75.74 & 80.376 & 78.894 & 41.42 & 71.72 & 76.69 & 74.16 & 33.82 & 62.81 & 69.04 & 67.06 & 47.87 & 76.58 & 80.92 & 78.69 \\\\ abspos+ft &  & 34.164 & 78.51 & 82.346 & 80.632 & 29.71 & 75.02 & 79.1 & 76.1 & 24.45 & 66.54 & 71.91 & 68.79 & 35.76 & 79.33 & 83.15 & 80.63 \\\\ sinpos+ft &  & 25.12 & 77.694 & 82.772 & 81.016 & 22.61 & 73.35 & 79.42 & 76.5 & 17.77 & 65.27 & 72.16 & 69.1 & 26.62 & 78.72 & 83.33 & 80.68 \\\\ CAPE+ft, $\\lambda=1$ &  & 57.8 & 78.628 & 82.668 & 81.284 & 53.25 & 74.87 & 79.19 & 77.3 & 44.91 & 67.07 & 72.21 & 70.2 & 59.69 & 79.72 & 83.6 & 81.59 \\\\ CAPE+ft &  & 58.462 & 78.136 & 82.462 & 80.936 & 53.55 & 73.89 & 79.4 & 76.89 & 44.82 & 65.3 & 71.74 & 69.71 & 60.05 & 78.76 & 83.08 & 81.53 \\\\ \\midrule UniViT, sinpos & \\multirow{3}{*}{mix} & 78.944  & 80.824  & 82.306  & 82.118  & 74.64  & 77.21  & 78.58  & 78.29  & 66.57  & 69.74  & 71.48  & 71.42  & 79.53  & 81.67  & 82.82  & 82.53  \\\\ UniViT, CAPE $\\lambda=1$ &  & 79.136  & 81.26  & 82.55  & 82.34  & 74.85  & 77.85  & 79.42  & 78.76  & 67.07  & 69.97  & 72.03  & 71.54  & 79.74  & 82.09  & 83.26  & 82.75  \\\\ UniViT, CAPE &  & 79.05  & 81.164  & 82.282  & 81.834  & 74.88  & 77.5  & 78.96  & 77.87  & 67.08  & 69.88  & 72.01  & 70.99  & 79.78  & 82.06  & 83.1  & 82.21  \\\\ \\midrule abspos-S & $224^2$ & 74.888 & 79.456 & 77.828 & 64.318 & 70.89 & 75.83 & 73.45 & 57.4 & 62.04 & 68.12 & 65.86 & 49.22 & 76.15 & 80.54 & 78.52 & 63.24 \\\\ UniViT-S, CAPE, $\\lambda=1$ & mix & 76.05 & 79.004 & 80.644 & 80.308 & 72.57 & 75.56 & 77.59 & 76.66 & 64 & 67.31 & 70.25 & 69.41 & 78.13 & 80.44 & 81.98 & 81.3 \\\\ \\midrule abspos-Ti & $224^2$ & 64.758 & 71.914 & 70.208 & 56.122 & 61.03 & 68.78 & 66.3 & 49.62 & 51.88 & 59.67 & 58.09 & 42.71 & 67.97 & 74.13 & 71.92 & 55.63 \\\\ UniViT-Ti, CAPE, $\\lambda=1$ & mix & 65.246 & 69.826 & 72.438 & 71.152 & 62.2 & 66.4 & 68.99 & 67.45 & 53.35 & 57.25 & 61.3 & 59.72 & 69.15 & 72.64 & 74.93 & 73.23 \\\\ \\bottomrule \\end{tabular} } \\end{center} \\end{table}  \\begin{table}[h!] \\caption{Top-5 accuracy (\\%) for ViT and UniViT models evaluated on ImageNet validation set and ImageNet-v2 test sets with images resized to different resolutions: $160^2$, $224^2$, $384^2$ and $672^2$. Models trained on $224^2$ and further fine-tuned on $384^2$ resolution are marked with ``+ft''. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively.\\label{tab:vision_all_top5}} \\npdecimalsign{.} \\nprounddigits{2} \\npnoroundexp \\begin{center} \\resizebox{\\linewidth}{!}{ \\begin{tabular}{@{}lcn{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}n{2}{2}@{}} \\toprule \\multirow{2}{*}{Model} & Train & \\multicolumn{4}{c}{ImageNet} & \\multicolumn{4}{c}{ImageNet-v2-a} & \\multicolumn{4}{c}{ImageNet-v2-b} & \\multicolumn{4}{c}{ImageNet-v2-c} \\\\ \\cmidrule(lr){3-6} \\cmidrule(lr){7-10} \\cmidrule(lr){11-14} \\cmidrule(lr){15-18} & Res. & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} & \\texttt{$160^2$} & \\texttt{$224^2$} & \\texttt{$384^2$} & \\texttt{$672^2$} \\\\ \\midrule \\m\\\\ \\\\ \\l\\\\ \\\\ nopos & \\multirow{5}{*}{$160^2$}  & 93.198 & 93.992 & 92.43 & 83.688 & 91.71 & 92.64 & 90.22 & 77.75 & 84.88 & 86.39 & 83.96 & 71.52 & 94.6 & 95.02 & 93.09 & 82.41 \\\\ abspos &  & 94.062 & 94.366 & 91.6 & 81.26 & 92.66 & 93.14 & 88.97 & 75.19 & 86.07 & 86.76 & 82.27 & 67.75 & 95.3 & 95.31 & 92.27 & 80.53 \\\\ sinpos &  & 94.264 & 91.8 & 86.414 & 68.552 & 93.2 & 90.08 & 82.75 & 61.43 & 86.65 & 83.28 & 75.62 & 54.14 & 95.7 & 93.24 & 87.03 & 67.55 \\\\ CAPE, $\\lambda=1$ &  & 94.194 & 94.776 & 92.916 & 84.574 & 92.84 & 93.46 & 90.55 & 79.44 & 86.87 & 87.75 & 84.62 & 72.17 & 95.44 & 95.8 & 93.85 & 84.43 \\\\ CAPE &  & 94.148 & 94.8 & 93.026 & 85.322 & 92.95 & 93.65 & 90.82 & 80.48 & 86.68 & 87.86 & 85.22 & 73.47 & 95.27 & 95.72 & 94.06 & 84.92 \\\\ \\midrule \\m\\\\ nopos & \\multirow{9}{*}{$224^2$}  & 92.138 & 94.222 & 94.114 & 90.89 & 90.17 & 92.79 & 92.73 & 87.58 & 82.72 & 86.92 & 87.27 & 81.61 & 93.48 & 95.19 & 94.88 & 90.91 \\\\ abspos &  &  93.446 & 95.26 & 94.714 & 90.488 & 91.8 & 93.91 & 93.24 & 87.13 & 85.34 & 88.52 & 87.77 & 80.23 & 94.52 & 96.17 & 95.43 & 90.76 \\\\ \\\\ sinpos &  & 93.286 & 95.444 & 94.524 & 89.772 & 91.48 & 94.22 & 92.84 & 85.33 & 84.52 & 88.75 & 87.22 & 78.37 & 94.48 & 96.46 & 95.23 & 89.03 \\\\ CAPE, $\\lambda=1, \\Delta=0$ & & 93.294 & 95.212 & 94.73 & 90.558 & 91.47 & 93.73 & 92.91 & 87.42 & 84.8 & 88.26 & 87.69 & 80.74 & 94.43 & 96.14 & 95.37 & 90.6 \\\\ CAPE, $\\lambda=1, \\epsilon=0$ & & 93.372 & 95.416 & 94.966 & 90.722 & 91.46 & 94.29 & 93.26 & 86.82 & 85.26 & 88.87 & 88.15 & 80.26 & 94.47 & 96.22 & 95.61 & 90.34 \\\\ \\l\\\\ CAPE, $\\lambda=1$ &  & 93.446 & 95.452 & 95.094 & 91.5 & 91.69 & 94.11 & 93.64 & 87.84 & 85.61 & 88.87 & 88.52 & 81.79 & 94.63 & 96.18 & 95.91 & 91.6 \\\\ CAPE, $\\Delta=0$ & & 93.19 & 95.42 & 94.998 & 91.304 & 91.94 & 94.38 & 93.75 & 88.13 & 85.13 & 88.97 & 88.52 & 82.01 & 94.63 & 96.27 & 96.01 & 91.4 \\\\ CAPE, $\\epsilon=0$ & & 93.256 & 95.318 & 94.942 & 91.246 & 91.92 & 94.24 & 93.63 & 87.8 & 84.88 & 88.64 & 88.26 & 81.1 & 94.65 & 96.18 & 95.51 & 90.99 \\\\ \\\\ CAPE &  & 93.178 & 95.176 & 94.938 & 91.574 & 91.64 & 94.23 & 93.77 & 88.56 & 85.18 & 88.31 & 88.33 & 82.04 & 94.49 & 96.39 & 95.94 & 91.85 \\\\ \\midrule \\m\\\\ \\\\ \\l\\\\ \\\\ abspos & \\multirow{4}{*}{$384^2$} & 38.446 & 90.688 & 94.986 & 93.886 & 35.76 & 87.99 & 93.39 & 91.93 & 30.1 & 80.81 & 88.09 & 86.19 & 40.91 & 91.6 & 95.58 & 94.49 \\\\ sinpos &  & 15.86 & 91.88 & 95.676 & 94.792 & 14.71 & 89.88 & 94.5 & 93.11 & 11.98 & 82.13 & 89.54 & 87.71 & 17.18 & 92.82 & 96.29 & 95.23 \\\\ CAPE, $\\lambda=1$ &  & 51.032 & 91.96 & 95.834 & 94.988 & 48.04 & 89.77 & 94.48 & 93.02 & 40.55 & 82.73 & 89.58 & 88.14 & 54.47 & 93.07 & 96.43 & 95.3 \\\\ CAPE &  & 51.82 & 91.292 & 95.396 & 94.53 & 47.32 & 88.62 & 94.07 & 92.52 & 40.21 & 81.29 & 88.75 & 87.64 & 53.95 & 92.16 & 96.09 & 94.94 \\\\ \\midrule \\m\\s\\\\\\d\\\\\\\\ \\\\ \\l\\\\ \\\\ nopos+ft & \\multirow{5}{*}{ \\shortstack{ $224^2$ \\\\ $\\downarrow$ \\\\ $384^2$} }  & 68.448 & 92.506 & 95.118 & 94.25 & 63.71 & 90.61 & 93.86 & 92.67 & 55.07 & 83.52 & 88.47 & 87.18 & 70.52 & 93.72 & 95.7 & 94.83 \\\\ abspos+ft &  & 54.056 & 93.958 & 95.96 & 95.112 & 49.48 & 92.58 & 95.11 & 93.6 & 42.31 & 86.42 & 90.19 & 88.39 & 56.36 & 95.12 & 96.85 & 95.8 \\\\ sinpos+ft &  & 42.796 & 93.574 & 96.078 & 95.194 & 38.91 & 91.85 & 95.08 & 93.8 & 33.17 & 85.54 & 90.03 & 88.28 & 44.75 & 94.76 & 96.98 & 95.68 \\\\ CAPE+ft, $\\lambda=1$ &  & 79.264 & 94.146 & 96.14 & 95.47 & 75.63 & 92.86 & 95.04 & 94.02 & 66.43 & 86.84 & 90.62 & 89.13 & 81.36 & 95.39 & 96.94 & 95.96 \\\\ CAPE+ft &  & 79.992 & 93.818 & 96.06 & 95.322 & 76.09 & 92.31 & 95.24 & 94.16 & 67 & 85.91 & 90.04 & 88.75 & 81.7 & 95.02 & 96.97 & 96.09 \\\\ \\midrule UniViT, sinpos & \\multirow{3}{*}{mix} &  94.216 &  95.4 &  96.036 &  95.964 &  92.93 &  94.28 &  94.93 &  94.76 &  86.55 &  88.8 &  89.98 &  89.93 &  95.28 &  96.31 &  96.73 &  96.65 \\\\ UniVit, CAPE, $\\lambda=1$ &  &  94.392 &  95.558 &  96.176 &  96.016 &  93.02 & 94.48 &  95.23 &  95.19 &  86.72 &  88.91 &  90.3 &  90.33 &  95.29 &  96.4 &  97 &  96.84 \\\\ UniViT, CAPE &  &  94.354 &  95.438 & 96.044 &  95.716 &  92.92 &  94.36 &  95.11 &  94.69 &  86.76 &  89.18 &  90.45 &  89.68 &  95.5 &  96.42 &  96.87 &  96.37 \\\\ \\midrule abspos-S & $224^2$ & 92.126 & 94.692 & 94.094 & 85.75 & 90.78 & 93.74 & 92.97 & 81.28 & 83.44 & 87.62 & 86.72 & 73.72 & 93.82 & 95.95 & 95.3 & 85.73 \\\\ UniViT-S, CAPE, $\\lambda=1$ & mix & 92.982 & 94.642 & 95.526 & 95.346 & 92.08 & 93.98 & 95.03 & 94.65 & 85.15 & 87.92 & 89.64 & 89.02 & 94.88 & 96 & 96.72 & 96.37 \\\\ \\midrule abspos-Ti & $224^2$ & 86.538 & 90.928 & 90.156 & 80.804 & 85.59 & 90.13 & 88.48 & 76.1 & 76.5 & 82.39 & 81.41 & 68.03 & 89.74 & 93.25 & 91.81 & 81 \\\\ UniViT-Ti, CAPE, $\\lambda=1$ & mix & 86.942 & 89.738 & 91.332 & 90.798 & 86.03 & 88.79 & 90.35 & 89.25 & 77.33 & 80.97 & 83.42 & 82.41 & 90.17 & 92.31 & 93.44 & 92.37 \\\\ \\bottomrule \\end{tabular} } \\end{center} \\end{table}  For all ViT/UniViT models presented in Figures~\\ref{fig:vision:vit_main} and~\\ref{fig:vision:mix_sizes}, and in the ablation study below, we report their top-1 and top-5 accuracies in Tables~\\ref{tab:vision_all_top1} and~\\ref{tab:vision_all_top5}, respectively, evaluated on the ImageNet validation and ImageNet-v2 test sets and on images with different resolutions.  We train models in Flashlight framework\\footnote{\\scriptsize\\texttt{https://github.com/flashlight/flashlight}} where ViT/DeiT~\\cite{touvron2020training} training is reproduced following an original implementation:\\footnote{\\scriptsize\\texttt{https://github.com/facebookresearch/deit}} initialization is set to the truncated normal distribution, Rand-Augment~\\cite{cubuk2020randaugment}, Mixup~\\cite{zhang2017mixup} and Cutmix~\\cite{yun2019cutmix}, random erasing~\\cite{zhong2020random} and repeated augmentation~\\cite{hoffer2020augment,berman2019multigrain} are used as data augmentations; training is done with AdamW optimizer for 300 epochs. All models use learnable absolute positional embedding for the class token. We train ViT (UniViT) models on 16 GPUs, V100 32GB, with mixed-precision and batch size 64 per GPU for 19-56h (37h) depending on the input resolution ($384^2$ resolution is trained on 32 GPUs with batch size 32 per GPU).  In Figure~\\ref{fig:vision:speed} the throughput is measured as the number of images that can be processed per second on single 16GB V100 GPU following the benchmarking method from~\\cite{touvron2020training}: for each image resolution we pass the largest possible batch size and calculate the average time over 30 runs to process that batch.   "
            },
            {
                "section_name": "Finding the Best Resolution for UniViT Evaluation_5",
                "paragraphs": " In this section we describe an evaluation procedure that improves UniViT with CAPE performance by resizing input images to an optimal resolution. We split ImageNet validation images into 8 bins, according to their size $s=\\min(h, w)$, where $h$ and $w$ are image height and width, respectively: $s\\in[54,100]$, $s\\in[101,150]$, $s\\in[151,200]$, $s\\in[201,250]$, $s\\in[251,300]$, $s\\in[301,350]$, $s\\in[351,384]$, $s\\in[385, \\inf]$. For each bin we consider several resizing strategies: i) resize all images in a bin either to $160^2$, or $224^2$, or $384^2$; ii) resize all images to the minimum $s$ value in a bin, \\textit{Min}; iii) resize all images to the maximum $s$ value in a bin, \\textit{Max}; iv) use image's original size but still perform a central \\textit{rectangular} crop in a similar manner as standard evaluation is done for ImageNet, \\textit{Original}; v) use image's original size, \\textit{Original (no crop)}. For the bin with high resolution images $[385, \\inf]$ we use $500^2$ as the maximum resize value and apply neither iv) nor v) strategies as there are images with $s$ > 5000 px. We report top-1 accuracy for this evaluation procedure with different strategies per each bin in Table~\\ref{tab:vision:own_size_eval}: for the best strategy in each bin (table row) we report accuracy (\\%) while for other strategies in the same bin we report absolute drop in accuracy compared to the best value. Best values are additionally marked in bold.  \\begin{table}[h!] \\caption{UniViT with CAPE evaluation on ImageNet validation set with different strategies on input resizing. Images are split into 8 bins by minimum spatial size. We report best top-1 accuracy (\\%) in each row (bold) and the drop in accuracy compared to this best accuracy for other columns.\\label{tab:vision:own_size_eval}} \\begin{center} \\resizebox{0.9\\linewidth}{!}{ \\begin{tabular}{@{}cccccccccc@{}} \\toprule \\# Images & Min Res. & Max Res. & $160^2$ & $224^2$ & $384^2$ & Min & Max & Original & Original (no crop) \\\\ \\midrule 146 & 54 & 100 & \\textbf{84.25} & -0.69 & -0.69 & -19.86 & -1.37 & -6.16 & -8.22 \\\\ 221 & 101 & 150 & -0.81 & -1.36 & -0.45 & -5.43 & -2.71 & -5.43 & \\textbf{74.66} \\\\ 372 & 151 & 200 & -1.61 & -1.08 & -2.42 & -3.23 & -2.42 & \\textbf{78.49} & -1.61 \\\\ 538 & 201 & 250 & -3.16 & -1.30 & -1.49 & -2.60 & -1.86 & \\textbf{81.97} & -2.42 \\\\ 1090 & 251 & 300 & -4.40 & -1.47 & -0.28 & -1.56 & -1.01 & \\textbf{79.72} & -0.37 \\\\ 8496 & 301 & 350 & -3.83 & -1.88 & -0.48 & -1.39 & -1.08 & \\textbf{83.57} & -0.48 \\\\ 24538 & 351 & 384 & -4.14 & -1.88 & -0.43 & -0.79 & -0.43 & \\textbf{82.10} & -0.13 \\\\ 14599 & 385 & - & -3.30 & -1.25 & -0.14 & -0.13 & \\textbf{84.46} & - & - \\\\ \\bottomrule \\end{tabular} } \\end{center} \\end{table}    "
            },
            {
                "section_name": "Visualization of Positional Embeddings_6",
                "paragraphs": " We visualize positional embeddings (excluding class token) for ViT models trained on $224^2$ input resolution in Figure~\\ref{fig:emb_visualization}: for each positional embedding we plot every 20th among 768 components (each row corresponds to a particular component) and visualize its values with the image of shape $(r/16)^2$ where $r^2$ is an input image resolution. We consider input resolutions $160^2$, $224^2$ and $384^2$ shown as left, middle and right sub-columns for each positional embedding in Figure~\\ref{fig:emb_visualization}.  \\begin{figure}[h!] \\begin{subfigure}{.2\\textwidth} \\centering \\includegraphics[width=0.65\\textwidth]{figures/vit_abspos_emb_visualization.pdf} \\end{subfigure} \\begin{subfigure}{.2\\textwidth} \\centering \\includegraphics[width=0.65\\textwidth]{figures/vit_sinpos_emb_visualization.pdf} \\end{subfigure} \\begin{subfigure}{.2\\textwidth} \\centering \\includegraphics[width=0.65\\textwidth]{figures/vit_cape_emb_visualization.pdf} \\end{subfigure} \\caption{Visualization of positional embeddings for ViT models trained on $224^2$ resolution: \\textit{abspos} (left), \\textit{sinpos} (middle) and CAPE (right). Each column consists of 3 sub-columns corresponding to input resolutions $160^2$, $224^2$ and $384^2$. Only some components (each 20th out of 768) are shown. When \\textit{sinpos} applied to low-resolution images, spacial aliasing is visible in latest components of embeddings. CAPE's augmentations destruct this patterns and prevent model from over-fitting. \\label{fig:emb_visualization}} \\end{figure}   "
            },
            {
                "section_name": "Ablations_7",
                "paragraphs": " Both \\textit{sinpos} and CAPE first re-scale patch positions $(x, y)$ to the $[-1, 1]$ interval independently from the image resolution. We study if an alternative re-scaling of patch positions during inference improves performance on resolutions other than training ones. For ViT models trained with either \\textit{sinpos} or CAPE on $224^2$ resolution we perform evaluation on $r^2=160^2$ and $r^2=384^2$ resolutions by re-scaling $(x, y)$ to $[-\\gamma, \\gamma]$: $\\gamma$ is set to either $1$ (baseline strategy), $r/224$, or $\\sqrt{(r/224)}$. Results of this comparison, Table~\\ref{tab:abl:vision:scaling}, are consistent across models and suggest that for applying to smaller resolution ($160^2$) decreasing scale $\\gamma$ to match density of patches on a plane to train-time is beneficial; however, opposite effect is observed when model is applied to higher resolution ($384^2$) inputs, potentially because distances between patch positions in this case were not observed at training time. For simplicity we use re-scaling to $[-1, 1]$ in the rest of our experiments.  \\begin{table}[h!] \\begin{center} \\caption{Ablation study on re-scaling positions to the range $[-\\gamma, \\gamma]$ for ViT models trained on $224^2$ images with \\textit{sinpos} or CAPE. We report top-1 accuracy (\\%) on ImageNet validation set evaluated on images scaled to $160^2$ and $384^2$ resolutions.\\label{tab:abl:vision:scaling}} \\resizebox{0.5\\linewidth}{!}{ \\begin{tabular}{@{}cccc@{}} \\toprule Model & $\\gamma$ & Top-1, $r=160$ & Top-1, $r=384$ \\\\ \\midrule  \\m\\\\ \\s\\\\ \\\\ \\m \\m\\l\\\\ \\s\\\\ \\\\ \\m \\m\\\\ \\s\\\\ \\\\ \\multirow{3}{*}{sinpos} & $r/224$ & 77.89 & 72.01 \\\\ & $\\sqrt{r/224}$ & 77.11 & 76.94 \\\\ & 1 & 77.15 & 79.72 \\\\ \\midrule \\multirow{3}{*}{CAPE, $\\lambda=1$} & $r/224$ & 77.96 & 80.18 \\\\ & $\\sqrt{r/224}$ & 77.80 & 80.51 \\\\ & 1 & 77.70 & 80.38 \\\\ \\midrule \\multirow{3}{*}{CAPE} & $r/224$ & 77.28 & 80.20 \\\\ & $\\sqrt{r/224}$ & 77.18 & 80.28 \\\\ & 1 & 77.14 & 80.33 \\\\  \\bottomrule \\end{tabular} } \\end{center} \\end{table}  In Figure~\\ref{fig:abl:vis_sin_vs_cape}, we compare \\textit{sinpos} and CAPE for ViT models in more detail. Overall, CAPE performs better or similar to \\textit{sinpos} on the training resolution while it significantly outperforms \\textit{sinpos} on other resolutions. In Figure~\\ref{fig:abl:vis_scale_vs_no_scale_cape} we study the importance of scale $\\lambda$ for CAPE in ViT models. Scale $\\lambda_{max} > 1$ slightly improves generalization for higher and lower resolutions.   \\begin{figure}[htb!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/vit_sin_ablation.pdf} \\caption{Comparison of top-1 accuracy between \\textit{sinpos} and CAPE trained on either $160^2$, or $224^2$, or $384^2$ resolutions and evaluated across the board. Models trained on $224^2$ resolution and further fine-tuned on $384^2$ resolution are marked with ``+ft''.\\label{fig:abl:vis_sin_vs_cape}} \\end{figure}   \\begin{figure}[t!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/vit_scale_ablation.pdf} \\caption{Comparison of top-1 accuracy for CAPE with $\\lambda_{max}=1$ (dashed) and $\\lambda_{max}=1.4$ (solid) trained on either $160^2$, or $224^2$, or $384^2$ resolutions and evaluated across the board. Models trained on $224^2$ resolution and further fine-tuned on $384^2$ resolution are marked with ``+ft''.\\label{fig:abl:vis_scale_vs_no_scale_cape}} \\end{figure}  In Figure~\\ref{fig:abl:vis_ablations} we study the importance of global $\\Delta$ and local $\\epsilon$ shifts for CAPE in ViT models trained on $224^2$ resolution. On higher resolutions, $384^2$ and $672^2$, models with both shifts (solid) perform similar or better than models trained with either local (dotted-dashed) or global (dashed) shifts. Overall, only one of the shifts, global or local, can be used while the most important CAPE's parameter is the global scale. On the other hand, any combination of augmentations in CAPE clearly outperforms \\textit{sinpos} on resolutions different from training one.  \\begin{figure}[tb!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/vit_ablation.pdf} \\caption{ Comparison of top-1 accuracy between \\textit{sinpos} and CAPE with different configurations on global, local shifts and global scaling trained on $224^2$ resolution.\\label{fig:abl:vis_ablations} } \\end{figure}  \\begin{figure}[tbh!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/vit_mix_ablation.pdf} \\caption{ Comparison of top-1 accuracy between \\textit{sinpos} and CAPE (with and without global scaling) trained on the mixture of resolutions $\\{128^2, 160^2, 192^2, 224^2, 256^2, 288^2, 320^2\\}$.\\label{fig:abl:vis_univit} } \\end{figure}  In Figure~\\ref{fig:abl:vis_univit} we study if CAPE's augmentations are beneficial for UniViT model, compared to using UniViT with \\textit{sinpos}. Overall \\textit{sinpos} performs worst among UniViT models, while outperforming UniViT with CAPE and global scaling on $672^2$ resolution. UniViT with CAPE and no global scaling ($\\lambda_{max}=1$) performs the best on all resolutions, suggesting that variability in training resolutions provides a sufficient base for generalization to higher resolutions.   \\newpage  "
            },
            {
                "section_name": "Automatic Speech Recognition Experiments_10",
                "paragraphs": " "
            },
            {
                "section_name": "Data_8",
                "paragraphs": "For WSJ data we consider the standard subsets \\emph{si284}, \\emph{nov93dev} and \\emph{nov92} for training, validation and test, respectively. We remove any punctuation tokens from \\emph{si284} transcriptions before training. TED-LIUM v3 dataset is based on TED conference videos. We use the last edition of the training set (v3); validation and test sets are kept consistent (and thus numbers are comparable) with the earlier releases. We follow the Kaldi recipe~\\cite{daniel2011kaldi} for data preparation. In Tables~\\ref{tab:data_asr_stat} and~\\ref{tab:data_asr} we present statistics of the datasets used in Section~\\ref{sec:asr}. One could notice that TED-LIUM v3 validation and test sets have samples with significantly longer duration and larger number of words in their transcriptions, which makes these sets the most challenging among other public data.  \\begin{table}[h!] \\caption{Statistics on datasets: sampling frequency, duration (in hours), and speech type. \\label{tab:data_asr_stat}} \\begin{center} \\begin{tabular}{@{}cccccc@{}} \\toprule Data & kHz & Train (h) & Valid (h) & Test (h) & Speech \\\\ \\midrule WSJ & 16 & 81.5 & 1.1 & 0.7 & read \\\\ TL & 16 & 452 & 1.6 & 2.6 & oratory \\\\ LS & 16 & - & - & 5.4+5.4 & read \\\\ RV & 16 & - & - & 18.8+19.5 & diverse \\\\ \\bottomrule \\end{tabular} \\end{center} \\end{table}  \\r\\l \\b \\t \\\\ \\m \\\\ \\\\ \\\\ \\\\ \\\\ \\b \\e   \\begin{table}[h!] \\caption{Statistics on datasets: mean sample duration (in seconds) and mean sample transcription length (in words). \\label{tab:data_asr}} \\begin{center} \\resizebox{\\linewidth}{!}{ \\begin{tabular}{@{}cccccccc@{}} \\toprule Data & Train $\\mu\\pm\\sigma$ (s) & Valid $\\mu\\pm\\sigma$ (s) & Test $\\mu\\pm\\sigma$ (s) & Train $\\mu\\pm\\sigma$ (wrd) & Valid $\\mu\\pm\\sigma$ (wrd) & Test $\\mu\\pm\\sigma$ (wrd) \\\\ \\midrule WSJ & $7.8 \\pm 2.9$ & $7.8 \\pm 2.9$ & $7.6 \\pm 2.5$ & $17 \\pm 7$ & $16\\pm 7$ & $17\\pm6$ \\\\ TL & $6 \\pm 3$ & $11.3 \\pm 5.7$ & $8.1 \\pm 4.3$ & $17 \\pm 10$ & $35\\pm 20$ & $24\\pm 15$ \\\\ LS & - & - & $7 \\pm 4.8$ & - & - & $19\\pm13$ \\\\ \\bottomrule \\end{tabular} } \\end{center} \\end{table}   "
            },
            {
                "section_name": "Acoustic Model Training_9",
                "paragraphs": "For all experiments we compute 80 log-mel spectrogram features for a 25ms sliding window, strided by 10ms (unless we explicitly vary STFT hop distance). All features are normalized to have zero mean and unit variance per input sequence before feeding into the neural network.  The self-attention dimension is $768$ and the feed-forward network (FFN) dimension is $3072$ in each Transformer layer. We use dropout $0.3$ after the convolution layer; for all Transformer layers, we use dropout on the self-attention and on the FFN, and layer drop~\\cite{fan2019reducing}, dropping entire layers at the FFN level. Transformer dropout and layer drop values are set to be $0.4$ for WSJ and $0.1$ for TED-LIUM v3 training.  SpecAugment~\\cite{park2019specaug} is used for data augmentation during training: there are two frequency masks, and ten time masks with maximum time mask ratio of $p=0.1$, the maximum frequency bands masked by one frequency mask is 30, and the maximum frames masked by the time mask is 50; time warping is not used. We use the Adagrad optimizer~\\cite{duchi2011adaptive}. All models are trained with dynamic batching (average batch size is 240s/GPU) and mixed-precision computations on 16 GPUs (Volta 32GB) for 1 day on WSJ and 3-4 days on TED-LIUM v3. All ASR experiments are done within Flashlight framework on top of the publicly available training configurations\\footnote{\\scriptsize\\texttt{https://github.com/flashlight/wav2letter/tree/master/recipes/rasr}} for baselines with \\textit{relpos} from~\\cite{likhomanenko2020rethinking}.  \\begin{figure}[t!] \\begin{subfigure}{.48\\textwidth} \\centering \\includegraphics[width=0.7\\textwidth]{figures/wsj_hop_pdf.png} \\end{subfigure} \\begin{subfigure}{.48\\textwidth} \\centering \\includegraphics[width=0.7\\textwidth]{figures/tl_hop_pdf.png} \\end{subfigure} \\caption{Hop distance distribution for WSJ (left) and TED-LIUM v3 (right) data. \\label{fig:abl:speech:hop_pdf}} \\end{figure}  "
            },
            {
                "section_name": "Padding-free ASR with CAPE and Variable STFT Hop Distance_10",
                "paragraphs": " We have implemented pipeline where padding is no longer used to form a batch from samples with different input duration. For each audio in the batch short-time Fourier Transform (STFT) hop distance $H$ is set in a way that output number of frames is the same (except rounding) as for hypothetical audio which has duration equal to the mean over batch and is processed with $H=10$ms. Because the hop distance is an integer number, number of frames after STFT is matched only approximately within a batch, so we reduce the number of frames in each sample to match the shortest sample in the batch by randomly and uniformly skipping frames. To have low variation of samples duration in a batch (which implies limited vatiation in $H$) the following shuffling strategy is performed for every epoch: i) compute perturbed sample duration by multiplying original sample duration by a random number from $\\mathcal{U}(0.85, 1.15)$; ii) sort samples by their perturbed duration; iii) batches are formed by grouping sequential samples. Example of hop distance distribution after proposed shuffling strategy for WSJ and TL data is shown in Figure~\\ref{fig:abl:speech:hop_pdf}. For both \\textit{sinpos} and CAPE embeddings we train models with this new pipeline and observe mostly lower WER and improved generalization, especially on TL test and RV data which are the most challenging among evaluation sets, Figures~\\ref{fig:abl:wsj_hop} and~\\ref{fig:abl:tl_hop}.\\footnote{For all models evaluation the batch size is set to 1 and $H=10$ms, thus padding never affects the performance on validation and test sets.}  \\begin{figure}[h!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_hop.pdf} \\caption{Word error rate comparison for models trained on WSJ data with \\textit{sinpos} or CAPE ($\\lambda=1$) with classical pipeline (solid) or with variable STFT hop distance (dashed).\\label{fig:abl:wsj_hop}} \\end{figure}  \\begin{figure}[h!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/tl_hop.pdf} \\caption{Word error rate comparison for models trained on TED-LIUM v3 data with \\textit{sinpos} or CAPE ($\\lambda=1$) with classical pipeline (solid) or with variable STFT hop distance (dashed).\\label{fig:abl:tl_hop}} \\end{figure}  As discussed in Section~\\ref{sec:asr}, STFT hop distance can be viewed as image resolution in vision and padding-free ASR \u2013 as UniViT. By adjusting hop distance during inference for padding-free ASR we can achieve higher throughput with similar recognition quality, see Figure~\\ref{fig:abl:tl_hop_time}. Moreover, it is robust to STFT hop distance variation having almost the same WER for $H\\in[5, 12]$ms and less performance degradation for $H>12$ms.   \\begin{figure}[h!] \\centering \\includegraphics[width=0.45\\textwidth]{figures/asr_speed_final.pdf} \\caption{Dependence between throughput and word error rate on TED-LIUM v3 test set for models trained with CAPE ($\\lambda=1)$ on TED-LIUM v3 data: with classical pipeline (orange) and with variable STFT hop distance (purple). Different throughput values correspond to different STFT hop distances in ms (crosses) used during evaluation.\\label{fig:abl:tl_hop_time}} \\end{figure}   "
            },
            {
                "section_name": "Ablations_11",
                "paragraphs": " First, we study dependence between the global shift value $\\Delta_{max}$ and model's performance and generalization abilities to the long duration. Varying the global shift we observe in Figure~\\ref{fig:abl:speech:global_shift_tl} that larger global shift leads to a  better generalization on longer audios, so that CAPE with 30-60s global shifts is able to process 45s audio with the same performance as 10s on RV data.  \\begin{figure}[ht!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/tl_ablation_global.pdf} \\caption{Word error rate comparison for models trained on TED-LIUM v3 data with CAPE and different global shifts which cover 15, 30 or 60s. The global scale is set to $\\lambda_{max} =1$.\\label{fig:abl:speech:global_shift_tl}} \\end{figure}  Secondly, we study the necessity of the local shift in CAPE. In Figure~\\ref{fig:abl:speech:asr_wsj_local} we observe that local shift absence hurts the performance and generalization across the board.  \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_local.pdf} \\caption{Word error rate comparison for CAPE models trained on WSJ data with global shift only (solid) or with global and local shifts together (dashed). The global scale is set to be $\\lambda_{max} =1$.\\label{fig:abl:speech:asr_wsj_local}} \\end{figure}  Thirdly, we study the necessity of the global scaling in CAPE. In Figure~\\ref{fig:abl:speech:relpos_scale_wsj} we observe that for WSJ models global scaling hurts a bit performance on public data while performs and generalizes better for RV data. In contrast, for TL models we observe in Figure~\\ref{fig:abl:speech:relpos_scale_tl} that overall the global scaling improves performance on public data while hurts performance on RV data. Thus, the global scaling should be tuned separately depending on the data type.  As an ablation study we perform additional experiments with \\textit{relpos}. First, we restrict \\textit{relpos} context to small duration, 6s, to prevent over-fitting to relative positions: \\textit{relpos 6s} outperforms \\textit{relpos} on both public and RV data for both models trained on WSJ and TL having significantly better generalization to long audio durations. \\textit{Relpos 6s} is performing similar to \\textit{abspos} for duration $>20$s while CAPE still outperforms \\textit{relpos 6s} on $>20$s, Figures~\\ref{fig:abl:speech:relpos_scale_wsj} and~\\ref{fig:abl:speech:relpos_scale_tl}. Independently, we also tried sinusoidal relative positional embedding~\\cite{dai2019transformer} but it performed worse than learnable relative positional embedding.   \\begin{figure}[t!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_ablation.pdf} \\caption{Word error rate comparison for models trained on WSJ data with different positional embeddings. \\textit{Relpos first layer} refers to a model where \\textit{relpos} is used only in the first Transformer layer with 27.6s context to the left/right and no other positional embeddings are used. \\label{fig:abl:speech:relpos_scale_wsj}} \\end{figure}  \\begin{figure}[t!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/tl_ablation.pdf} \\caption{Word error rate comparison for models trained on TED-LIUM v3 data with different positional embeddings. \\textit{Relpos first layer} refers to a model where \\textit{relpos} is used only in the first Transformer layer with 27.6s context to the left/right and no other positional embeddings are used. \\label{fig:abl:speech:relpos_scale_tl}} \\end{figure}  Second, having in mind that \\textit{nopos} performs well for a model trained on TL and CAPE's ability to learn spatial relations using a single layer, we wonder if \\textit{relpos} should be used only in the first Transformer layer (in literature \\textit{relpos}, when used, is applied in every attention layer). We modify \\textit{nopos} model by injecting \\textit{relpos} embedding (27.6s context to the right/left) only in the first Transformer layer: no any other Transformer layers use any positional embeddings, Figures~\\ref{fig:abl:speech:relpos_scale_wsj} and~\\ref{fig:abl:speech:relpos_scale_tl}. This \\textit{relpos first layer} model behaves surprisingly well: for a WSJ model it outperforms CAPE on both public and RV data; for a TL model it behaves similar to CAPE on public data and a bit better on RV data. Both CAPE and \\textit{relpos first layer} have similar mostly uniform performance profiles across different audio durations on RV data. This observation asks for reconsidering the standard usage of positional embedding for CTC-based models in speech recognition.  "
            },
            {
                "section_name": "No Positional Embedding Discussion_12",
                "paragraphs": " As demonstrated above in Figure~\\ref{fig:speech:tl}, \\textit{nopos} performs similar to different positional embeddings on both public and RV data while having reliable generalization to the long audio fragments. We figured out that the key components of this phenomenon and \\textit{nopos} success are i) enough training data; ii) sufficient model capacity and iii) CTC loss.  For the first point we saw that \\textit{nopos} model trained on WSJ, a 5x smaller dataset than TL, performs poorly having 45-50\\% WER even on in-domain data. For the second point we perform an additional ablation on WSJ data by decreasing dropout and layer drop in each Transformer layer from 0.4 to 0.1: with increased model capacity \\textit{nopos} reduces the WER by 30\\% and gets closer to other positional embeddings, Figure~\\ref{fig:abl:speech:asr_nopos}. For the third point we perform another ablation by comparing with sequence-to-sequence (seq2seq) training: we use exactly the same encoder $\\mathbf{H}^{L_e}$ (with various positional embeddings) but replace last linear layer and CTC loss with the decoder, encoder-decoder attention, and cross-entropy loss where the probability distribution of the transcription is factorized as \\begin{equation*} p(y_1, ..., y_n) = \\prod_{i=1}^n p(y_i \\ | \\ y_0, ..., y_{i-1}, \\mathbf{H}^{L_e}) \\end{equation*} where $y_0$ is a special symbol indicating the beginning of the transcription. The decoder is a stack of 6 Transformer layers with encoding dimension 256, learnable relative positional embedding with 9.6s left-only context and 4 attention heads. Dropout and layer drop in the decoder layers are set to 0.2.  \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.9\\textwidth]{figures/wsj_nopos.pdf} \\caption{ Word error rate comparison for models trained on WSJ data with different positional embeddings. Baseline models, \\textit{nopos} and CAPE, use 0.4 dropout and 0.4 layer drop in every Transformer layer, while \\textit{nopos, dropout=0.1} uses 0.1 for both values.\\label{fig:abl:speech:asr_nopos}} \\end{figure}   \\begin{figure}[htbp!] \\centering \\includegraphics[width=0.85\\textwidth]{figures/tl_s2s.pdf} \\caption{ Word error comparison for CTC and seq2seq models trained on TED-LIUM v3 data without any positional embedding in the encoder (\\textit{nopos}) or with learnable relative positional embedding in every encoder-Transformer layer (\\textit{relpos}).\\label{fig:abl:speech:asr_s2s}} \\end{figure}  In Figure~\\ref{fig:abl:speech:asr_s2s} we show comparison between CTC and seq2seq models trained on TL with either \\textit{nopos} or \\textit{relpos} in the encoder.\\footnote{Encoder remains the same for both CTC and seq2seq models; for seq2seq models decoder is also identical.} Seq2seq \\textit{nopos} performs significantly worse than seq2seq \\textit{relpos} and moreover has higher WER variation on validation data. This result is opposite to the \\textit{nopos} CTC-based training, suggesting that CTC loss is able to train with enough data or model capacity and no positions provided.  These observations only partially overlap with known results: for seq2seq training it was shown recently that relative positions can be modeled via a deep stack of convolutional layers in the encoder~\\cite{mohamed2019transformers} or via convolutions inserted directly in each encoder's Transformer layer~\\cite{zhang2020pushing}. In contrast to the listed works, our encoder has vanilla Transformer layers and only one convolutional layer at the beginning. Thus, \\textit{nopos} model has very limited context to model relative positions, which affects seq2seq training (it has to ``locate'' corresponding timepoint in audio with attention) more than CTC-based, which uses explicit time ordering. In line with this interpretation, for the hybrid ASR systems dropping positional information does not drive to significant deterioration of quality~\\cite{wang2020transformer}.   "
            },
            {
                "section_name": "Machine Translation_11",
                "paragraphs": " "
            },
            {
                "section_name": "Technical Details_13",
                "paragraphs": " For machine translation experiments we have implemented CAPE within ADMIN's~\\cite{liu2019deep,liu2020very} open-sourced code\\footnote{\\scriptsize \\texttt{https://github.com/LiyuanLucasLiu/Transformer-Clinic}} which is based on Fairseq\\footnote{\\scriptsize  \\texttt{https://github.com/pytorch/fairseq}} toolkit~\\cite{ott2019fairseq}. We precisely follow open-sourced recipes for ADMIN with \\textit{sinpos}\\footnote{\\scriptsize  \\texttt{https://github.com/LiyuanLucasLiu/Transformer-Clinic/blob/master/nmt-experiments}: \\texttt{wmt14\\_en-de.md} and \\texttt{wmt14\\_en-fr.md}} including data preparation step; the only change we introduce is usage of different positional embeddings. Besides, we do not perform model averaging as was done in~\\cite{liu2019deep,liu2020very}.  All English-German (DE) models are trained for 150 epochs on 4 GPUs (Volta V100 16GB) for 30h (6l-6L) or 70h (18L-18L). English-French (FR) 6L-6L models are trained for 75 epochs on 8 GPUs (Volta V100 16GB) for 60h. Each configuration is trained starting from 3 different random seeds.  As mentioned in Section~\\ref{sec:mt} we scale positions of source language by a factor $\\alpha$ computed based on train data statistics only as ${\\alpha = \\frac{\\text{\\# tokens in target corpus}}{\\text{\\# tokens in source corpus}} \\in\\mathbb{R}}$: it is set to $\\alpha=1.0337$ for DE and $\\alpha=1.1632$ for FR. For all machine translation experiments with CAPE we skip the mean-normalization step to have source and target sentences aligned at the first position and prepend ``begin of sentence'' in the source sentences to give a hint of first position for the model's decoder (as after global shift there is no way to determine the first position from its positional embedding anymore). Additionally, we do not apply any global scaling. For the global shift we sweep values 5, 10 and 50 while the local shift is set to maximum to preserve positions order, $\\epsilon_{max}=0.5$.  \\begin{table}[h!] \\caption{Slowdown for relpos / no relpos. (Top) 1000 context left/right, (Bottom) 100 context left/right. Run on V-100 GB32, 100 runs, 10 runs warmup, batch 50, emb 768, head 8. \\label{tab:speed_relpos}} \\begin{center} \\resizebox{0.9\\linewidth}{!}{ \\begin{tabular}{@{}ccccccc@{}} \\toprule Model & FP16 Len-10 & FP32 Len-10 & FP16 Len-100 & FP32 Len-100 & FP16 Len-1000 & FP32 Len-1000 \\\\ \\midrule  Transformer layer & 2.1 & 2.3 & 3.3 & 2.3 & 2.2 & 1.7 \\\\ \\midrule Encoder & 2.2 & 2.2 & 2.6 & 1.7 & 2.0 & 1.6 \\\\ \\midrule Decoder & 1.8 & 1.7 & 1.9 & 1.4 & 1.6 & 1.4 \\\\ \\midrule \\midrule  Transformer layer & 2.1 & 2.1 & 1.9 & 1.2 & 1.4 & 1.2 \\\\ \\midrule Encoder & 2.3 & 2.4 & 1.5 & 1.1 & 1.4 & 1.2 \\\\ \\midrule Decoder & 1.7 & 1.7 & 1.2 & 1.1 & 1.2 & 1.1 \\\\ \\bottomrule \\end{tabular} } \\end{center} \\end{table}                                                                                                                                                                                   "
            },
            {
                "section_name": "Computational Cost_14",
                "paragraphs": " For machine translation experiments we estimate the forward and backward time slowdown when learnable relative positional embedding is used in Transformer layer compared to vanilla Transformer layer. Besides we evaluate slowdown for entire encoder and decoder. Forward and backward time benchmarking is done in Fairseq (PyTorch) for different input sequences length (10, 100, 1000), for different context size of relative positional embedding (100 and 1000 tokens to left/right) in full-precision (fp32) and half-precision (fp16) floating-point computations. We use Volta V-100 32GB GPU and report slowdown for average time of forward and backward passes measured across 100 updates with 10 updates of warmup, see Table~\\ref{tab:speed_relpos}. The batch size is set to 50, attention heads are set to 8 and embedding dimension is 768 (typical use case), there are 6 Transformer layers in encoder and decoder. Results in Table~\\ref{tab:speed_relpos} demonstrate that relative positional embedding indeed has additional computational cost which is even more pronounced with mixed-precision computations.   \\end{document}    "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2106.03143/cape_scheme.png",
                "caption": "Example of CAPE's transformations for an image: patches positions are scaled to $[-1, 1]\\times [-1, 1]$; random global shift, local shift, and global scaling are then applied to the grid of positions. \\label{fig:cape} ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_base10.png",
                "caption": "Top-1 accuracy on ImageNet and ImageNet-v2 for ViT models trained with different positional embeddings on $224^2$ resolution (solid) and further fine-tuned on $384^2$ (dashed, ``+ft''). Insets focus on higher accuracies. The full list of top-1 and top-5 accuracies can be found in Appendix~\\ref{app:vision:tech}, Tables~\\ref{tab:vision_all_top1} and~\\ref{tab:vision_all_top5}. \\label{fig:vision:vit_main} ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_size_best.png",
                "caption": "Top-1 accuracy on ImageNet and ImageNet-v2 for ViT models with either \\textit{abspos} or CAPE trained on each particular resolution and UniViT model trained on the mixture of resolutions.\\label{fig:vision:mix_sizes}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_speed_final.png",
                "caption": "Accuracy with respect to throughput on ImageNet at inference time under variation of image resolution. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively. \\label{fig:vision:speed}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj.png",
                "caption": "Word error rate for models trained on WSJ with different positional embeddings.\\label{fig:speech:wsj}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl.png",
                "caption": "Word error rate for models trained on TED-LIUM v3 with different positional embeddings.\\label{fig:speech:tl}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_aug.png",
                "caption": "Validation WER for models trained with different augmentations: ``baseline'' is a model with \\textit{sinpos}, ``+CAPE'' adds CAPE's global and local shifts, ``+SpecAug'' adds SpecAugment.\\label{fig:asr_aug}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/mt_logloss.png",
                "caption": "Average negative log loss across consecutive positions measured on test sets with stacked sentences for models with different positional embeddings: 6L-6L DE (left) and 6L-6L FR (right). \\label{fig:mt}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_abspos_emb_visualization.png",
                "caption": "Visualization of positional embeddings for ViT models trained on $224^2$ resolution: \\textit{abspos} (left), \\textit{sinpos} (middle) and CAPE (right). Each column consists of 3 sub-columns corresponding to input resolutions $160^2$, $224^2$ and $384^2$. Only some components (each 20th out of 768) are shown. When \\textit{sinpos} applied to low-resolution images, spacial aliasing is visible in latest components of embeddings. CAPE's augmentations destruct this patterns and prevent model from over-fitting. \\label{fig:emb_visualization}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_sinpos_emb_visualization.png",
                "caption": "Visualization of positional embeddings for ViT models trained on $224^2$ resolution: \\textit{abspos} (left), \\textit{sinpos} (middle) and CAPE (right). Each column consists of 3 sub-columns corresponding to input resolutions $160^2$, $224^2$ and $384^2$. Only some components (each 20th out of 768) are shown. When \\textit{sinpos} applied to low-resolution images, spacial aliasing is visible in latest components of embeddings. CAPE's augmentations destruct this patterns and prevent model from over-fitting. \\label{fig:emb_visualization}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_cape_emb_visualization.png",
                "caption": "Visualization of positional embeddings for ViT models trained on $224^2$ resolution: \\textit{abspos} (left), \\textit{sinpos} (middle) and CAPE (right). Each column consists of 3 sub-columns corresponding to input resolutions $160^2$, $224^2$ and $384^2$. Only some components (each 20th out of 768) are shown. When \\textit{sinpos} applied to low-resolution images, spacial aliasing is visible in latest components of embeddings. CAPE's augmentations destruct this patterns and prevent model from over-fitting. \\label{fig:emb_visualization}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_sin_ablation.png",
                "caption": "Comparison of top-1 accuracy between \\textit{sinpos} and CAPE trained on either $160^2$, or $224^2$, or $384^2$ resolutions and evaluated across the board. Models trained on $224^2$ resolution and further fine-tuned on $384^2$ resolution are marked with ``+ft''.\\label{fig:abl:vis_sin_vs_cape}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_scale_ablation.png",
                "caption": "Comparison of top-1 accuracy for CAPE with $\\lambda_{max}=1$ (dashed) and $\\lambda_{max}=1.4$ (solid) trained on either $160^2$, or $224^2$, or $384^2$ resolutions and evaluated across the board. Models trained on $224^2$ resolution and further fine-tuned on $384^2$ resolution are marked with ``+ft''.\\label{fig:abl:vis_scale_vs_no_scale_cape}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_ablation.png",
                "caption": " Comparison of top-1 accuracy between \\textit{sinpos} and CAPE with different configurations on global, local shifts and global scaling trained on $224^2$ resolution.\\label{fig:abl:vis_ablations} ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/vit_mix_ablation.png",
                "caption": " Comparison of top-1 accuracy between \\textit{sinpos} and CAPE (with and without global scaling) trained on the mixture of resolutions $\\{128^2, 160^2, 192^2, 224^2, 256^2, 288^2, 320^2\\}$.\\label{fig:abl:vis_univit} ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj_hop_pdf.png",
                "caption": "Hop distance distribution for WSJ (left) and TED-LIUM v3 (right) data. \\label{fig:abl:speech:hop_pdf}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_hop_pdf.png",
                "caption": "Hop distance distribution for WSJ (left) and TED-LIUM v3 (right) data. \\label{fig:abl:speech:hop_pdf}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj_hop.png",
                "caption": "Word error rate comparison for models trained on WSJ data with \\textit{sinpos} or CAPE ($\\lambda=1$) with classical pipeline (solid) or with variable STFT hop distance (dashed).\\label{fig:abl:wsj_hop}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_hop.png",
                "caption": "Word error rate comparison for models trained on TED-LIUM v3 data with \\textit{sinpos} or CAPE ($\\lambda=1$) with classical pipeline (solid) or with variable STFT hop distance (dashed).\\label{fig:abl:tl_hop}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/asr_speed_final.png",
                "caption": "Dependence between throughput and word error rate on TED-LIUM v3 test set for models trained with CAPE ($\\lambda=1)$ on TED-LIUM v3 data: with classical pipeline (orange) and with variable STFT hop distance (purple). Different throughput values correspond to different STFT hop distances in ms (crosses) used during evaluation.\\label{fig:abl:tl_hop_time}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_ablation_global.png",
                "caption": "Word error rate comparison for models trained on TED-LIUM v3 data with CAPE and different global shifts which cover 15, 30 or 60s. The global scale is set to $\\lambda_{max} =1$.\\label{fig:abl:speech:global_shift_tl}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj_local.png",
                "caption": "Word error rate comparison for CAPE models trained on WSJ data with global shift only (solid) or with global and local shifts together (dashed). The global scale is set to be $\\lambda_{max} =1$.\\label{fig:abl:speech:asr_wsj_local}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj_ablation.png",
                "caption": "Word error rate comparison for models trained on WSJ data with different positional embeddings. \\textit{Relpos first layer} refers to a model where \\textit{relpos} is used only in the first Transformer layer with 27.6s context to the left/right and no other positional embeddings are used. \\label{fig:abl:speech:relpos_scale_wsj}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_ablation.png",
                "caption": "Word error rate comparison for models trained on TED-LIUM v3 data with different positional embeddings. \\textit{Relpos first layer} refers to a model where \\textit{relpos} is used only in the first Transformer layer with 27.6s context to the left/right and no other positional embeddings are used. \\label{fig:abl:speech:relpos_scale_tl}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/wsj_nopos.png",
                "caption": " Word error rate comparison for models trained on WSJ data with different positional embeddings. Baseline models, \\textit{nopos} and CAPE, use 0.4 dropout and 0.4 layer drop in every Transformer layer, while \\textit{nopos, dropout=0.1} uses 0.1 for both values.\\label{fig:abl:speech:asr_nopos}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/tl_s2s.png",
                "caption": " Word error comparison for CTC and seq2seq models trained on TED-LIUM v3 data without any positional embedding in the encoder (\\textit{nopos}) or with learnable relative positional embedding in every encoder-Transformer layer (\\textit{relpos}).\\label{fig:abl:speech:asr_s2s}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table1.tex",
                "caption": "Top-1 accuracy (\\%) for ViT and UniViT models evaluated on ImageNet validation set and ImageNet-v2 test sets with images resized to different resolutions: $160^2$, $224^2$, $384^2$ and $672^2$. Models trained on $224^2$ and further fine-tuned on $384^2$ resolution are marked with ``+ft''. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively.\\label{tab:vision_all_top1}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table2.tex",
                "caption": "Top-5 accuracy (\\%) for ViT and UniViT models evaluated on ImageNet validation set and ImageNet-v2 test sets with images resized to different resolutions: $160^2$, $224^2$, $384^2$ and $672^2$. Models trained on $224^2$ and further fine-tuned on $384^2$ resolution are marked with ``+ft''. ``-S'' and ``-Ti'' refer to small and tiny architectures~\\cite{dosovitskiy2020image}, respectively.\\label{tab:vision_all_top5}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table3.tex",
                "caption": "UniViT with CAPE evaluation on ImageNet validation set with different strategies on input resizing. Images are split into 8 bins by minimum spatial size. We report best top-1 accuracy (\\%) in each row (bold) and the drop in accuracy compared to this best accuracy for other columns.\\label{tab:vision:own_size_eval}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table4.tex",
                "caption": "Ablation study on re-scaling positions to the range $[-\\gamma, \\gamma]$ for ViT models trained on $224^2$ images with \\textit{sinpos} or CAPE. We report top-1 accuracy (\\%) on ImageNet validation set evaluated on images scaled to $160^2$ and $384^2$ resolutions.\\label{tab:abl:vision:scaling}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table5.tex",
                "caption": "Statistics on datasets: sampling frequency, duration (in hours), and speech type. \\label{tab:data_asr_stat}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table6.tex",
                "caption": "Statistics on datasets: mean sample duration (in seconds) and mean sample transcription length (in words). \\label{tab:data_asr}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2106.03143/table7.tex",
                "caption": "Slowdown for relpos / no relpos. (Top) 1000 context left/right, (Bottom) 100 context left/right. Run on V-100 GB32, 100 runs, 10 runs warmup, batch 50, emb 768, head 8. \\label{tab:speed_relpos}",
                "description": ""
            }
        ]
    },
    "2005.14165": {
        "title": "Language Models are Few-Shot Learners",
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
        "tldr": "GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "\\label{section:Introduction} Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer.  First, single-layer representations were learned using word vectors \\cite{mikolov2013efficient, pennington2014glove} and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations \\cite{dai2015semi, mccann2017learned, peters2018dissecting} (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models \\cite{vaswani2017attention}  have been directly fine-tuned, entirely removing the need for task-specific architectures \\cite{radford2018gpt1, devlin2018bert, howard2018universal}.   This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms \\cite{raffel2019t5, liu2019roberta, yang2019xlnet, lan2019albert}. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task.  Removing this limitation would be desirable, for several reasons.  First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.  There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.  For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.  Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance \\cite{hendrycks2020pretrained} observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it \\cite{yogatama2019learning, mccoy2019right}. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task \\cite{gururangan2018annotation, niven2019probing}.  Third, humans do not require large supervised datasets to learn most language tasks -- a brief directive in natural language (e.g. ``please tell me if this sentence describes something happy or something sad'') or at most a tiny number of demonstrations (e.g. ``here are two examples of people acting brave; please give a third example of bravery'') is often sufficient to enable a human to perform a new task to at least a reasonable degree of competence.  Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -- it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.  To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.  One potential route towards addressing these issues is meta-learning\\footnote{In the context of language models this has sometimes been called ``zero-shot transfer'', but this term is potentially ambiguous: the method is ``zero-shot'' in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples.  To avoid this confusion, we use the term ``meta-learning'' to capture the inner-loop / outer-loop structure of the general method, and the term ``in context-learning\" to refer to the inner loop of meta-learning. We further specialize the description to ``zero-shot\", ``one-shot\", or ``few-shot\" depending on how many demonstrations are provided at inference time.  These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training -- this is an important issue which we discuss later in the paper, but ``meta-learning'' is intended to encompass both possibilities, and simply describes the inner-outer loop structure.} -- which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure \\ref{figure:metalearning}). Recent work \\cite{radford2019language} attempts to do this via what we call ``in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.   While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -- for example \\cite{radford2019language} achieves only 4\\% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art.  Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.  Another recent trend in language modeling may offer a way forward.  In recent years the capacity of transformer language models has increased substantially, from 100 million parameters \\cite{radford2018gpt1}, to 300 million parameters \\cite{devlin2018bert}, to 1.5 billion parameters \\cite{radford2019language}, to 8 billion parameters \\cite{shoeybi2019megatronlm}, 11 billion parameters \\cite{raffel2019t5}, and finally 17 billion parameters \\cite{turing_17m}.  Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale \\cite{kaplan2020scaling}.  Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.   \\begin{figure} \\begin{center} \\includegraphics[width=0.7\\linewidth]{graphs/aggregate_performance.png} \\end{center} \\caption{\\textbf{Aggregate performance for all 42 accuracy-denominated benchmarks}~~~While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure \\ref{graph:superglue_analysis} for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.} \\label{figure:aggregate_performance} \\end{figure}  In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities.  Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.  For each task, we evaluate GPT-3 under 3 conditions: (a) ``few-shot learning'', or in-context learning where we allow as many demonstrations as will fit into the model\u2019s context window (typically 10 to 100), (b) ``one-shot learning'', where we allow only one demonstration, and (c) ``zero-shot'' learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.  GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.  Figure \\ref{graph:scramble_prompted} illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, $K$. Few-shot learning also improves dramatically with model size.  Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study.  We emphasize that these ``learning'' curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.  Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models).  For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting.  Similarly, GPT-3 achieves 64.3\\% accuracy on TriviaQA in the zero-shot setting, 68.0\\% in the one-shot setting, and 71.2\\% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.  GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once.  We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.  At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3.  This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.  By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure \\ref{figure:aggregate_performance}, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).  We also undertake a systematic study of ``data contamination'' -- a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects.  Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.  In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings.  Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.  Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.  The remainder of this paper is organized as follows.  In Section \\ref{section:Approach}, we describe our approach and methods for training GPT-3 and evaluating it.  Section \\ref{section:Results} presents results on the full range of tasks in the zero-, one- and few-shot settings.  Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} addresses questions of data contamination (train-test overlap).  Section \\ref{section:Limitations} discusses limitations of GPT-3.  Section \\ref{section:Broader_Impacts} discusses broader impacts.  Section \\ref{section:Related Work} reviews related work and Section \\ref{section:Conclusion} concludes.       "
            },
            {
                "section_name": "Approach_2",
                "paragraphs": "\\label{section:Approach}  Our basic pre-training approach, including model, data, and training, is similar to the process described in \\cite{radford2019language}, with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to \\cite{radford2019language}, but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure \\ref{figure:eval strategies} for an illustration):  \\begin{itemize} \\item \\textbf{Fine-Tuning (FT)} has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution \\cite{mccoy2019right}, and the potential to exploit spurious features of the training data \\cite{ gururangan2018annotation, niven2019probing}, potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \\item \\textbf{Few-Shot (FS)} is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning \\cite{radford2019language}, but no weight updates are allowed.  As shown in Figure \\ref{figure:eval strategies}, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving $K$ examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set $K$ in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window ($n_{\\mathrm{ctx}}=2048$).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML~\\citep{hochreiter2001learning, vinyals2016matching} -- both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. \\item \\textbf{One-Shot (1S)} is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. \\item \\textbf{Zero-Shot (0S)} is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.  This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of  pre-training data), but is also the most challenging setting.  In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases ``unfairly hard''.  For example, if someone is asked to ``make a table of world records for the 200m dash'', this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult).  Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks -- for example, in the translation example in Figure \\ref{figure:eval strategies}, a human would likely know what to do from just the text instruction. \\end{itemize}   Figure \\ref{figure:eval strategies} shows the four methods using the example of translating English to French.  In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.  We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.  Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work.  Sections \\ref{section:Model and Architectures}-\\ref{section:Training Process} below give details on our models, training data, and training process respectively. Section \\ref{section:Evaluation} discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.   \\subsection{Model and Architectures} \\label{section:Model and Architectures} We use the same model and architecture as GPT-2 \\cite{radford2019language}, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \\cite{child2019generating}. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.  Previous work \\cite{kaplan2020scaling} suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.    Table \\ref{table:param} shows the sizes and architectures of our 8 models.  Here $n_{\\mathrm{params}}$ is the total number of trainable parameters, $n_{\\mathrm{layers}}$ is the total number of layers, $d_{\\mathrm{model}}$ is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, $d_{\\mathrm{ff}}$ $= 4 \\ast d_{\\mathrm{model}}$), and $d_{\\mathrm{head}}$ is the dimension of each attention head.  All models use a context window of $n_{\\mathrm{ctx}}=2048$ tokens.  We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s.  Previous work \\cite{kaplan2020scaling} suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.   \\subsection{Training Dataset} \\label{section:Training Dataset} Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset\\footnote{\\url{https://commoncrawl.org/the-data/}} \\cite{raffel2019t5} constituting nearly a trillion words.  This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets.  Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.  Details of the first two points (processing of Common Crawl) are described in Appendix \\ref{appendix:common_crawl_filtering}. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset \\cite{radford2019language}, collected by scraping links over a longer period of time, and first described in \\cite{kaplan2020scaling}, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.   Table \\ref{table:dataset} shows the final mixture of datasets that we used in training.  The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens.    Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data.   A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training.  To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.  Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.  \\subsection{Training Process} \\label{section:Training Process} As found in \\cite{kaplan2020scaling, mcc2018batchsize}, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size \\cite{mcc2018batchsize}. Table \\ref{table:param} shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.   All models were trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft.  Details of the training process and hyperparameter settings are described in Appendix \\ref{appendix:model_training}.    \\subsection{Evaluation} \\label{section:Evaluation} For few-shot learning, we evaluate each example in the evaluation set by randomly drawing $K$ examples from that task\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task.  For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set.  For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.  $K$ can be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is $n_{\\mathrm{ctx}}=2048$ for all models and typically fits $10$ to $100$ examples.  Larger values of $K$ are usually but not always better, so when a separate development and test set are available, we experiment with a few values of $K$ on the development set and then run the best value on the test set.  For some tasks (see Appendix \\ref{appendix:task_phrasing}) we also use a natural language prompt in addition to (or for $K=0$, instead of) demonstrations.  On tasks that involve choosing one correct completion from several options (multiple choice), we provide $K$ examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion.  For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing $\\frac{P(\\mathrm{completion} | \\mathrm{context})}{P(\\mathrm{completion} | \\mathrm{answer\\_context})}$, where $\\mathrm{answer\\_context}$ is the string \\texttt{\"Answer: \"} or \\texttt{\"A: \"} and is used to prompt that the completion should be an answer but is otherwise generic.  On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. ``True\" or ``False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by \\cite{raffel2019t5} (see Appendix \\ref{appendix:task_phrasing}) for details.  On tasks with free-form completion, we use beam search with the same parameters as \\cite{raffel2019t5}: a beam width of 4 and a length penalty of $\\alpha = 0.6$.  We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot).  When the test set is private, our model is often too large to fit on the test server, so we report results on the development set.  We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.      "
            },
            {
                "section_name": "Results_3",
                "paragraphs": "\\label{section:Results}  In Figure \\ref{graph:compute} we display training curves for the 8 models described in Section \\ref{section:Approach}. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in \\cite{kaplan2020scaling}, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.  Below, we evaluate the 8 models described in Section \\ref{section:Approach} (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.  In Section \\ref{section:Language Modeling, Cloze, and Completion Tasks} we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks.  In Section \\ref{section:Closed Book Question Answering / Knowledge Base Tasks} we evaluate on ``closed book'' question answering tasks: tasks which require using the information stored in the model\u2019s parameters to answer general knowledge questions.  In Section \\ref{section:Translation} we evaluate the model\u2019s ability to translate between languages (especially one-shot and few-shot).  In Section \\ref{section:Winograd-Style_Tasks} we evaluate the model\u2019s performance on Winograd Schema-like tasks.  In Section \\ref{section:Common_Sense_Reasoning} we evaluate on datasets that involve commonsense reasoning or question answering.  In Section \\ref{section:Reading_Comprehension} we evaluate on reading comprehension tasks, in Section \\ref{section:SuperGLUE} we evaluate on the SuperGLUE benchmark suite, and in \\ref{section:ANLI} we briefly explore NLI.  Finally, in Section \\ref{section:Synthetic_and_Qualitative_Tasks}, we invent some additional tasks designed especially to probe in-context learning abilities -- these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.   \\subsection{Language Modeling, Cloze, and Completion Tasks} \\label{section:Language Modeling, Cloze, and Completion Tasks} In this section we test GPT-3\u2019s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.  \\subsubsection{Language Modeling} \\label{section:Language Modeling} We calculate zero-shot perplexity on the Penn Tree Bank (PTB) \\cite{marcus1994penn} dataset measured in \\cite{radford2019language}. We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set.  PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.   \\subsubsection{LAMBADA} \\label{section:LAMBADA}   The LAMBADA dataset \\cite{paperno2016lambada} tests the modeling of long-range dependencies in text -- the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark. \\cite{bisk2020experience} reflect on the small 1.5\\% improvement achieved by a doubling of model size between two recent state of the art results (\\cite{shoeybi2019megatronlm} and \\cite{turing_17m}) and argue that ``continuing to expand hardware and data sizes by orders of magnitude is not the path forward''. We find that path is still promising and in a zero-shot setting GPT-3 achieves 76\\% on LAMBADA, a gain of 8\\% over the previous state of the art.  LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters \\cite{radford2019language} (which ban ``continuation'' words). The few-shot setting instead allows us to ``frame'' the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:  \\hspace{3cm}Alice was friends with Bob.  Alice went to visit her friend \\underline{\\hspace{1cm}}. $\\to$ Bob  \\hspace{3cm}George bought some baseball equipment, a ball, a glove, and a \\underline{\\hspace{1cm}}. $\\to$  When presented with examples formatted this way, GPT-3 achieves 86.4\\% accuracy in the few-shot setting, an increase of over 18\\% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20\\%, for GPT-3 it improves accuracy by 10\\%. Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern.  One note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data -- however analysis performed in Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} suggests negligible impact on performance.  \\subsubsection{HellaSwag} \\label{section:HellaSwag} The HellaSwag dataset \\cite{zellers2019hellaswag} involves picking the best ending to a story or set of instructions.  The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve 95.6\\% accuracy).  GPT-3 achieves 78.1\\% accuracy in the one-shot setting and 79.3\\% accuracy in the few-shot setting, outperforming the 75.4\\% accuracy of a fine-tuned 1.5B parameter language model \\cite{zellers2019defending} but still a fair amount lower than the overall SOTA of 85.6\\% achieved by the fine-tuned multi-task model ALUM.   \\subsubsection{StoryCloze} \\label{section:StoryCloze} We next evaluate GPT-3 on the StoryCloze 2016 dataset \\cite{mostafazadeh2016corpus}, which involves selecting the correct ending sentence for five-sentence long stories.  Here GPT-3 achieves 83.2\\% in the zero-shot setting and 87.7\\% in the few-shot setting (with $K=70$). This is still 4.1\\% lower than the fine-tuned SOTA using a BERT based model \\cite{li2019story} but improves over previous zero-shot results by roughly 10\\%.      \\subsection{Closed Book Question Answering} \\label{section:Closed Book Question Answering / Knowledge Base Tasks} \\begin{table} \\centering \\begin{tabular}{l l l l} \\toprule Setting & NaturalQS & WebQS & TriviaQA \\\\ \\midrule RAG (Fine-tuned, Open-Domain) \\cite{lewis2020retrieval} & \\textbf{44.5} & \\textbf{45.5} & \\textbf{68.0} \\\\ T5-11B+SSM (Fine-tuned, Closed-Book) \\cite{roberts2020much} & 36.6 & 44.7 & 60.5 \\\\ T5-11B (Fine-tuned, Closed-Book) & 34.5  & 37.4 & 50.1 \\\\ GPT-3 Zero-Shot & 14.6 & 14.4 & 64.3 \\\\ GPT-3 One-Shot & 23.0 & 25.3 & \\textbf{68.0} \\\\ GPT-3 Few-Shot & 29.9 & 41.5 & \\textbf{71.2} \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Results on three Open-Domain QA tasks.} GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings.  TriviaQA few-shot result is evaluated on the wiki split test server.} \\label{table:question} \\end{table}  \\begin{figure} \\vspace{-1em}\\centering\\includegraphics[width=0.8\\linewidth]{graphs/scale_plots/triviaqa-wiki.png} \\caption{On TriviaQA GPT3's performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases.  One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG \\cite{lewis2020retrieval}} \\label{graph:triviaqa} \\end{figure}  In this section we measure GPT-3\u2019s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted ``open-book''. \\cite{roberts2020much} recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as ``closed-book''.  Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3.   We evaluate GPT-3 on the 3 datasets in \\cite{roberts2020much}: Natural Questions \\cite{Kwiatkowski2019nq}, WebQuestions \\cite{berant2013semantic}, and TriviaQA \\cite{joshi2017triviaqa}, using the same splits.  Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q\\&A dataset itself is also not permitted.  The results for GPT-3 are shown in Table \\ref{table:question}. On TriviaQA, we achieve 64.3\\% in the zero-shot setting, 68.0\\% in the one-shot setting, and 71.2\\% in the few-shot setting.  The zero-shot result already outperforms the fine-tuned T5-11B by 14.2\\%, and also outperforms a version with Q\\&A tailored span prediction during pre-training by 3.8\\%.  The one-shot result improves by 3.7\\% and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents \\cite{lewis2020retrieval}. GPT-3's few-shot result further improves performance another 3.2\\% beyond this.  On WebQuestions (WebQs), GPT-3 achieves 14.4\\% in the zero-shot setting, 25.3\\% in the one-shot setting, and 41.5\\% in the few-shot setting.  This compares to 37.4\\% for fine-tuned T5-11B, and 44.7\\% for fine-tuned T5-11B+SSM, which uses a Q\\&A-specific pre-training procedure.  GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models.  Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions and/or the style of their answers are out-of-distribution for GPT-3.  Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.  On Natural Questions (NQs) GPT-3 achieves 14.6\\% in the zero-shot setting, 23.0\\% in the one-shot setting, and 29.9\\% in the few-shot setting, compared to 36.6\\% for fine-tuned T5 11B+SSM.  Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS.  In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3's capacity and broad pretraining distribution.  Overall, on one of the three datasets GPT-3's one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning.  On all 3 datasets, we find that performance scales very smoothly with model size (Figure \\ref{graph:triviaqa} and Appendix \\ref{appendix:results_on_all_tasks} Figure \\ref{graph:all_qa}), possibly reflecting the idea that model capacity translates directly to more \u2018knowledge\u2019 absorbed in the parameters of the model.      \\subsection{Translation} \\label{section:Translation} \\%\\%\\h\\c  For GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in \\ref{section:Training Dataset} the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3's training data is still primarily English (93\\% by word count), it also includes 7\\% of text in other languages. These languages are documented in the \\href{https://github.com/openai/gpt-3}{supplemental material}. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.  Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation \\cite{sennrich2015improving} to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren't strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.     Results are shown in Table \\ref{table:translation}. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation \\cite{liu2019multi}.  Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity.  This is shown in Figure \\ref{graph:translation} in the case of few-shot results, and scaling for all three settings is shown in Appendix \\ref{appendix:results_on_all_tasks}.    \\subsection{Winograd-Style Tasks} \\label{section:Winograd-Style_Tasks} The Winograd Schemas Challenge \\cite{levesque2012winograd} is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human.  Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions such as the adversarially-mined Winogrande dataset \\cite{sakaguchi2019winogrande} still significantly lag human performance.  We test GPT-3\u2019s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.  On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same ``partial evaluation'' method described in \\cite{radford2019language}.  Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section.  On Winograd GPT-3 achieves 88.3\\%, 89.7\\%, and 88.6\\% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance.  We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks}).  On the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves 70.2\\% in the zero-shot setting, 73.2\\% in the one-shot setting, and 77.7\\% in the few-shot setting.  For comparison a fine-tuned RoBERTA model achieves 79\\%, state-of-the-art is 84.6\\% achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by \\cite{sakaguchi2019winogrande} is 94.0\\%.    \\subsection{Common Sense Reasoning} \\label{section:Common_Sense_Reasoning} Next we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA) \\cite{bisk2019piqa}, asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves 81.0\\% accuracy zero-shot, 80.5\\% accuracy one-shot, and 82.8\\% accuracy few-shot (the last measured on PIQA's test server).  This compares favorably to the 79.4\\% accuracy prior state-of-the-art of a fine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10\\% worse than human performance, but GPT-3's few-shot and even zero-shot result outperform the current state-of-the-art.  Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} for details.  ARC \\cite{Clark2018ThinkYH} is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the ``Challenge'' version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to  correctly answer, GPT-3 achieves 51.4\\% accuracy in the zero-shot setting, 53.2\\% in the one-shot setting, and 51.5\\% in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline (55.9\\%) from UnifiedQA \\cite{khashabi2020unifiedqa}. On the ``Easy'' version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8\\%, 71.2\\%, and 70.1\\% which slightly exceeds a fine-tuned RoBERTa baseline from \\cite{khashabi2020unifiedqa}. However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\u2019s few-shot results by 27\\% on the challenge set and 22\\% on the easy set.  On OpenBookQA \\cite{Mihaylov2018CanAS}, GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3's few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.  Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.    \\subsection{Reading Comprehension} \\label{section:Reading_Comprehension} Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3's performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.  GPT-3 performs best (within 3 points of the human baseline) on CoQA \\cite{reddy2019coqa} a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC \\cite{choi2018quac} a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP \\cite{dua2019drop}, a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems \\cite{ran2019numnet}. On SQuAD 2.0 \\cite{rajpurkar2018know}, GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE \\cite{lai2017race}, a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45\\% behind SOTA.    \\begin{table}  \\begin{center} \\begin{tabular}{lcccccc} \\toprule &  SuperGLUE &     BoolQ &        CB &    CB &      COPA &       RTE \\\\ &    Average &  Accuracy &  Accuracy &    F1 &  Accuracy &  Accuracy \\\\ \\midrule Fine-tuned SOTA & \\textbf{89.0} & \\textbf{91.0} & \\textbf{96.9} & \\textbf{93.9} & \\textbf{94.8} & \\textbf{92.5} \\\\ Fine-tuned BERT-Large &       69.0 &      77.4 &      83.6 &  75.7 &      70.6 &      71.7 \\\\ GPT-3 Few-Shot &       71.8 &      76.4 &      75.6 &  52.0 &      92.0 &      69.0 \\\\ \\end{tabular} \\end{center}  \\begin{center} \\begin{tabular}{lcccccc} \\toprule &       WiC &       WSC &   MultiRC &  MultiRC &    ReCoRD &  ReCoRD \\\\ &  Accuracy &  Accuracy &  Accuracy &      F1a &  Accuracy &      F1 \\\\ \\midrule Fine-tuned SOTA &   \\textbf{76.1} &   \\textbf{93.8} &   \\textbf{62.3} &  \\textbf{88.2} &   \\textbf{92.5} & \\textbf{93.3} \\\\ Fine-tuned BERT-Large &      69.6 &      64.6 &      24.1 &     70.0 &      71.3 &    72.0 \\\\ GPT-3 Few-Shot &      49.4 &      80.1 &      30.5 &     75.4 &      90.2 &    91.1 \\\\ \\bottomrule \\end{tabular} \\end{center}  \\caption{ Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. } \\label{table:superglue} \\end{table}  \\begin{figure} \\centering\\includegraphics[width=1.0\\linewidth]{figures/superglue_analysis.png} \\caption{ \\textbf{ Performance on SuperGLUE increases with model size and number of examples in context.} A value of $K=32$ means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table \\ref{table:superglue}). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context. } \\label{graph:superglue_analysis} \\end{figure}  \\subsection{SuperGLUE} \\label{section:SuperGLUE} In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark \\cite{wang2019superglue} \\citep{wang2019superglue} \\citep{clark2019boolq} \\citep{demarneffe:cb} \\citep{roemmele2011choice} \\citep{khashabi2018looking} \\citep{zhang2018record} \\citep{dagan2006pascal} \\citep{bar2006second} \\citep{giampiccolo2007third} \\citep{bentivogli2009fifth} \\citep{pilehvar2018wic} \\citep{poliak2018dnc}. GPT-3\u2019s test-set performance on the SuperGLUE dataset is shown in Table \\ref{table:superglue}.  In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.  We observe a wide range in GPT-3\u2019s performance across tasks.  On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1\\% in the few-shot setting (note that GPT-3 achieves 88.6\\% on the original Winograd dataset as described in Section \\ref{section:Winograd-Style_Tasks}).  On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at 75.6\\% in the few-shot setting.  WiC is a notable weak spot with few-shot performance at 49.4\\% (at random chance).  We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance.  This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) -- GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.  Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure \\ref{graph:superglue_analysis}). We scale $K$ up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of $K$, we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.   \\begin{figure} \\centering\\includegraphics[width=0.8\\linewidth]{graphs/scale_plots/anli_r3_test.png} \\caption{\\textbf{Performance of GPT-3 on ANLI Round 3.} Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2\\%). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.} \\label{graph:anli} \\end{figure} \\subsection{NLI} \\label{section:ANLI} Natural Language Inference (NLI) \\cite{fyodorov2000natural} concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56\\%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset \\cite{nie2019adversarial}. ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ($\\sim33\\%$), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure \\ref{graph:anli} and full results for all rounds can be found in Appendix \\ref{appendix:results_on_all_tasks}. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.  \\subsection{Synthetic and Qualitative Tasks} \\label{section:Synthetic_and_Qualitative_Tasks}  One way to probe GPT-3\u2019s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task.  We devise several tasks to test this class of abilities.  First, we test GPT-3\u2019s ability to perform arithmetic.  Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training.  Third, we test GPT-3\u2019s ability to solve SAT-style analogy problems few-shot.  Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation.  We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.  \\subsubsection{Arithmetic} \\label{section:Arithmetic}  To test GPT-3's ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:  \\begin{itemize} \\item \\textbf{2 digit addition (2D+)} -- The model is asked to add two integers sampled uniformly from $[0, 100)$, phrased in the form of a question, e.g. ``Q: What is 48 plus 76? A: 124.'' \\item \\textbf{2 digit subtraction (2D-)} -- The model is asked to subtract two integers sampled uniformly from $[0, 100)$; the answer may be negative.  Example: ``Q: What is 34 minus 53? A: -19''. \\item \\textbf{3 digit addition (3D+)} -- Same as 2 digit addition, except numbers are uniformly sampled from $[0, 1000)$. \\item \\textbf{3 digit subtraction (3D-)} -- Same as 2 digit subtraction, except numbers are uniformly sampled from $[0, 1000)$. \\item \\textbf{4 digit addition (4D+)} -- Same as 3 digit addition, except uniformly sampled from $[0, 10000)$. \\item \\textbf{4 digit subtraction (4D-)} -- Same as 3 digit subtraction, except uniformly sampled from $[0, 10000)$. \\item \\textbf{5 digit addition (5D+)} -- Same as 3 digit addition, except uniformly sampled from $[0, 100000)$. \\item \\textbf{5 digit subtraction (5D-)} -- Same as 3 digit subtraction, except uniformly sampled from $[0, 100000)$. \\item \\textbf{2 digit multiplication (2Dx)} -- The model is asked to multiply two integers sampled uniformly from $[0, 100)$, e.g. ``Q: What is 24 times 42? A: 1008''. \\item \\textbf{One-digit composite (1DC)} -- The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, ``Q: What is 6+(4*8)? A: 38''. The three 1 digit numbers are selected uniformly on $[0, 10)$ and the operations are selected uniformly from \\{+,-,*\\}. \\end{itemize}  In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.  First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure \\ref{graph:arithmetic}.  On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving 100\\% accuracy on 2 digit addition, 98.9\\% at 2 digit subtraction, 80.2\\% at 3 digit addition, and 94.2\\% at 3-digit subtraction.  Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26\\% accuracy on four digit operations and 9-10\\% accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits.  GPT-3 also achieves 29.2\\% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3\\% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations.   As Figure \\ref{graph:arithmetic} makes clear, small models do poorly on all of these tasks -- even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10\\% of the time.  One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly.  Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly outperforms few-shot learning for all smaller models.  All three settings for the full GPT-3 are shown in Table \\ref{table:arithmetic}, and model capacity scaling for all three settings is shown in Appendix \\ref{appendix:results_on_all_tasks}.  To spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \\texttt{\"<NUM1> + <NUM2> =\"} and \\texttt{\"<NUM1> plus <NUM2>\"}.  Out of 2,000 addition problems we found only 17 matches (0.8\\%) and out of 2,000 subtraction problems we found only 2 matches (0.1\\%), suggesting that only a trivial fraction of the correct answers could have been memorized.  In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a ``1'', suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.  Overall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.    \\subsubsection{Word Scrambling and Manipulation Tasks} \\label{section:Word_Scrambling_and_Manipulation_Tasks}  To test GPT-3's ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 ``character manipulation'' tasks.  Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word.  The 5 tasks are:   \\begin{itemize} \\item \\textbf{Cycle letters in word (CL)} -- The model is given a word with its letters cycled, then the ``='' symbol, and is expected to generate the original word.  For example, it might be given ``lyinevitab'' and should output ``inevitably''. \\item \\textbf{Anagrams of all but first and last characters (A1)} -- The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word.  Example: criroptuon = corruption. \\item \\textbf{Anagrams of all but first and last 2 characters (A2)} -- The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word.  Example: opoepnnt $\\to$ opponent. \\item \\textbf{Random insertion in word (RI)} -- A random punctuation or space character is inserted between each letter of a word, and the model must output the original word.  Example: s.u!c/c!e.s s i/o/n = succession. \\item \\textbf{Reversed words (RW)} -- The model is given a word spelled backwards, and must output the original word.  Example: stcejbo $\\to$ objects. \\end{itemize}  For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by \\cite{norvig2009ngram} of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure \\ref{graph:scramble}.  Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9\\% on removing random insertions, 38.6\\% on cycling letters, 40.2\\% on the easier anagram task, and 15.1\\% on the more difficult anagram task (where only the first and last letters are held fixed).  None of the models can reverse the letters in a word.  In the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table \\ref{table:unscramble}).  This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).  We can further quantify performance by plotting ``in-context learning curves'', which show task performance as a function of the number of in-context examples.  We show in-context learning curves for the Symbol Insertion task in Figure \\ref{graph:scramble_prompted}.  We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.  Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average $\\sim0.7$ words per token), so from the LM\u2019s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.   \\subsubsection{SAT Analogies} \\label{section:SAT_Analogies} To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 ``SAT analogy'' problems \\cite{DBLP:journals/corr/cs-CL-0309035}.  Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005.  A typical example is ``audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation''.  The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is ``sanctimonious is to hypocrisy''.  On this task GPT-3 achieves 65.2\\% in the few-shot setting, 59.1\\% in the one-shot setting, and 53.7\\% in the zero-shot setting, whereas the average score among college applicants was 57\\% \\cite{DBLP:journals/corr/abs-cs-0508103}  (random guessing yields 20\\%).  As shown in Figure \\ref{graph:sat}, the results improve with scale, with the the full 175 billion model improving by over 10\\% compared to the 13 billion parameter model.   \\subsubsection{News Article Generation} \\label{section:News_Article_Generation} Previous work on generative language models qualitatively tested their ability to generate synthetic ``news articles'' by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story \\cite{radford2019language}.  Relative to \\cite{radford2019language}, the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective -- for example GPT-3 often interprets the proposed first sentence of a ``news article'' as a tweet and then posts synthetic responses or follow-up tweets.  To solve this problem we employed GPT-3\u2019s few-shot learning abilities by providing three previous news articles in the model\u2019s context to condition it.  With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the ``news'' genre.  To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. \\cite{kreps-all-the-news} and Zellers et al. \\cite{zellers2019defending}. Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality.\\footnote{This task is also relevant to the potential misuse of language models discussed in Section \\ref{section:misuse_of_language_models}.}  In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website \\href{newser.com}{newser.com} (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model\\footnote{We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on participants drawn from the general US population. See Appendix \\ref{appendix:human_assessment} for details.}. Participants were asked to select whether the article was ``very likely written by a human'', ``more likely written by a human'', ``I don't know'', ``more likely written by a machine'', or ``very likely written by a machine''.  The articles we selected were not in the models\u2019 training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a ``control model'': a 160M parameter model with no context and increased output randomness.  Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was  $\\sim86\\%$\\, where 50\\% is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at $\\sim52\\%$ (see Table \\ref{table:generation}).\\footnote{We use a two-sample Student\u2019s T-Test to test for significant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.} Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.\\footnote{If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50\\%. Indeed, many individual participants scored below 50\\% on this task.} This is true despite the fact that participants spend more time on each output as model size increases (see Appendix \\ref{appendix:human_assessment}).   Examples of synthetic articles from GPT-3 are given in Figures \\ref{completion:newsgen_best} and \\ref{completion:newsgen_worst}.\\footnote{Additional non-news samples can be found in Appendix \\ref{appendix:additional_samples}.} Much of the text is---as indicated by the evaluations---difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.    Related work on language model detection by Ippolito et al. \\cite{ippolito2019automatic} indicates that automatic discriminators like \\textsc{Grover} \\cite{zellers2019defending} and GLTR \\cite{gehrmann2019gltr} may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.  Ippolito et al. \\cite{ippolito2019automatic} also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.  We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was $\\sim88\\%$, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at $\\sim52\\%$ (see Table \\ref{table:generation_long}). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.   \\begin{figure} \\centering\\noindent\\fbox{ \\footnotesize \\tt \\parbox{0.95\\linewidth}{ {\\vspace{0.5em} {\\color{gray} A \"whatpu\" is a small, furry animal native to Tanzania.  An example of a sentence that uses the word whatpu is: \\\\ We were traveling in Africa and we saw these very cute whatpus.\\\\ \\rule{\\linewidth}{0.5pt}\\vspace{0.5em} To do a \"farduddle\" means to jump up and down really fast.  An example of a sentence that uses the word farduddle is:\\\\} \\textbf{One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles.}\\\\ \\r\\l\\v {\\color{gray}A \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is:\\\\} \\textbf{I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there. It was delicious.}\\\\\\rule{\\linewidth}{0.5pt}\\vspace{0.5em} {\\color{gray}A \"Burringo\" is a car with very fast acceleration.  An example of a sentence that uses the word Burringo is:\\\\} \\textbf{In our garage we have a Burringo that my father drives to work every day.}\\\\\\rule{\\linewidth}{0.5pt}\\vspace{0.5em} {\\color{gray}A \"Gigamuru\" is a type of Japanese musical instrument.  An example of a sentence that uses the word Gigamuru is:\\\\} \\textbf{I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.}\\\\\\rule{\\linewidth}{0.5pt}\\vspace{0.5em} {\\color{gray}To \"screeg\" something is to swing a sword at it.  An example of a sentence that uses the word screeg is:\\\\} \\textbf{We screeghed at each other for several minutes and then we went outside and ate ice cream.} }}} \\caption{Representative GPT-3 completions for the few-shot task of using a new word in a sentence.  Boldface is GPT-3\u2019s completions, plain text is human prompts.  In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions.  Nothing task-specific is provided to GPT-3 other than the conditioning shown here.} \\label{completion:novel words} \\end{figure}  \\begin{figure} \\centering\\noindent\\fbox{ \\parbox{0.95\\linewidth}{ {\\vspace{0.5em} \\footnotesize \\tt{\\color{gray}Poor English input: I eated the purple berries.\\\\ Good English output: I ate the purple berries.\\\\ Poor English input: Thank you for picking me as your designer. I'd appreciate it. \\\\ Good English output: Thank you for choosing me as your designer. I appreciate it.\\\\ Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.\\\\ Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.\\\\ Poor English input: I'd be more than happy to work with you in another project.\\\\} \\textbf{Good English output: I'd be more than happy to work with you on another project.} \\\\\\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: Please provide me with a short brief of the design you're looking for and that'd be nice if you could share some examples or project you did before.\\\\} \\textbf{Good English output: Please provide me with a brief description of the design you're looking for and that would be nice if you could share some examples or projects you have done before.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: The patient was died.\\\\} \\textbf{Good English output: The patient died.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: We think that Leslie likes ourselves.\\\\} \\textbf{Good English output: We think that Leslie likes us.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: Janet broke Bill on the finger.\\\\} \\textbf{Good English output: Janet broke Bill's finger.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: Mary arranged for, in St. Louis, John to rent a house cheap.\\\\} \\textbf{Good English output: Mary arranged for John to rent a house in St. Louis.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em}  {\\color{gray}Poor English input: Today I have went to the store to to buys some many bottle of water.\\\\} \\textbf{Good English output: Today I went to the store to buy some bottles of water.}\\\\ \\rule{\\linewidth}{0.8pt}\\vspace{0.5em} {\\color{gray}Poor English input: I have tried to hit ball with bat, but my swing is has miss.\\\\} \\textbf{Good English output: I tried to hit the ball with the bat, but my swing missed.}} }} \\caption{Representative GPT-3 completions for the few-shot task of correcting English grammar.  Boldface is GPT-3\u2019s completions, plain text is human prompts.  In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions.  Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the ``Poor English input/Good English output'' framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \u201cgood\u201d is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning).} \\label{completion:grammar}  \\end{figure} \\subsubsection{Learning and Using Novel Words} \\label{section:Learning_and_Using_Novel_Words} A task studied in developmental linguistics \\cite{carey1978acquiring} is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word\u2019s meaning from only one usage.  Here we qualitatively test GPT-3\u2019s ability to do the former.  Specifically, we give GPT-3 the definition of a nonexistent word, such as ``Gigamuru'', and then ask it to use it in a sentence.  We provide one to five previous examples of a (separate) nonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word.  Table \\ref{completion:novel words} shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3.  These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts.  In all cases the generated sentence appears to be a correct or at least plausible use of the word.  In the final sentence the model generates a plausible conjugation for the word ``screeg'' (namely ``screeghed''), although the use of the word is slightly awkward (``screeghed at each other'') despite being plausible in the sense that it could describe a toy sword fight.  Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.    \\subsubsection{Correcting English Grammar} \\label{section:Correcting_English_Grammar} Another task well suited for few-shot learning is correcting English grammar.  We test this with GPT-3 in the few-shot setting by giving prompts of the form \\texttt{\"Poor English Input: <sentence>{\\textbackslash}n Good English Output: <sentence>\"}.  We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats).  Results are shown in Figure \\ref{completion:grammar}.     "
            },
            {
                "section_name": "Measuring and Preventing Memorization Of Benchmarks_4",
                "paragraphs": "\\begin{figure} \\centering\\includegraphics[width=0.8\\linewidth]{figures/training_curves.png}  \\caption{\\textbf{GPT-3 Training Curves}~~~We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.} \\label{graph:training_curves} \\end{figure} \\label{section:measuring_and_preventing_memorization_of_benchmarks} Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets.  Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.  This concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data \\cite{trinh2018simple} detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 \\cite{radford2019language} also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).  GPT-3 operates in a somewhat different regime.  On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization.  On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure \\ref{graph:training_curves}).  Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.  We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn't feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.  For each benchmark, we produce a `clean' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix \\ref{appendix:test_set_contamination}.  We then evaluate GPT-3 on these clean benchmarks, and compare to the original score.  If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results.  If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure \\ref{graph:contamination}.  Although potential contamination is often high (with a quarter of benchmarks scoring over 50\\%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated.  We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.  Below, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.  \\begin{figure} \\centering\\includegraphics[width=0.9\\linewidth]{figures/contamination_graph.png}  \\caption{\\textbf{Benchmark contamination analysis}~~~ We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset.  Performance on most benchmarks changed negligibly, but some were flagged for further review.  On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section \\ref{section:Results} with an asterisk. We find no evidence that other benchmarks are affected.} \\label{graph:contamination} \\end{figure}  Our analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below: \\begin{itemize} \\item \\textbf{Reading Comprehension:} Our initial analysis flagged $>$90\\% of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question. \\item \\textbf{German translation:} We found 25\\% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU.  Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news. \\item \\textbf{Reversed Words and Anagrams:} Recall that these tasks are of the form ``\\texttt{alaok = koala}\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g ``\\texttt{kayak = kayak}\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal.  Related to this, the symbol insertion task shows high overlap but no effect on performance -- this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches. \\item \\textbf{PIQA:} The overlap analysis flagged 29\\% of examples as contaminated, and observed a 3 percentage point absolute decrease (4\\% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis.  We therefore mark our PIQA results with an asterisk to denote this potential contamination. \\item \\textbf{Winograd:} The overlap analysis flagged 45\\% of examples, and found a 2.6\\% decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk. \\item \\textbf{Language modeling:} We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children's Book Test dataset, to be almost entirely contained in our training data.  Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark. \\end{itemize}  We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed.  These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5\\% of the full dataset.  Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization.  Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.  An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset.  It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier.  However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.  Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity.  Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix \\ref{appendix:test_set_contamination}.     "
            },
            {
                "section_name": "Limitations_5",
                "paragraphs": "\\label{section:Limitations} GPT-3 and our analysis of it have a number of limitations.  Below we describe some of these and suggest directions for future work.  First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks.  On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs.  We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3\u2019s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with ``common sense physics'', despite doing well on some datasets (such as PIQA \\cite{bisk2019piqa}) that test this domain.  Specifically GPT-3 has difficulty with questions of the type ``If I put cheese into the fridge, will it melt?''.  Quantitatively, GPT-3\u2019s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section \\ref{section:Results}, and in particular it does little better than chance when evaluated one-shot or even few-shot on some ``comparison'' tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks.  This is especially striking given GPT-3\u2019s strong few-shot performance on many other tasks.  GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models~\\cite{raffel2019t5}. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer.  This could be a possible explanation for GPT-3's lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE).  We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the ``best of both worlds''.  A more fundamental limitation of the general approach described in this paper -- scaling up any LM-like model, whether autoregressive or bidirectional -- is that it may eventually run into (or could already be running into) the limits of the pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. \\cite{roberts2020much} demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions.  Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world \\cite{bisk2020experience}.  For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary.  Promising future directions in this vein might include learning the objective function from humans \\cite{ziegler2019finetuning}, fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world \\cite{chen2019uniter}.  Another limitation broadly shared by language models is poor sample efficiency during pre-training.  While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime \\cite{linzen2020can}.  Improving pre-training sample efficiency is an important direction for future work, and might come from  grounding in the physical world to provide additional information, or from algorithmic improvements.  A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks ``from scratch'' at inference time, or if it simply recognizes and identifies tasks that it has learned during training.  These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo.  Where GPT-3 is on this spectrum may also vary from task to task.  Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations.  Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.  A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form.  One possible future direction to address this is distillation \\cite{hinton2015distilling} of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible.  Distillation is well-explored in general \\cite{liu2019improving} but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.  Finally, GPT-3 shares some limitations common to most deep learning systems -- its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on.  This last issue -- biases in the data that may lead the model to generate stereotyped or prejudiced content -- is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section \\ref{section:Broader_Impacts}).      "
            },
            {
                "section_name": "Broader Impacts_6",
                "paragraphs": "\\label{section:Broader_Impacts} Language models have a wide range of beneficial applications for society,  including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.  Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them.  The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section \\ref{section:misuse_of_language_models}, and issues of bias, fairness, and representation within models like GPT-3 in Section \\ref{section:Fairness_Bias_and_Representation}.  We also briefly discuss issues of energy efficiency (Section \\ref{section:Energy Usage}).    \\subsection{Misuse of Language Models} \\label{section:misuse_of_language_models} Malicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact \\cite{ross2012risk}.  We discuss three factors: potential misuse applications, threat actors, and external incentive structures.  \\subsubsection{Potential Misuse Applications} \\label{section:Potential_Misuse_Applications} Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.  The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in \\ref{section:News_Article_Generation} represents a concerning milestone in this regard.  \\subsubsection{Threat Actor Analysis} \\label{section:Threat_Actor_Analysis} Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to `advanced persistent threats' (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas \\cite{solaiman2019release}.  To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.  Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for ``targeting'' or ``controlling'' the content of language models are still at a very early stage.      \\subsubsection{External Incentive Structures} \\label{section:External_Incentive_Structures} Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.  Ease of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99\\% of the time, but produces incoherent outputs 1\\% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.  Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.   \\subsection{Fairness, Bias, and Representation} \\label{section:Fairness_Bias_and_Representation} Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms~\\cite{youtube}. We have conducted an analysis of biases in the model in order to better understand GPT-3\u2019s limitations when it comes to fairness, bias, and representation. \\footnote{Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, \\cite{huang2019reducing, nadeem2020stereoset, sheng2019woman}.}   Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model's biases even within the studied categories.  Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data.  Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.    \\subsubsection{Gender} \\label{section:Gender} In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \\texttt{\"The \\{occupation\\} was a\"} (Neutral Variant). 83\\% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \\texttt{\"The detective was a\"} and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.  We also tested how these probabilities changed when we shifted the context to be the \\texttt{\"The competent \\{occupation\\} was a\"} (Competent Variant), and when we shifted the context to be \\texttt{\"The incompetent \\{occupation\\} was a\"} (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with  \\texttt{\"The competent \\{occupation\\} was a,\"} the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \\texttt{\"The \\{occupation\\} was a\"}. With the prompt \\texttt{\"The incompetent \\{occupation\\} was a\"} the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as $\\frac{1}{n_{\\mathrm{jobs}}}\\sum_{\\mathrm{jobs}}\\log(\\frac{ P(\\mathrm{female}|\\mathrm{Context})}{P(\\mathrm{male}|\\mathrm{Context}))})$ -  was $-1.11$ for the Neutral Variant, $-2.14$  for the Competent Variant and $-1.15$ for the Incompetent Variant.   We also carried out pronoun resolution on the Winogender dataset \\cite{rudinger2018gender} using two methods which further corroborated the model's tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \\texttt{\"The advisor met with the advisee because she wanted to get advice about job applications. `She' refers to the\"} and found the option with the lowest probability between the two possible options (Choices between {Occupation Option: advisor; Participant Option: advisee}).  Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17\\%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7\\% vs 76.7\\%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy (60\\%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.  We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-selected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature of 1 and top\\_p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \\texttt{\"He was very\"}, \\texttt{\"She was very\"}, \\texttt{\"He would be described as\"}, \\texttt{\"She would be described as\"}\\footnote{We  only  used  male  and  female  pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which \u2018they\u2019 refers to a singular noun from those where it didn\u2019t, but other forms of gender bias are likely present and could be studied using different approaches.}. We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger \\cite{loper2002nltk}. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.  Table \\ref{table:gender} shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. ``Most Favored'' here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category.  To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.       \\subsubsection{Race} \\label{section:Race} To investigate racial bias in GPT-3, we seeded the model with prompts such as - \\texttt{\"The \\{race\\} man was very\"}, \\texttt{\"The \\{race\\} woman was very\"} and \\texttt{\"People would describe the \\{race\\} person as\"} and generated 800 samples for each of the above prompts, with \\texttt{\\{race\\}} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation \\cite{huang2019reducing}, we explored how race impacted sentiment. We measured sentiment using Senti WordNet \\cite{baccianella2010sentiwordnet} for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).  It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.  Across the models we analyzed, `Asian' had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, 'Black' had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.   \\subsubsection{Religion} \\label{section:Religion} We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length $\\approx$50 with a temperature of 1 and a top $p$ of $0.9$ for every prompt. Our prompts were of the nature \\texttt{\"\\{Religion practitioners\\} are\"} (Eg. \\texttt{\"Christians are\"}) for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.  The following is an example output from the model:\\\\ \\begin{tabularx}{\\linewidth}{X} \\toprule \\texttt{{\\color{gray}\"Buddhists are} \\textbf{divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"}} \\\\ \\bottomrule \\end{tabularx}  Similar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion \\texttt{Islam}, we found that words such as \\texttt{ramadan}, \\texttt{prophet} and \\texttt{mosque} co-occurred at a higher rate than for other religions. We also found that words such as \\texttt{violent}, \\texttt{terrorism} and \\texttt{terrorist} co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3.    \\subsubsection{Future Bias and Fairness Challenges} \\label{section:Future_Bias_and_Fairness_Challenges} We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as  subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from \\cite{1810.03993}.  Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive \\cite{qian2019reducing,huang2019reducing}, so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems  \\cite{blodgett2020language}. Thus, mitigation work should not be approached purely with a metric driven objective to `remove' bias as this has been shown to have blind spots \\cite{gonen2019lipstick, nissim2019fair} but in a holistic manner.   \\subsection{Energy Usage} \\label{section:Energy Usage} Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure \\ref{figure:training flops}). This means we should be cognizant of the cost and efficiency of such models, as advocated by \\cite{schwartz2019}.  The use of large-scale pre-training also gives another lens through which to view the efficiency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation \\cite{liu2019improving} can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation \\cite{hernandez2020}.      "
            },
            {
                "section_name": "Related Work_7",
                "paragraphs": "\\label{section:Related Work} Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters \\cite{jozefowicz2016exploring}. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion.  Work in this vein has successively increased model size: 213 million parameters \\cite{vaswani2017attention} in the original paper, 300 million parameters \\cite{devlin2018bert}, 1.5 billion parameters \\cite{radford2019language}, 8 billion parameters  \\cite{shoeybi2019megatronlm}, 11 billion parameters \\cite{raffel2019t5}, and most recently 17 billion parameters \\cite{turing_17m}.  A second line of work has focused on increasing parameter count but not computation, as a means of increasing models\u2019 capacity to store information without increased computational cost.  These approaches rely on the conditional computation framework \\cite{bengio2013cond} and specifically, the mixture-of-experts method \\cite{shazeer2017outrageously} has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models \\cite{aharoni201950B}, though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time \\cite{graves2016act} and  the universal transformer \\cite{dehghani2018ut}. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy.  Several efforts have also systematically studied the effect of scale on language model performance.  \\cite{kaplan2020scaling,rosenfeld2019constructive,li2020train,hestness2017deep}, find a smooth power-law trend in loss as autoregressive language models are scaled up.  This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure \\ref{graph:compute}), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling.  Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible.  This approach includes ALBERT \\cite{lan2019albert} as well as general \\cite{hinton2015distilling} and task-specific \\cite{sanh2019distilbert,jiao2019tinybert,yoon2016sequencedistil} approaches to distillation of language models.  These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models.  As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering \\cite{Kwiatkowski2019nq,Iyyer2014quiz, Clark2018ThinkYH,Mihaylov2018CanAS}, reading comprehension \\cite{choi2018quac,reddy2019coqa}, and adversarially constructed datasets designed to be difficult for existing language models \\cite{sakaguchi2019winogrande,nie2019adversarial}.  In this work we test our models on many of these datasets.  Many previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on.  Recent efforts include~\\citep{raffel2019t5, roberts2020much}, which fine-tuned an 11 billion parameter language model, and~\\cite{guu2020realm}, which focused on attending over a large corpus of data at test time.  Our work differs in focusing on in-context learning but could be combined in the future with those of \\cite{guu2020realm,lewis2020retrieval}.   Metalearning in language models has been utilized in \\cite{radford2019language}, though with much more limited results and no systematic study.  More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general.  Here there is an extensive literature, including matching networks~\\citep{vinyals2016matching}, RL2 \\cite{Duan2016RL2FR},  learning to optimize~\\citep{ravi2016optimization, andrychowicz2016learning, li2017learning} and MAML \\cite{Finn2017ModelAgnosticMF}.  Our approach of stuffing the model\u2019s context with previous examples is most structurally similar to RL2 and also resembles~\\cite{hochreiter2001learning}, in that an inner loop of adaptation takes place through computation in the model\u2019s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in ~\\cite{reed2017few} and~\\cite{gu2018meta} studied low-resource NMT as a few-shot learning problem.  While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning \\cite{schick2020exploiting}. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA \\cite{xie2019unsupervised} also explore methods of fine-tuning when very little labeled data is available.  Giving multi-task models instructions in natural language was first formalized in a supervised setting with \\cite{mccann2018natural} and utilized for some tasks (such as summarizing) in a language model with \\cite{radford2019language}.  The notion of presenting tasks in natural language was also explored in the text-to-text transformer  \\cite{raffel2019t5}, although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates.  Another approach to increasing generality and transfer-learning capability in language models is multi-task learning \\cite{caruana1997multitask}, which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one.  If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task.  Multi-task learning has shown some promising initial results~\\citep{liu2015representation, liu2018generating} and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets \\cite{phang2018sentence} and pushed the boundaries on certain tasks \\cite{khashabi2020unifiedqa}, but is still limited by the need to manually curate collections of datasets and set up training curricula.  By contrast pre-training at large enough scale appears to offer a ``natural'' broad distribution of tasks implicitly contained in predicting the text itself.  One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation \\cite{tobin2017domain}, human interaction \\cite{Ziegler2019FineTuningLM}, or active learning \\cite{mackay1992active}.  Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality \\cite{devlin2018bert}, prefixLM \\cite{dai2015semi} and encoder-decoder architectures \\cite{lewis2019bart,raffel2019t5}, random permutations during training \\cite{yang2019xlnet}, architectures that improve the efficiency of sampling \\cite{dai2019transformerXL}, improvements in data and training procedures \\cite{liu2019roberta}, and efficiency increases in the embedding parameters \\cite{lan2019albert}.  Many of these techniques provide significant gains on downstream tasks.  In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations.  However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3\u2019s scale with these algorithmic techniques is a promising direction for future work.        "
            },
            {
                "section_name": "Conclusion_8",
                "paragraphs": "\\label{section:Conclusion} We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of state-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly.  We documented roughly predictable trends of scaling in performance without using fine-tuning.  We also discussed the social impacts of this class of model.  Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.  \\section*{Acknowledgements} The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper.  Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI's infrastructure.  Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design.  Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText).  Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale. \\newpage \\section*{Contributions} \\label{sec:contributions} \\textbf{Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu} implemented the large-scale models, training infrastructure, and model-parallel strategies.  \\textbf{Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder} conducted pre-training experiments.  \\textbf{Ben Mann and Alec Radford} collected, filtered, deduplicated, and conducted overlap analysis on the training data.  \\textbf{Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry} implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks.  \\textbf{Jared Kaplan and Sam McCandlish} initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research.  \\textbf{Ben Mann} implemented sampling without replacement during training.  \\textbf{Alec Radford} originally demonstrated few-shot learning occurs in language models.  \\textbf{Jared Kaplan and Sam McCandlish} showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods.  \\textbf{Prafulla Dhariwal} implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training.  \\textbf{Rewon Child and Mark Chen} developed an early version of our model-parallel strategy.  \\textbf{Rewon Child and Scott Gray} contributed the sparse transformer.  \\textbf{Aditya Ramesh} experimented with loss scaling strategies for pretraining.  \\textbf{Melanie Subbiah and Arvind Neelakantan} implemented, experimented with, and tested beam search.  \\textbf{Pranav Shyam} worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature.  \\textbf{Sandhini Agarwal} conducted the fairness and representation analysis.  \\textbf{Girish Sastry and Amanda Askell} conducted the human evaluations of the model.  \\textbf{Ariel Herbert-Voss} conducted the threat analysis of malicious use.  \\textbf{Gretchen Krueger} edited and red-teamed the policy sections of the paper.  \\textbf{Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner} optimized OpenAI\u2019s clusters to run the largest models efficiently.  \\textbf{Scott Gray} developed fast GPU kernels used during training.  \\textbf{Jack Clark} led the analysis of ethical impacts \u2014 fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.  \\textbf{Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark} wrote the paper.  \\textbf{Sam McCandlish} led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.  \\textbf{Alec Radford} advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training.  \\textbf{Ilya Sutskever} was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work.  \\textbf{Dario Amodei} designed and led the research.   \\appendix \\clearpage \\newpage "
            },
            {
                "section_name": "Details of Common Crawl Filtering_9",
                "paragraphs": "\\label{appendix:common_crawl_filtering} As mentioned in Section \\ref{section:Training Dataset}, we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication:  \\begin{enumerate}  \\item In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark's standard tokenizer and HashingTF \\footnote{\\url{https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\\#pyspark.ml.feature.HashingTF}}. For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl.  We used this classifier to score Common Crawl documents. We kept each document in our dataset iff  \\[\\verb|np.random.pareto|(\\alpha) > 1 - \\verb|document_score|\\]  We chose $\\alpha = 9$ in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. $\\alpha$ was chosen to match the distribution of scores from our classifier on WebText.  We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. \\item To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark's MinHashLSH implementation with 10 hashes, using the same features as were used for classification above.  We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10\\%.  \\end{enumerate}  After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix \\ref{appendix:test_set_contamination}. "
            },
            {
                "section_name": "Details of Model Training_10",
                "paragraphs": "\\label{appendix:model_training} To train all versions of GPT-3, we use Adam with $\\beta_1=0.9$, $\\beta_2=0.95$, and $\\epsilon=10^{-8}$, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10\\% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10\\% of the original learning rate).  There is a linear LR warmup over the first 375 million tokens.  We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.  Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting. All models use weight decay of 0.1 to provide a small amount of regularization \\cite{loshchilov2017decoupled}.  During training we always train on sequences of the full $n_{\\mathrm{ctx}}=2048$ token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency.  Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated.  This allows for efficient training without need for any special sequence-specific masking. "
            },
            {
                "section_name": "Details of Test Set Contamination Studies_11",
                "paragraphs": "\\label{appendix:test_set_contamination} In section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results.  \\paragraph{Initial training set filtering} We attempted to remove text occurring in benchmarks from training data by searching for $13-$gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding $13-$gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than $200$ characters long were discarded. Documents split into more than 10 pieces were considered contaminated and removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored $13-$grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository\\footnote{\\url{https://github.com/openai/gpt-3/blob/master/overlap_frequency.md}}.  \\paragraph{Overlap methodology} For our benchmark overlap analysis in Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks}, we used a variable number of words $N$ to check for overlap for each dataset, where $N$ is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of $N$ we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for $N$ and the amount of data marked as dirty are shown in Table \\ref{table:overlap_master}. Unlike GPT-2's use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40\\% of our filtered Common Crawl documents per Section \\ref{table:dataset}.  We define a `dirty' example as one with any $N$-gram overlap with any training document, and a `clean' example as one with no collision.  Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children's Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table \\ref{table:overlap_master}   \\paragraph{Overlap results} To understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1\\% or 2\\% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly \\emph{better}, our filtering scheme may have preferentially marked easier examples as dirty.  This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94\\% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6\\% of examples that remain after filtering come from a slightly different distribution than the dirty examples.  Figure \\ref{graph:contamination} shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section \\ref{section:measuring_and_preventing_memorization_of_benchmarks} for details on the datasets we flagged for further review.  \\clearpage "
            },
            {
                "section_name": "Total Compute Used to Train Language Models_12",
                "paragraphs": "\\label{appendix:total_compute_calculations}    This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure \\ref{figure:training flops}. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10\\% of the total compute for the models we are analyzing.  Calculations can be seen in Table \\ref{table:total_compute_calculations} and are explained within the table caption.  "
            },
            {
                "section_name": "Human Quality Assessment of Synthetic News Articles_13",
                "paragraphs": "\\label{appendix:human_assessment} This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the $\\sim200$ word news articles, and then describe the preliminary investigation of $\\sim500$ word news articles generated by GPT-3.  \\textit{Participants:} We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded  for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was $\\sim38$ years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid \\$12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once.  \\textit{Procedure and design:} We arbitrarily selected 25 news articles that appeared in \\href{newser.com}{newser.com} in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants\u2019 judgments.  The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text.  In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuffled for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz.   \\textit{Statistical Tests:} To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the \\verb|scipy.stats.ttest_ind| function. When plotting a regression line in the graph of average participant accuracy vs model size, we fit a power law of the form $ax^{-b}$. The 95\\% confidence intervals were estimated from the t-distribution of the sample mean.   \\textit{Duration statistics}: In the main text, we discussed the finding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure \\ref{graph:newsduration}. Lower accuracy scores despite increased time investment from participants supports the finding that larger models generate harder-to-distinguish news articles.  \\textit{Preliminary investigation of $\\sim500$ word articles:} We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table \\ref{table:study_long}). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid \\$12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.  "
            },
            {
                "section_name": "Additional Samples from GPT-3_14",
                "paragraphs": "\\label{appendix:additional_samples} GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper.  As an example, in Figure \\ref{sample:FakeStevensPoem}, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens.  We first experimented with a few prompts, then generated four  samples with no additional editing or selection (sampling at temperature $1$ using nucleus sampling \\cite{DBLP:journals/corr/abs-1904-09751} with $P=0.9$). Completions  were truncated when the model began to write a new title and author heading, or broke into prose commentary.   \\begin{figure} { \\tt \\scriptsize \\begin{center}\\begin{tabular}{l l} \\toprule Context $\\to$ & The City \\\\ & BY C. P. CAVAFY  \\\\ & TRANSLATED BY EDMUND KEELEY \\\\ & [Poem text omitted] \\\\ \\\\ \\\\ & SOME TREES \\\\ & John Ashbery \\\\ & [Poem text omitted] \\\\ \\\\ \\\\ & Shadows on the Way \\\\ & Wallace Stevens \\\\ \\midrule & \\\\ -------- Generated Poem 1  -------- &  -------- Generated Poem 3  -------- \\\\ & \\\\ I must have shadows on the way & The sun was all we had. Now, in the shade\\\\ If I am to walk I must have & All is changed. The mind must dwell on those\\\\ Each step taken slowly and alone & White fields, that to its eyes were always old;\\\\ To have it ready made & Those ancient gleams, convoluting\\\\ & The way they lay among the huge roots,\\\\ And I must think in lines of grey & The great dark tomes of reverie,\\\\ To have dim thoughts to be my guide & The plumed labyrinth of the sea.\\\\ Must look on blue and green & \\\\ And never let my eye forget & When our own breath gonne faint and fade\\\\ That color is my friend & Up the long stairway of the dead\\\\ And purple must surround me too & We shall remember well\\\\ & The blindness of the light, the whiteness\\\\ The yellow of the sun is no more & Of that white land. The footsteps, and\\\\ Intrusive than the bluish snow & The grasses in that light, and in the shade\\\\ That falls on all of us. I must have & The menacing life.\\\\ Grey thoughts and blue thoughts walk with me & \\\\ If I am to go away at all. & We played, too, once, in the warmest rooms.\\\\ & To one content, there is one content\\\\ -------- Generated Poem 2  --------  & For all mankind. The forest glades\\\\ & Are all the more for being in fable,\\\\ There is a way in the world. & And wood and town in legend shadowed-\\\\ That way a man may live in a small town & Once more to return. The one content\\\\ His lifetime and see no man come, & Has in it every happiness, more brave\\\\ Never the stir of men about a market, & For being purposeless; and so\\\\ Never the bobbing of covered carts, & Those island gardens, deep in the mind,\\\\ Never the creak passed on the stair & Make of remoteness familiarity\\\\ Closing some other life; an old man & And of the unspoken name a friend.\\\\ Can prosper in that town, can prosper & \\\\ When his store is seen to close, his wife &  -------- Generated Poem 4  --------  \\\\ Fades in his house, and the world shouts in vain & \\\\ That no one comes to hear him & Nobody will come to this place. It is a road that leads nowhere.\\\\ Where his small bell tolls. & The solitude is deep. The mountains are high.\\\\ In that small town & But they are desolate, and they turn the traveler\u2019s face\\\\ Everything is known. & Towards the North. All the sounds of the world are far away.\\\\ The ways of the young men are known, & When the wind rises above the trees,\\\\ And the ways of the young women & The boughs bow to the ground.\\\\ As they walk to the Spring; & Even the birds that inhabit the tangle of weeds\\\\ The rumors are all known, & That is the roadside cover, are silent. One listens,\\\\ The quarrels, the hurt, the envy & But hears no roar of the forest. One is alone.\\\\ Of the men and the women. & One will be taken.\\\\ Till a day comes & One will be taken.\\\\ That way out of the world & There is no utterance, there is no conversation,\\\\ That leads to a wood. & But one is uneasy all the same....\\\\ Young, the young man & There is a thin blue mist,\\\\ Walks, a lover now, & A darkness rising like smoke,\\\\ Knows the streets as they are and is of them. & And within that darkness\\\\ & A possession of the heart.\\\\ Comes to the wood. & One will be taken.... It was here, and it will be here again-\\\\ & Here, under this sky empty and full of light.\\\\ There, in the wood, among trees, & \\\\ He sees shadows on the way, hears voices, & \\\\ hears the wind and the rustling of leaves; & \\\\ Through an open glade & \\\\ He sees a shape and the shape hears: & \\\\ It waits as he waits, & \\\\ Waits as the shadows wait, & \\\\ As the voices wait; & \\\\ Shadows on the way, voices in the wind. & \\\\ & \\\\ \\\\ \\bottomrule \\end{tabular}\\end{center} } \\caption{Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title `Shadows on the Way'.} \\label{sample:FakeStevensPoem} \\end{figure} \\clearpage \\newpage "
            },
            {
                "section_name": "Details of Task Phrasing and Specifications_15",
                "paragraphs": "\\label{appendix:task_phrasing} The following figures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here.   \\begin{figure}[!h] { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Article: \\\\ & Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic ---- as long as they occur in are respectful and intelligent manner. \\\\ & In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they're trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. \\\\ & As a general rule,it's best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one's salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host's team. \\\\ &  \\\\ & Q: What shouldn't you do when talking about sports with colleagues from another country? \\\\ &  \\\\ & A: Criticizing the sports of your colleagues' country. \\\\ &  \\\\ & Q: Which is typically a friendly topic in most places according to the author? \\\\ &  \\\\ & A: Sports. \\\\ &  \\\\ & Q: Why are people from Asia more private in their conversation with others? \\\\ &  \\\\ & A: They don't want to have their good relationship with others harmed by informal conversation. \\\\ &  \\\\ & Q: The author considers politics and religion   \\_  . \\\\ &  \\\\ & A: \\\\ \\midrule Correct Answer $\\to$ & taboo \\\\ Incorrect Answer $\\to$ & cheerful topics \\\\ Incorrect Answer $\\to$ & rude topics \\\\ Incorrect Answer $\\to$ & topics that can never be talked about\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in \\ref{section:Approach}.}  \\label{eval:RACE-h}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & anli 2: anli 2: The Gold Coast Hotel \\& Casino is a hotel and casino located in Paradise, Nevada. This locals' casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile ($\\sim1.6\\mathrm{km}$) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. \\\\ & Question: The Gold Coast is a budget-friendly casino. True, False, or Neither? \\\\ \\midrule Correct Answer $\\to$ & Neither \\\\ Incorrect Answer $\\to$ & True \\\\ Incorrect Answer $\\to$ & False\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ANLI R2}  \\label{eval:ANLI R2}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Article: \\\\ & Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. \\\\ & Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. \\\\ & Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. \\\\ & Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you'll be really happy.\" \\\\ &  \\\\ & Q: Which of the following is True according to the passage? \\\\ &  \\\\ & A: If a kid hated four people,he or she had to carry four potatoes. \\\\ &  \\\\ & Q: We can learn from the passage that we should  \\_  . \\\\ &  \\\\ & A: throw away the hatred inside \\\\ &  \\\\ & Q: The children complained about  \\_  besides the weight trouble. \\\\ &  \\\\ & A: the smell \\\\ &  \\\\ & Q: Mrs.Smith asked her students to write    \\_  on the potatoes. \\\\ &  \\\\ & A: \\\\ \\midrule Correct Answer $\\to$ & names \\\\ Incorrect Answer $\\to$ & numbers \\\\ Incorrect Answer $\\to$ & time \\\\ Incorrect Answer $\\to$ & places\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in \\ref{section:Approach}.}  \\label{eval:RACE-m}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & How to apply sealant to wood. \\\\ \\midrule Correct Answer $\\to$ & Using a brush, brush on sealant onto wood until it is fully saturated with the sealant. \\\\ Incorrect Answer $\\to$ & Using a brush, drip on sealant onto wood until it is fully saturated with the sealant.\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for PIQA}  \\label{eval:PIQA}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & My body cast a shadow over the grass because \\\\ \\midrule Correct Answer $\\to$ & the sun was rising. \\\\ Incorrect Answer $\\to$ & the grass was cut.\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for COPA}  \\label{eval:COPA}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to \"Second Amendment people\" in a speech and warned that the words that politicians use can incite violence and undermine democracy. \"Trump's words are an incitement to the type of political violence that touched me personally,\" Rabin wrote in USAToday. He said that Trump's appeal to \"Second Amendment people\" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- \"were a new level of ugliness in an ugly campaign season.\" \\\\ & \\\\ &   - The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. \\\\ &   - Warns of \"parallels\" between Israel of the 1990s and the U.S. today. \\\\ \\midrule Correct Answer $\\to$ & - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump's aggressive rhetoric. \\\\  Correct Answer $\\to$ & - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump's aggressive rhetoric. \\\\  Incorrect Answer $\\to$ & - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton's aggressive rhetoric. \\\\ Incorrect Answer $\\to$ & - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.'s aggressive rhetoric. \\\\ Incorrect Answer $\\to$ & - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin's aggressive rhetoric. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ReCoRD. We consider the context above to be a single \"problem\" because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script.}  \\label{eval:ReCoRD}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health \\& Sport. He also serves on the Justice and Education \\& Skills committees in the Scottish Parliament. \\\\ & Question: Fulton James MacGregor is a Scottish politican who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither? \\\\ \\midrule Correct Answer $\\to$ & Neither \\\\ Incorrect Answer $\\to$ & True \\\\ Incorrect Answer $\\to$ & False\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ANLI R1}  \\label{eval:ANLI R1}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Organisms require energy in order to do what? \\\\ \\midrule Correct Answer $\\to$ & mature and develop. \\\\ Incorrect Answer $\\to$ & rest soundly. \\\\ Incorrect Answer $\\to$ & absorb light. \\\\ Incorrect Answer $\\to$ & take in nutrients.\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in \\ref{section:Approach}.}  \\label{eval:OpenBookQA}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They \\\\ \\midrule Correct Answer $\\to$ & bake them, then frost and decorate. \\\\ Incorrect Answer $\\to$ & taste them as they place them on plates. \\\\ Incorrect Answer $\\to$ & put the frosting on the cake as they pan it. \\\\ Incorrect Answer $\\to$ & come out and begin decorating the cake as well.\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for HellaSwag}  \\label{eval:HellaSwag}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: \\$43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. \\\\ & Question: The loophole is now gone True, False, or Neither? \\\\ \\midrule Correct Answer $\\to$ & False \\\\ Incorrect Answer $\\to$ & True \\\\ Incorrect Answer $\\to$ & Neither\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ANLI R3}  \\label{eval:ANLI R3}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? \\\\ & Answer: \\\\ \\midrule Correct Answer $\\to$ & dry palms \\\\ Incorrect Answer $\\to$ & wet palms \\\\ Incorrect Answer $\\to$ & palms covered with oil \\\\ Incorrect Answer $\\to$ & palms covered with lotion\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in \\ref{section:Approach}.}  \\label{eval:ARC (Challenge)}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & lull is to trust as \\\\ \\midrule Correct Answer $\\to$ & cajole is to compliance \\\\ Incorrect Answer $\\to$ & balk is to fortitude \\\\ Incorrect Answer $\\to$ & betray is to loyalty \\\\ Incorrect Answer $\\to$ & hinder is to destination \\\\ Incorrect Answer $\\to$ & soothe is to passion\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for SAT Analogies}  \\label{eval:SAT Analogies}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Correct Context $\\to$ & Grace was happy to trade me her sweater for my jacket. She thinks the sweater\\\\ Incorrect Context $\\to$ & Grace was happy to trade me her sweater for my jacket. She thinks the jacket\\\\ \\midrule Target Completion $\\to$ & looks dowdy on her. \\\\ \\bottomrule \\end{tabularx} } \\caption{Formatted dataset example for Winograd.  The `partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.}  \\label{eval:Winograd}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Correct Context $\\to$ & Johnny likes fruits more than vegetables in his new keto diet because the fruits \\\\ Incorrect Context $\\to$ & Johnny likes fruits more than vegetables in his new keto diet because the vegetables \\\\ \\midrule Target Completion $\\to$ & are saccharine. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Winogrande. The `partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.}  \\label{eval:Winogrande}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & READING COMPREHENSION ANSWER KEY \\\\ & While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department's counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government's legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President's life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he'd help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued. \\\\ & \\\\ & Who did The State Department feel should visit both India and Pakistan? \\\\ \\midrule Correct Answer $\\to$ & - [False] Bin Laden \\\\ Incorrect Answer $\\to$ & - [True] Bin Laden\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use $K$ to refer to the number of \\textbf{questions} shown within the context.   }  \\label{eval:MultiRC}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Question: Which factor will most likely cause a person to develop a fever? \\\\ & Answer: \\\\ \\midrule Correct Answer $\\to$ & a bacterial population in the bloodstream \\\\ Incorrect Answer $\\to$ & a leg muscle relaxing after exercise \\\\ Incorrect Answer $\\to$ & several viral particles on the skin \\\\ Incorrect Answer $\\to$ & carbohydrates being digested in the stomach\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in \\ref{section:Approach}.}  \\label{eval:ARC (Easy)}  \\end{figure} \\clearpage \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home. \\\\ \\midrule Correct Answer $\\to$ & Bob believed that there were good people in the world. \\\\ Incorrect Answer $\\to$ & Bob contemplated how unfriendly the world was.\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for StoryCloze}  \\label{eval:StoryCloze}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. \\\\ &  \\\\ & The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75\\% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. \\\\ &  \\\\ & Q: what is the most populous municipality in Finland? \\\\ &  \\\\ & A: Helsinki \\\\ &  \\\\ & Q: how many people live there? \\\\ &  \\\\ & A: 1.4 million in the metropolitan area \\\\ &  \\\\ & Q: what percent of the foreign companies that operate in Finland are in Helsinki? \\\\ &  \\\\ & A: 75\\% \\\\ &  \\\\ & Q: what towns are a part of the metropolitan area? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for CoQA}  \\label{eval:CoQA}  \\end{figure}     \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Please unscramble the letters into a word, and write that word: \\\\ & asinoc = \\\\ \\midrule Target Completion $\\to$ & casino\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Cycled Letters} \\label{eval:CL} \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Passage: Saint Jean de Br\\'ebeuf  was a French Jesuit missionary who travelled to New France  in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Br\\'ebeuf and another missionary were captured when an Iroquois raid took over a Huron village . Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Br\\'ebeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. \\\\ & Question: How many years did Saint Jean de Br\\'ebeuf stay in New France before he went back to France for a few years? \\\\ & Answer: \\\\ \\midrule Target Completion $\\to$ & 4 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for DROP}  \\label{eval:DROP}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Fill in blank: \\\\ &  \\\\ & She held the torch in front of her. \\\\ &  \\\\ & She caught her breath. \\\\ &  \\\\ & \"Chris? There's a step.\" \\\\ &  \\\\ & \"What?\" \\\\ &  \\\\ & \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there's more than a \\_\\_\\_\\_. -\\ensuremath{>} \\\\ \\midrule Target Completion $\\to$ & step\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for LAMBADA}  \\label{eval:LAMBADA}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Please unscramble the letters into a word, and write that word: \\\\ & skicts = \\\\ \\midrule Target Completion $\\to$ & sticks\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Anagrams 1 (A1)}  \\label{eval:A1}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Please unscramble the letters into a word, and write that word: \\\\ & volwskagen = \\\\ \\midrule Target Completion $\\to$ & volkswagen\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Anagrams 2}  \\label{eval:A2}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: Who played tess on touched by an angel? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & Delloreese Patricia Early (July 6, 1931 {\\textendash} November 19, 2017), known professionally as Della Reese \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Natural Questions}  \\label{eval:NQs}  \\end{figure}   \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & TITLE: William Perry (American football) - Professional career \\\\ & PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan.  Perry's \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\"  While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents' goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team's success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman.  His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12.  Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa). \\\\ &  \\\\ & Q: what team did he play for? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & the Chicago Bears \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for QuAC}  \\label{eval:Quac}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Please unscramble the letters into a word, and write that word: \\\\ & r e!c.i p r o.c a/l = \\\\ \\midrule Target Completion $\\to$ & reciprocal\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Symbol Insertion}  \\label{eval:RI}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Please unscramble the letters into a word, and write that word:\\\\ & taefed = \\\\ \\midrule Target Completion $\\to$ & defeat\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Reversed Words}  \\label{eval:RW}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Title: The\\_Blitz \\\\ &  \\\\ & Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Ger\\\"at beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Ger\\\"at, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger\\\"at. \\\\ &  \\\\ & Q: How many sorties were flown in March 1941? \\\\ &  \\\\ & A: 4,000 \\\\ &  \\\\ & Q: When did the Luftwaffe fly inland missions? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & only on moonlit nights \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for SQuADv2}  \\label{eval:SQuADv2}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g ({\\textbackslash}displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. \\\\ & question: is the normal force equal to the force of gravity? \\\\ & answer: \\\\ \\midrule Target Completion $\\to$ & yes\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for BoolQ}  \\label{eval:BoolQ}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there's still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn't mean Manhattan comes cheap. \\\\ & question: Manhattan comes cheap. true, false, or neither? \\\\ & answer: \\\\ \\midrule Target Completion $\\to$ & false\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for CB}  \\label{eval:CB}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. \\\\ & question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? \\\\ & answer: \\\\ \\midrule Target Completion $\\to$ & False\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for RTE}  \\label{eval:RTE}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & An outfitter provided everything needed for the safari. \\\\ & Before his first walking holiday, he went to a specialist outfitter to buy some boots. \\\\ & question: Is the word `outfitter' used in the same way in the two sentences above? \\\\ & answer: \\\\ \\midrule Target Completion $\\to$ & no\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for WiC}  \\label{eval:WiC}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Final Exam with Answer Key \\\\ & Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. \\\\ & ===== \\\\ & Passage: Mr. Moncrieff visited Chester's luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward's allowance on the ground that he no longer requires *his* financial support. \\\\ & Question: In the passage above, what does the pronoun \"*his*\" refer to? \\\\ & Answer: \\\\ \\midrule Target Completion $\\to$ & mr. moncrieff\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for WSC}  \\label{eval:WSC}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: `Nude Descending A Staircase' is perhaps the most famous painting by which 20th century artist? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & MARCEL DUCHAMP \\\\ Target Completion $\\to$ & r mutt \\\\ Target Completion $\\to$ & duchamp \\\\ Target Completion $\\to$ & marcel duchamp \\\\ Target Completion $\\to$ & R.Mutt \\\\ Target Completion $\\to$ & Marcel duChamp \\\\ Target Completion $\\to$ & Henri-Robert-Marcel Duchamp \\\\ Target Completion $\\to$ & Marcel du Champ \\\\ Target Completion $\\to$ & henri robert marcel duchamp \\\\ Target Completion $\\to$ & Duchampian \\\\ Target Completion $\\to$ & Duchamp \\\\ Target Completion $\\to$ & duchampian \\\\ Target Completion $\\to$ & marcel du champ \\\\ Target Completion $\\to$ & Marcel Duchamp \\\\ Target Completion $\\to$ & MARCEL DUCHAMP\\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions.}  \\label{eval:TriviaQA}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What school did burne hogarth establish? \\\\ &  \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & School of Visual Arts \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for WebQA}  \\label{eval:WebQA-no-dev}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Keinesfalls d\\\"urfen diese f\\\"ur den kommerziellen Gebrauch verwendet werden. = \\\\ \\midrule Target Completion $\\to$ & In no case may they be used for commercial purposes. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for De$\\to$En.  This is the format for one- and few-shot learning, for this and other langauge tasks, the format for zero-shot learning is ``Q: What is the \\{language\\} translation of \\{sentence\\} A: \\{translation\\}.\" }  \\label{eval:De_En}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & In no case may they be used for commercial purposes. = \\\\ \\midrule Target Completion $\\to$ & Keinesfalls d\\\"urfen diese f\\\"ur den kommerziellen Gebrauch verwendet werden. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for En$\\to$De}  \\label{eval:En_De}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. = \\\\ \\midrule Target Completion $\\to$ & L'analyse de la distribution de fr\\'equence des stades larvaires d'I. verticalis dans une s\\'erie d'\\'etangs a \\'egalement d\\'emontr\\'e que les larves m\\^ales \\'etaient \\`a des stades plus avanc\\'es que les larves femelles. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for En$\\to$Fr}  \\label{eval:En_Fr}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & L'analyse de la distribution de fr\\'equence des stades larvaires d'I. verticalis dans une s\\'erie d'\\'etangs a \\'egalement d\\'emontr\\'e que les larves m\\^ales \\'etaient \\`a des stades plus avanc\\'es que les larves femelles. = \\\\ \\midrule Target Completion $\\to$ & Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Fr$\\to$En}  \\label{eval:Fr_En}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey's accession to the European Union, despite Turkey's continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. = \\\\ \\midrule Target Completion $\\to$ & Adev\\u{a}rul este c\\u{a} v\\u{a} dori\\c{t}i, cu orice pre\\c{t} \\c{s}i {\\^\\i}mpotriva dorin\\c{t}ei europenilor, s\\u{a} continua\\c{t}i negocierile de aderare a Turciei la Uniunea European\\u{a}, {\\^\\i}n ciuda refuzului continuu al Turciei de a recunoa\\c{s}te Ciprul \\c{s}i {\\^\\i}n ciuda faptului c\\u{a} reformele democratice au ajuns {\\^\\i}ntr-un punct mort. \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for En$\\to$Ro}  \\label{eval:En_Ro}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Adev\\u{a}rul este c\\u{a} v\\u{a} dori\\c{t}i, cu orice pre\\c{t} \\c{s}i {\\^\\i}mpotriva dorin\\c{t}ei europenilor, s\\u{a} continua\\c{t}i negocierile de aderare a Turciei la Uniunea European\\u{a}, {\\^\\i}n ciuda refuzului continuu al Turciei de a recunoa\\c{s}te Ciprul \\c{s}i {\\^\\i}n ciuda faptului c\\u{a} reformele democratice au ajuns {\\^\\i}ntr-un punct mort. = \\\\ \\midrule Target Completion $\\to$ & The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey's accession to the European Union, despite Turkey's continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. \\\\  \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Ro$\\to$En}  \\label{eval:Ro_En}  \\end{figure}   \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is (2 * 4) * 6? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 48 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 1DC}  \\label{eval:single_digit_three_ops}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 17 minus 14? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 3 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 2D-}  \\label{eval:two_digit_subtraction}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 98 plus 45? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 143 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 2D+}  \\label{eval:two_digit_addition}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 95 times 45? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 4275 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 2Dx}  \\label{eval:two_digit_multiplication}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 509 minus 488? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 21 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 3D-}  \\label{eval:three_digit_subtraction}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 556 plus 497? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 1053 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 3D+}  \\label{eval:three_digit_addition}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 6209 minus 3365? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 2844 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 4D-}  \\label{eval:four_digit_subtraction}  \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 9923 plus 617? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 10540 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 4D+}  \\label{eval:four_digit_addition}  \\end{figure}    \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q:  What is 40649 minus 78746? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & -38097 \\\\ \\bottomrule \\end{tabularx} } \\caption{Formatted dataset example for Arithmetic 5D$-$} \\label{eval:five_digit_subtraction} \\end{figure} \\begin{figure} { \\tt \\footnotesize \\begin{tabularx}{\\linewidth}{r X} \\toprule Context $\\to$ & Q: What is 65360 plus 16204? \\\\ & A: \\\\ \\midrule Target Completion $\\to$ & 81564 \\\\ \\bottomrule \\end{tabularx} }  \\caption{Formatted dataset example for Arithmetic 5D+}  \\label{eval:five_digit_addition}  \\end{figure}  \\clearpage \\newpage "
            },
            {
                "section_name": "Results on All Tasks for All Model Sizes_16",
                "paragraphs": "\\label{appendix:results_on_all_tasks}       \\begin{table}[!h] { \\setlength\\tabcolsep{1pt} \\centering \\tiny \\begin{adjustwidth}{-.8in}{-.8in} \\begin{center} \\begin{tabular}{llccl<{\\hspace{4ex}}llllllll<{\\hspace{4 ex}}llllllll<{\\hspace{4ex}}llllllllc} \\toprule & & & & & \\multicolumn{8}{c}{Zero-Shot} & \\multicolumn{8}{c}{One-Shot} & \\multicolumn{8}{c}{Few-Shot}\\\\ \\cmidrule(r{4ex}){6-13} \\cmidrule(r{4ex}){14-21} \\cmidrule(r){22-30} Name &   Metric  & Split &  \\shortstack{Fine-tune\\\\SOTA} &    K &     Small &  Med &  Large &    XL &  2.7B &  6.7B &   13B &  175B &    Small &  Med &  Large &    XL &  2.7B &  6.7B &   13B &  175B &    Small &  Med &  Large &    XL &  2.7B &  6.7B &   13B &   175B & \\shortstack{175B \\\\(test server)} \\\\ \\midrule  HellaSwag &      acc &    dev &           85.6 &   20 &      33.7 &    43.6 &   51.0 &  54.7 &  62.8 &  67.4 &  70.9 &  78.9 &     33.0 &    42.9 &   50.5 &  53.5 &  61.9 &  66.5 &  70.0 &  78.1 &     33.5 &    43.1 &   51.3 &  54.9 &  62.9 &  67.3 &  71.3 &   79.3 &                      \\\\ LAMBADA &      acc &   test &           68.0 &   15 &      42.7 &    54.3 &   60.4 &  63.6 &  67.1 &  70.3 &  72.5 &  76.2 &     22.0 &    47.1 &   52.6 &  58.3 &  61.1 &  65.4 &  69.0 &  72.5 &     22.0 &    40.4 &   63.2 &  57.0 &  78.1 &  79.1 &  81.3 &   86.4 &                      \\\\ LAMBADA &      ppl &   test &           8.63 &   15 &      18.6 &    9.09 &   6.53 &  5.44 &  4.60 &  4.00 &  3.56 &  3.00 &    165.0 &    11.6 &   8.29 &  6.46 &  5.53 &  4.61 &  4.06 &  3.35 &    165.0 &    27.6 &   6.63 &  7.45 &  2.89 &  2.56 &  2.56 &   1.92 &                      \\\\ StoryCloze &      acc &   test &           91.8 &   70 &      63.3 &    68.5 &   72.4 &  73.4 &  77.2 &  77.7 &  79.5 &  83.2 &     62.3 &    68.7 &   72.3 &  74.2 &  77.3 &  78.7 &  79.7 &  84.7 &     62.3 &    70.2 &   73.9 &  76.1 &  80.2 &  81.2 &  83.0 &   87.7 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ NQs &      acc &   test &           44.5 &   64 &      0.64 &    1.75 &   2.71 &  4.40 &  6.01 &  5.79 &  7.84 &  14.6 &     1.19 &    3.07 &   4.79 &  5.43 &  8.73 &  9.78 &  13.7 &  23.0 &     1.72 &    4.46 &   7.89 &  9.72 &  13.2 &  17.0 &  21.0 &   29.9 &                      \\\\ TriviaQA &      acc &    dev &           68.0 &   64 &      4.15 &    7.61 &   14.0 &  19.7 &  31.3 &  38.7 &  41.8 &  64.3 &     4.19 &    12.9 &   20.5 &  26.5 &  35.9 &  44.4 &  51.3 &  68.0 &     6.96 &    16.3 &   26.5 &  32.1 &  42.3 &  51.6 &  57.5 &   71.2 &                 71.2 \\\\ WebQs &      acc &   test &           45.5 &   64 &      1.77 &    3.20 &   4.33 &  4.63 &  7.92 &  7.73 &  8.22 &  14.4 &     2.56 &    6.20 &   8.51 &  9.15 &  14.5 &  15.1 &  19.0 &  25.3 &     5.46 &    12.6 &   15.9 &  19.6 &  24.8 &  27.7 &  33.5 &   41.5 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ Ro$\\to$En 16 &  BLEU-mb &   test &           39.9 &   64 &      2.08 &    2.71 &   3.09 &  3.15 &  16.3 &  8.34 &  20.2 &  19.9 &     0.55 &    15.4 &   23.0 &  26.3 &  30.6 &  33.2 &  35.6 &  38.6 &     1.25 &    20.7 &   25.8 &  29.2 &  33.1 &  34.8 &  37.0 &   39.5 &                      \\\\ Ro$\\to$En 16 &  BLEU-sb &   test &                &   64 &      2.39 &    3.08 &   3.49 &  3.56 &  16.8 &  8.75 &  20.8 &  20.9 &     0.65 &    15.9 &   23.6 &  26.8 &  31.3 &  34.2 &  36.7 &  40.0 &     1.40 &    21.3 &   26.6 &  30.1 &  34.3 &  36.2 &  38.4 &   41.3 &                      \\\\ En$\\to$Ro 16 &  BLEU-mb &   test &           38.5 &   64 &      2.14 &    2.65 &   2.53 &  2.50 &  3.46 &  4.24 &  5.32 &  14.1 &     0.35 &    3.30 &   7.89 &  8.72 &  13.2 &  15.1 &  17.3 &  20.6 &     1.25 &    5.90 &   9.33 &  10.7 &  14.3 &  16.3 &  18.0 &   21.0 &                      \\\\ En$\\to$Ro 16 &  BLEU-sb &   test &                &   64 &      2.61 &    3.11 &   3.07 &  3.09 &  4.26 &  5.31 &  6.43 &  18.0 &     0.55 &    3.90 &   9.15 &  10.3 &  15.7 &  18.2 &  20.8 &  24.9 &     1.64 &    7.40 &   10.9 &  12.9 &  17.2 &  19.6 &  21.8 &   25.8 &                      \\\\ Fr$\\to$En 14 &  BLEU-mb &   test &           35.0 &   64 &      1.81 &    2.53 &   3.47 &  3.13 &  20.6 &  15.1 &  21.8 &  21.2 &     1.28 &    15.9 &   23.7 &  26.3 &  29.0 &  30.5 &  30.2 &  33.7 &     4.98 &    25.5 &   28.5 &  31.1 &  33.7 &  34.9 &  36.6 &   39.2 &                      \\\\ Fr$\\to$En 14 &  BLEU-sb &   test &                &   64 &      2.29 &    2.99 &   3.90 &  3.60 &  21.2 &  15.5 &  22.4 &  21.9 &     1.50 &    16.3 &   24.4 &  27.0 &  30.0 &  31.6 &  31.4 &  35.6 &     5.30 &    26.2 &   29.5 &  32.2 &  35.1 &  36.4 &  38.3 &   41.4 &                      \\\\ En$\\to$Fr 14 &  BLEU-mb &   test &           45.6 &   64 &      1.74 &    2.16 &   2.73 &  2.15 &  15.1 &  8.82 &  12.0 &  25.2 &     0.49 &    8.00 &   14.8 &  15.9 &  20.3 &  23.3 &  24.9 &  28.3 &     4.08 &    14.5 &   19.3 &  21.5 &  24.9 &  27.3 &  29.5 &   32.6 &                      \\\\ En$\\to$Fr 14 &  BLEU-sb &   test &           45.9 &   64 &      2.44 &    2.75 &   3.54 &  2.82 &  19.3 &  11.4 &  15.3 &  31.3 &     0.81 &    10.0 &   18.2 &  19.3 &  24.7 &  28.3 &  30.1 &  34.1 &     5.31 &    18.0 &   23.6 &  26.1 &  30.3 &  33.3 &  35.5 &   39.9 &                      \\\\ De$\\to$En 16 &  BLEU-mb &   test &           40.2 &   64 &      2.06 &    2.87 &   3.41 &  3.63 &  21.5 &  17.3 &  23.0 &  27.2 &     0.83 &    16.2 &   22.5 &  24.7 &  28.2 &  30.7 &  33.0 &  30.4 &     3.25 &    22.7 &   26.2 &  29.2 &  32.7 &  34.8 &  37.3 &   40.6 &                      \\\\ De$\\to$En 16 &  BLEU-sb &   test &                &   64 &      2.39 &    3.27 &   3.85 &  4.04 &  22.5 &  18.2 &  24.4 &  28.6 &     0.93 &    17.1 &   23.4 &  25.8 &  29.2 &  31.9 &  34.5 &  32.1 &     3.60 &    23.8 &   27.5 &  30.5 &  34.1 &  36.5 &  39.1 &   43.0 &                      \\\\ En$\\to$De 16 &  BLEU-mb &   test &            41.2 &   64 &      1.70 &    2.27 &   2.31 &  2.43 &  12.9 &  8.66 &  10.4 &  24.6 &     0.50 &    7.00 &   12.9 &  13.1 &  18.3 &  20.9 &  22.5 &  26.2 &     3.42 &    12.3 &   15.4 &  17.1 &  20.9 &  23.0 &  26.6 &   29.7 &                      \\\\ En$\\to$De 16 &  BLEU-sb &   test &           41.2 &   64 &      2.09 &    2.65 &   2.75 &  2.92 &  13.7 &  9.36 &  11.0 &  25.3 &     0.54 &    7.40 &   13.4 &  13.4 &  18.8 &  21.7 &  23.3 &  27.3 &     3.78 &    12.9 &   16.1 &  17.7 &  21.7 &  24.1 &  27.7 &   30.9 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ Winograd &      acc &   test &           93.8 &    7 &      66.3 &    72.9 &   74.7 &  76.9 &  82.4 &  85.7 &  87.9 &  88.3 &     63.4 &    68.5 &   72.9 &  76.9 &  82.4 &  84.6 &  86.1 &  89.7 &     63.4 &    67.4 &   73.6 &  76.9 &  84.3 &  85.4 &  82.4 &   88.6 &                      \\\\ Winogrande &      acc &    dev &           84.6 &   50 &      52.0 &    52.1 &   57.4 &  58.7 &  62.3 &  64.5 &  67.9 &  70.2 &     51.3 &    53.0 &   58.3 &  59.1 &  61.7 &  65.8 &  66.9 &  73.2 &     51.3 &    52.6 &   57.5 &  59.1 &  62.6 &  67.4 &  70.0 &   77.7 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ PIQA &      acc &    dev &           77.1 &   50 &      64.6 &    70.2 &   72.9 &  75.1 &  75.6 &  78.0 &  78.5 &  81.0 &     64.3 &    69.3 &   71.8 &  74.4 &  74.3 &  76.3 &  77.8 &  80.5 &     64.3 &    69.4 &   72.0 &  74.3 &  75.4 &  77.8 &  79.9 &   82.3 &                 82.8 \\\\ ARC (Challenge) &      acc &   test &           78.5 &   50 &      26.6 &    29.5 &   31.8 &  35.5 &  38.0 &  41.4 &  43.7 &  51.4 &     25.5 &    30.2 &   31.6 &  36.4 &  38.4 &  41.5 &  43.1 &  53.2 &     25.5 &    28.4 &   32.3 &  36.7 &  39.5 &  43.7 &  44.8 &   51.5 &                      \\\\ ARC (Easy) &      acc &   test &           92.0 &   50 &      43.6 &    46.5 &   53.0 &  53.8 &  58.2 &  60.2 &  63.8 &  68.8 &     42.7 &    48.2 &   54.6 &  55.9 &  60.3 &  62.6 &  66.8 &  71.2 &     42.7 &    51.0 &   58.1 &  59.1 &  62.1 &  65.8 &  69.1 &   70.1 &                      \\\\ OpenBookQA &      acc &   test &           87.2 &  100 &      35.6 &    43.2 &   45.2 &  46.8 &  53.0 &  50.4 &  55.6 &  57.6 &     37.0 &    39.8 &   46.2 &  46.4 &  53.4 &  53.0 &  55.8 &  58.8 &     37.0 &    43.6 &   48.0 &  50.6 &  55.6 &  55.2 &  60.8 &   65.4 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ Quac &       f1 &    dev &           74.4 &    5 &      21.2 &    26.8 &   31.0 &  30.1 &  34.7 &  36.1 &  38.4 &  41.5 &     21.1 &    26.9 &   31.9 &  32.3 &  37.4 &  39.0 &  40.6 &  43.4 &     21.6 &    27.6 &   32.9 &  34.2 &  38.2 &  39.9 &  40.9 &   44.3 &                      \\\\ RACE-h &      acc &   test &           90.0 &   10 &      35.2 &    37.9 &   40.1 &  40.9 &  42.4 &  44.1 &  44.6 &  45.5 &     34.3 &    37.7 &   40.0 &  42.0 &  43.8 &  44.3 &  44.6 &  45.9 &     34.3 &    37.0 &   40.4 &  41.4 &  42.3 &  44.7 &  45.1 &   46.8 &                      \\\\ RACE-m &      acc &   test &           93.1 &   10 &      42.1 &    47.2 &   52.1 &  52.3 &  54.7 &  54.4 &  56.7 &  58.4 &     42.3 &    47.3 &   51.7 &  55.2 &  56.1 &  54.7 &  56.9 &  57.4 &     42.3 &    47.0 &   52.7 &  53.0 &  55.6 &  55.4 &  58.1 &   58.1 &                      \\\\ SQuADv2 &       em &    dev &           90.7 &   16 &      22.6 &    32.8 &   33.9 &  43.1 &  43.6 &  45.4 &  49.0 &  52.6 &     25.1 &    37.5 &   37.9 &  47.9 &  47.9 &  51.1 &  56.0 &  60.1 &     27.5 &    40.5 &   39.2 &  53.5 &  50.0 &  56.6 &  62.6 &   64.9 &                      \\\\ SQuADv2 &       f1 &    dev &           93.0 &   16 &      28.3 &    40.2 &   41.4 &  50.3 &  51.0 &  52.7 &  56.3 &  59.5 &     30.1 &    43.6 &   44.1 &  54.0 &  54.1 &  57.1 &  61.8 &  65.4 &     32.1 &    45.5 &   44.9 &  58.7 &  55.9 &  62.1 &  67.7 &   69.8 &                      \\\\ CoQA &       f1 &    dev &           90.7 &    5 &      34.5 &    55.0 &   61.8 &  65.3 &  71.1 &  72.8 &  76.3 &  81.5 &     30.6 &    52.1 &   61.6 &  66.1 &  71.8 &  75.1 &  77.9 &  84.0 &     31.1 &    52.0 &   62.7 &  66.8 &  73.2 &  77.3 &  79.9 &   85.0 &                      \\\\ DROP &       f1 &    dev &           89.1 &   20 &      9.40 &    13.6 &   14.4 &  16.4 &  19.7 &  17.0 &  24.0 &  23.6 &     11.7 &    18.1 &   20.9 &  23.0 &  26.4 &  27.3 &  29.2 &  34.3 &     12.9 &    18.7 &   24.0 &  25.6 &  29.7 &  29.7 &  32.3 &   36.5 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ BoolQ &      acc &    dev &           91.0 &   32 &      49.7 &    60.3 &   58.9 &  62.4 &  67.1 &  65.4 &  66.2 &  60.5 &     52.6 &    61.7 &   60.4 &  63.7 &  68.4 &  68.7 &  69.0 &  76.7 &     43.1 &    60.6 &   62.0 &  64.1 &  70.3 &  70.0 &  70.2 &   77.5 &                 76.4 \\\\ CB &      acc &    dev &           96.9 &   32 &      0.00 &    32.1 &   8.93 &  19.6 &  19.6 &  28.6 &  19.6 &  46.4 &     55.4 &    53.6 &   53.6 &  48.2 &  57.1 &  33.9 &  55.4 &  64.3 &     42.9 &    58.9 &   53.6 &  69.6 &  67.9 &  60.7 &  66.1 &   82.1 &                 75.6 \\\\ CB &       f1 &    dev &           93.9 &   32 &      0.00 &    29.3 &   11.4 &  17.4 &  22.4 &  25.1 &  20.3 &  42.8 &     60.1 &    39.8 &   45.6 &  37.5 &  45.7 &  28.5 &  44.6 &  52.5 &     26.1 &    40.4 &   32.6 &  48.3 &  45.7 &  44.6 &  46.0 &   57.2 &                 52.0 \\\\ Copa &      acc &    dev &           94.8 &   32 &      66.0 &    68.0 &   73.0 &  77.0 &  76.0 &  80.0 &  84.0 &  91.0 &     62.0 &    64.0 &   66.0 &  74.0 &  76.0 &  82.0 &  86.0 &  87.0 &     67.0 &    64.0 &   72.0 &  77.0 &  83.0 &  83.0 &  86.0 &   92.0 &                 92.0 \\\\ RTE &      acc &    dev &           92.5 &   32 &      47.7 &    49.8 &   48.4 &  56.0 &  46.6 &  55.2 &  62.8 &  63.5 &     53.1 &    47.3 &   49.5 &  49.5 &  54.9 &  54.9 &  56.3 &  70.4 &     52.3 &    48.4 &   46.9 &  50.9 &  56.3 &  49.5 &  60.6 &   72.9 &                 69.0 \\\\ WiC &      acc &    dev &           76.1 &   32 &      0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.00 &     50.0 &    50.3 &   50.3 &  49.2 &  49.4 &  50.3 &  50.0 &  48.6 &     49.8 &    55.0 &   53.0 &  53.0 &  51.6 &  53.1 &  51.1 &   55.3 &                 49.4 \\\\ WSC &      acc &    dev &           93.8 &   32 &      59.6 &    56.7 &   65.4 &  61.5 &  66.3 &  60.6 &  64.4 &  65.4 &     58.7 &    58.7 &   60.6 &  62.5 &  66.3 &  60.6 &  66.3 &  69.2 &     58.7 &    60.6 &   54.8 &  49.0 &  62.5 &  67.3 &  75.0 &   75.0 &                 80.1 \\\\ MultiRC &      acc &    dev &           62.3 &   32 &      4.72 &    9.65 &   12.3 &  13.6 &  14.3 &  18.4 &  24.2 &  27.6 &     4.72 &    9.65 &   12.3 &  13.6 &  14.3 &  18.4 &  24.2 &  27.6 &     6.09 &    11.8 &   16.8 &  20.8 &  24.7 &  23.8 &  25.0 &   32.5 &                 30.5 \\\\ MultiRC &      f1a &    dev &           88.2 &   32 &      57.0 &    59.7 &   60.4 &  59.9 &  60.0 &  64.5 &  71.4 &  72.9 &     57.0 &    59.7 &   60.4 &  59.9 &  60.0 &  64.5 &  71.4 &  72.9 &     45.0 &    55.9 &   64.2 &  65.4 &  69.5 &  66.4 &  69.3 &   74.8 &                 75.4 \\\\ ReCoRD &      acc &    dev &           92.5 &   32 &      70.8 &    78.5 &   82.1 &  84.1 &  86.2 &  88.6 &  89.0 &  90.2 &     69.8 &    77.0 &   80.7 &  83.0 &  85.9 &  88.0 &  88.8 &  90.2 &     69.8 &    77.2 &   81.3 &  83.1 &  86.6 &  87.9 &  88.9 &   89.0 &                 90.2 \\\\ ReCoRD &       f1 &    dev &           93.3 &   32 &      71.9 &    79.2 &   82.8 &  85.2 &  87.3 &  89.5 &  90.4 &  91.0 &     70.7 &    77.8 &   81.6 &  83.9 &  86.8 &  88.8 &  89.7 &  91.2 &     70.7 &    77.9 &   82.1 &  84.0 &  87.5 &  88.8 &  89.8 &   90.1 &                 91.1 \\\\ SuperGLUE &  average &    dev &           89.0 &      &      40.6 &    47.4 &   46.8 &  49.6 &  50.1 &  52.3 &  54.4 &  58.2 &     54.4 &    55.1 &   56.7 &  57.8 &  61.2 &  59.7 &  64.3 &  68.9 &     50.2 &    56.2 &   56.8 &  60.0 &  64.3 &  63.6 &  66.9 &   73.2 &                 71.8 \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ ANLI R1 &      acc &   test &           73.8 &   50 &      33.4 &    34.2 &   33.4 &  33.4 &  34.2 &  32.3 &  33.2 &  34.6 &     32.1 &    31.6 &   31.9 &  34.6 &  30.6 &  31.6 &  32.7 &  32.0 &     32.1 &    32.5 &   30.9 &  32.5 &  33.5 &  33.1 &  33.3 &   36.8 &                      \\\\ ANLI R2 &      acc &   test &           50.7 &   50 &      33.2 &    31.9 &   33.3 &  33.3 &  33.8 &  33.5 &  33.5 &  35.4 &     35.7 &    33.7 &   33.2 &  32.7 &  32.7 &  33.9 &  33.9 &  33.9 &     35.7 &    33.8 &   32.1 &  31.4 &  32.6 &  33.3 &  32.6 &   34.0 &                      \\\\ ANLI R3 &      acc &   test &           48.3 &   50 &      33.6 &    34.0 &   33.8 &  33.4 &  35.3 &  34.8 &  34.4 &  34.5 &     35.0 &    32.6 &   33.0 &  33.9 &  34.1 &  33.1 &  32.5 &  35.1 &     35.0 &    34.4 &   35.1 &  36.0 &  32.7 &  33.9 &  34.5 &   40.2 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ 2D+ &      acc &    n/a &                &   50 &      0.70 &    0.65 &   0.70 &  0.85 &  1.10 &  2.54 &  15.4 &  76.9 &     2.00 &    0.55 &   3.15 &  4.00 &  12.1 &  19.6 &  73.0 &  99.6 &     2.00 &    4.10 &   3.50 &  4.50 &  8.90 &  11.9 &  55.5 &  100.0 &                      \\\\ 2D- &      acc &    n/a &                &   50 &      1.25 &    1.25 &   1.25 &  1.25 &  1.60 &  7.60 &  12.6 &  58.0 &     1.15 &    0.95 &   1.45 &  1.95 &  3.85 &  11.5 &  44.6 &  86.4 &     1.15 &    1.45 &   2.25 &  2.70 &  7.35 &  13.6 &  52.4 &   98.9 &                      \\\\ 3D+ &      acc &    n/a &                &   50 &      0.10 &    0.10 &   0.05 &  0.10 &  0.10 &  0.25 &  1.40 &  34.2 &     0.15 &    0.00 &   0.10 &  0.30 &  0.45 &  0.95 &  15.4 &  65.5 &     0.15 &    0.45 &   0.30 &  0.55 &  0.75 &  0.90 &  8.40 &   80.4 &                      \\\\ 3D- &      acc &    n/a &                &   50 &      0.05 &    0.05 &   0.05 &  0.05 &  0.05 &  0.45 &  1.35 &  48.3 &     0.05 &    0.15 &   0.25 &  0.30 &  0.55 &  1.60 &  6.15 &  78.7 &     0.05 &    0.10 &   0.15 &  0.35 &  0.65 &  1.05 &  9.20 &   94.2 &                      \\\\ 4D+ &      acc &    n/a &                &   50 &      0.05 &    0.05 &   0.00 &  0.00 &  0.05 &  0.05 &  0.15 &  4.00 &     0.00 &    0.00 &   0.10 &  0.00 &  0.00 &  0.10 &  0.80 &  14.0 &     0.00 &    0.05 &   0.05 &  0.00 &  0.15 &  0.15 &  0.40 &   25.5 &                      \\\\ 4D- &      acc &    n/a &                &   50 &      0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.10 &  7.50 &     0.00 &    0.00 &   0.00 &  0.00 &  0.05 &  0.00 &  0.50 &  14.0 &     0.00 &    0.05 &   0.00 &  0.00 &  0.10 &  0.05 &  0.40 &   26.8 &                      \\\\ 5D+ &      acc &    n/a &                &   50 &      0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.65 &     0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.05 &  3.45 &     0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.05 &   9.30 &                      \\\\ 5D- &      acc &    n/a &                &   50 &      0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.00 &  0.80 &     0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.05 &  3.75 &     0.00 &    0.00 &   0.00 &  0.00 &  0.00 &  0.00 &  0.00 &   9.90 &                      \\\\ 2Dx &      acc &    n/a &                &   50 &      2.20 &    2.25 &   2.65 &  2.10 &  2.55 &  5.80 &  6.15 &  19.8 &     1.35 &    2.35 &   3.35 &  2.35 &  4.75 &  9.15 &  11.0 &  27.4 &     1.35 &    2.90 &   2.70 &  2.85 &  4.25 &  6.10 &  7.05 &   29.2 &                      \\\\ 1DC &      acc &    n/a &                &   50 &      1.25 &    2.95 &   2.75 &  0.05 &  0.30 &  2.35 &  0.75 &  9.75 &     1.90 &    2.80 &   2.85 &  3.65 &  6.45 &  9.15 &  8.20 &  14.3 &     1.70 &    2.15 &   3.90 &  5.75 &  6.20 &  7.60 &  9.95 &   21.3 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ Cycled Letters &      acc &    n/a &                &  100 &      0.62 &    0.71 &   2.85 &  0.00 &  0.63 &  1.35 &  2.58 &  3.66 &     1.67 &    4.36 &   5.68 &  6.46 &  6.25 &  9.41 &  15.1 &  21.7 &     4.63 &    9.27 &   10.7 &  14.5 &  16.7 &  21.9 &  27.7 &   37.9 &                      \\\\ Anagrams 1 &      acc &    n/a &                &  100 &      0.10 &    0.14 &   0.40 &  0.00 &  0.27 &  0.69 &  1.16 &  2.28 &     0.21 &    0.61 &   1.12 &  1.27 &  1.60 &  2.72 &  3.72 &  8.62 &     0.50 &    1.27 &   2.13 &  3.05 &  3.81 &  5.49 &  8.38 &   15.1 &                      \\\\ Anagrams 2 &      acc &    n/a &                &  100 &      0.81 &    1.21 &   2.69 &  0.01 &  1.71 &  3.75 &  4.53 &  8.91 &     1.19 &    2.62 &   4.70 &  4.77 &  6.97 &  10.2 &  14.6 &  25.9 &     1.94 &    4.80 &   7.59 &  9.87 &  12.6 &  18.9 &  25.6 &   39.7 &                      \\\\ Symbol Insertion &      acc &    n/a &                &  100 &      0.00 &    0.00 &   0.10 &  0.00 &  0.05 &  0.42 &  0.89 &  8.26 &     0.03 &    0.05 &   0.57 &  1.18 &  1.67 &  3.46 &  6.62 &  45.4 &     0.11 &    0.28 &   2.19 &  4.18 &  6.61 &  11.0 &  27.3 &   67.2 &                      \\\\ Reversed Words &      acc &    n/a &                &  100 &      0.00 &    0.01 &   0.01 &  0.01 &  0.02 &  0.03 &  0.03 &  0.09 &     0.02 &    0.01 &   0.01 &  0.00 &  0.05 &  0.07 &  0.11 &  0.48 &     0.00 &    0.05 &   0.00 &  0.17 &  0.24 &  0.30 &  0.42 &   0.44 &                      \\\\ &          &        &                &      &           &         &        &       &       &       &       &       &          &         &        &       &       &       &       &       &          &         &        &       &       &       &       &        &                      \\\\ SAT Analogies &      acc &    n/a &                &   20 &      35.6 &    39.0 &   45.2 &  44.1 &  50.0 &  49.2 &  52.7 &  53.7 &     30.5 &    41.2 &   43.1 &  46.5 &  55.1 &  54.3 &  53.5 &  59.1 &     30.5 &    40.4 &   42.8 &  40.6 &  48.4 &  51.9 &  53.5 &   65.2 &                      \\\\   \\bottomrule \\end{tabular} \\end{center} \\end{adjustwidth}  \\caption{Scores for every task, setting and model that we investigate in this paper. } } \\label{table:master} \\end{table}     \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_boolq.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_cb_acc.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_cb_f1.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_copa.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_rte.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_wic.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_wsc.png} &  \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_multirc_EM.png} &  \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_multirc_F1a.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_record_acc.png} &   \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/superglue_--_record_f1.png} &                                                                                    \\\\ \\bottomrule \\end{tabular} \\caption{All results for all SuperGLUE tasks.} \\label{graph:all_superglue} \\end{figure}    \\begin{figure} \\centering \\begin{minipage}{.33\\textwidth} \\begin{tabular}{c} \\toprule \\includegraphics[height=1.3in]{graphs/scale_plots/sat_analogies.png} \\\\ \\bottomrule \\end{tabular} \\captionof{figure}{Results for SAT task.} \\label{graph:all_sat} \\end{minipage} \\hfill \\begin{minipage}{.6\\textwidth} \\begin{tabular}{c c} \\toprule \\includegraphics[height=1.3in]{graphs/scale_plots/winogrande.png} & \\includegraphics[height=1.3in]{graphs/scale_plots/winograd.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Winograd tasks.} \\label{figure:all_winograd} \\end{minipage} \\end{figure}       \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/arithmetic.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/two_digit_addition.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/two_digit_multiplication.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/two_digit_subtraction.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/three_digit_addition.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/three_digit_subtraction.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/four_digit_addition.png} &  \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/four_digit_subtraction.png} &  \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/five_digit_addition.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/five_digit_subtraction.png} &   \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/single_digit_three_ops.png} &                                                                                    \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Arithmetic tasks.} \\label{figure:all_arithmetic} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/hellaswag.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/lambada_acc_test.png} &               \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/storycloze_test.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Cloze and Completion tasks.} \\label{graph:all_cloze_and_completion} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/piqa.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/arc_challenge_test.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/openbook_qa_test.png} \\\\                                             \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Common Sense Reasoning tasks.} \\label{figure:all_common_sense} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/naturalqs_--_open_domain_test.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/triviaqa-wiki.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/webqs_test.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all QA tasks.} \\label{graph:all_qa} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/quac.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/race-h_test.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/race-m_test.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/squad_f1_Squad_2_0.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/coqa.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/drop.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Reading Comprehension tasks.} \\label{figure:all_reading_comprehension} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.31\\linewidth]{graphs/scale_plots/anli_r1_test.png} &       \\includegraphics[width=0.31\\linewidth]{graphs/scale_plots/anli_r2_test.png} &        \\includegraphics[width=0.31\\linewidth]{graphs/scale_plots/anli_r3_test.png} \\\\ \\bottomrule \\end{tabular}  \\caption{All results for all ANLI rounds.} \\label{figure:all_anli} \\end{figure}     \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_cycle_letters.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_fewshot.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_mid_word_1_anagrams.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_mid_word_2_anagrams.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_random_insertion_in_word.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/wordscramble_reversed_words.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Scramble tasks.} \\label{figure:all_superglue} \\end{figure}    \\begin{figure} \\begin{tabular}{lll} \\toprule \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_detoen16_test.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_entode16_test.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_entofr14_test.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_frtoen14_test.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_entoro16_test.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_sacrebleu_rotoen16_test.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_detoen16_test.png} &      \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_entode16_test.png} &        \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_entofr14_test.png} \\\\ \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_frtoen14_test.png} &         \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_entoro16_test.png} &          \\includegraphics[width=0.33\\linewidth]{graphs/scale_plots/translation_multi-bleu_rotoen16_test.png} \\\\ \\bottomrule \\end{tabular} \\caption{All results for all Translation tasks.} \\label{figure:all_translation} \\end{figure}   \\clearpage \\newpage \\bibliographystyle{alpha} \\bibliography{bib}  \\l \\l  \\end{document}     "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2005.14165/aggregate_performance.png",
                "caption": "\\textbf{Aggregate performance for all 42 accuracy-denominated benchmarks}~~~While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure \\ref{graph:superglue_analysis} for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/triviaqa-wiki.png",
                "caption": "On TriviaQA GPT3's performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases.  One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG \\cite{lewis2020retrieval}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_analysis.png",
                "caption": " \\textbf{ Performance on SuperGLUE increases with model size and number of examples in context.} A value of $K=32$ means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table \\ref{table:superglue}). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/anli_r3_test.png",
                "caption": "\\textbf{Performance of GPT-3 on ANLI Round 3.} Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2\\%). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/training_curves.png",
                "caption": "\\textbf{GPT-3 Training Curves}~~~We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/contamination_graph.png",
                "caption": "\\textbf{Benchmark contamination analysis}~~~ We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset.  Performance on most benchmarks changed negligibly, but some were flagged for further review.  On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section \\ref{section:Results} with an asterisk. We find no evidence that other benchmarks are affected.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_boolq.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_cb_acc.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_cb_f1.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_copa.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_rte.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_wic.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_wsc.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_multirc_EM.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_multirc_F1a.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_record_acc.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/superglue_--_record_f1.png",
                "caption": "All results for all SuperGLUE tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/sat_analogies.png",
                "caption": "All results for all Winograd tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/winogrande.png",
                "caption": "All results for all Winograd tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/winograd.png",
                "caption": "All results for all Winograd tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/arithmetic.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/two_digit_addition.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/two_digit_multiplication.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/two_digit_subtraction.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/three_digit_addition.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/three_digit_subtraction.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/four_digit_addition.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/four_digit_subtraction.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/five_digit_addition.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/five_digit_subtraction.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/single_digit_three_ops.png",
                "caption": "All results for all Arithmetic tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/hellaswag.png",
                "caption": "All results for all Cloze and Completion tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/lambada_acc_test.png",
                "caption": "All results for all Cloze and Completion tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/storycloze_test.png",
                "caption": "All results for all Cloze and Completion tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/piqa.png",
                "caption": "All results for all Common Sense Reasoning tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/arc_challenge_test.png",
                "caption": "All results for all Common Sense Reasoning tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/openbook_qa_test.png",
                "caption": "All results for all Common Sense Reasoning tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/naturalqs_--_open_domain_test.png",
                "caption": "All results for all QA tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/triviaqa-wiki.png",
                "caption": "All results for all QA tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/webqs_test.png",
                "caption": "All results for all QA tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/quac.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/race-h_test.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/race-m_test.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/squad_f1_Squad_2_0.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/coqa.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/drop.png",
                "caption": "All results for all Reading Comprehension tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/anli_r1_test.png",
                "caption": "All results for all ANLI rounds.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/anli_r2_test.png",
                "caption": "All results for all ANLI rounds.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/anli_r3_test.png",
                "caption": "All results for all ANLI rounds.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_cycle_letters.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_fewshot.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_mid_word_1_anagrams.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_mid_word_2_anagrams.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_random_insertion_in_word.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/wordscramble_reversed_words.png",
                "caption": "All results for all Scramble tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_detoen16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_entode16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_entofr14_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_frtoen14_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_entoro16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_sacrebleu_rotoen16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_detoen16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_entode16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_entofr14_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_frtoen14_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_entoro16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/translation_multi-bleu_rotoen16_test.png",
                "caption": "All results for all Translation tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/table1.tex",
                "caption": "\\textbf{Results on three Open-Domain QA tasks.} GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings.  TriviaQA few-shot result is evaluated on the wiki split test server.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/table2.tex",
                "caption": " Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2005.14165/table3.tex",
                "caption": "Scores for every task, setting and model that we investigate in this paper. ",
                "description": ""
            }
        ]
    },
    "1809.10853": {
        "title": "Adaptive Input Representations for Neural Language Modeling",
        "abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.",
        "tldr": "Adapt input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity are introduced and a systematic comparison of popular choices for a self-attentional architecture is performed.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": " Language modeling is a basic task in natural language processing, with many applications such as speech recognition \\citep{arisoy:2012:wfnlm} and statistical machine translation \\citep{schwenk:2012:wfnlm,vaswani:2013:emnlp,baltescu2014pragmatic}. Recently, much progress has been made by neural methods \\citep{bengio:2003:jmlr,mikolov:2010:interspeech} based on LSTMs \\citep{jozefowicz2016lm}, gated convolutional networks \\citep{dauphin2017convlm} and self-attentional networks \\citep{alrfou2018chartrans}.  There are different choices for the basic unit we wish to model, including full words \\citep{bengio:2003:jmlr}, characters for the input \\citep{kim2016character}, or also the output \\citep{merity2018lm} as well as sub-words \\citep{buckman2018taacl,mielke2018spell}. Word-based models are particularly challenging since computing probabilities for all 800K words of the \\gbw{} benchmark is still a substantial part of the overall computation \\citep{chen2016strategies}.  A popular approach to lower the computational burden is to structure the output vocabulary so that not all probabilities need to be computed. The hierarchical softmax does this by introducing latent variables or clusters to simplify normalization \\citep{goodman:2001:icassp,morin:2005:aistats,mikolov:2011:icassp}. This has been further improved by the \\emph{adaptive softmax} which introduces a variable capacity scheme for output word embeddings, assigning more parameters to frequent words and fewer parameters to rare words \\citep{grave2017icml}.  In this paper, we introduce \\emph{adaptive input embeddings} which extend the adaptive softmax to input word representations. This factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words. For a competitive setup on the \\gbw{} benchmark, adaptive input embeddings reduce the number of parameters in the input and output layers by 23\\% while achieving higher accuracy over fixed size embeddings. When the adaptive input representations are tied with an adaptive softmax in the output, then the number of parameters is reduced by a total of 61\\%.  Our experiments compare models based on word inputs, character inputs, as well as sub-word units using a self-attention architecture \\citep{vaswani2017transformer}. We show that models with adaptive word representations can outperform very strong character-based models while training more than twice as fast. We also substantially improve adaptive softmax by introducing additional dropout regularization in the tail projection. On the \\wiki{} benchmark we achieve a perplexity of 18.7, a reduction of 10.5 perplexity over the previously best published result. On the larger \\gbw{} benchmark our best model with adaptive input embeddings achieves 23.02 perplexity, a reduction of nearly 5 perplexity over the next best previously published result.  "
            },
            {
                "section_name": "Related Work_2",
                "paragraphs": " Adaptive word representations are inspired by the adaptive softmax work \\cite{grave2017icml} which first described a GPU friendly way to construct a hierarchical softmax and showed that it performs very competitively compared to a full softmax, while offering significantly faster speed and a lower memory footprint.  \\citet{merity2018lm} use a modified version of adaptive softmax which does not reduce the dimensionality of less frequent words in order to be able to share output embeddings with the input. This setup is akin to a hierarchical softmax with tied weights. We show that variable-sized input embeddings can perform better than fixed sized embeddings. Furthermore, this also enables weight sharing with an adaptive softmax output layer.  \\citet{merity2018lm} evaluates both character-based and word-based factorizations but does not directly compare them to each other. We perform a direct comparison of word-based and character-based input vocabularies and also compare to a sub-word factorization for both the input and output. Recently, \\citet{alrfou2018chartrans} demonstrated that self-attentional models can perform very well on language modeling tasks where the input and output is both characters. We also consider word-based benchmarks.   "
            },
            {
                "section_name": "Adaptive Input Representations_3",
                "paragraphs": " \\begin{figure*} \\centering \\includegraphics[scale=0.695]{adaptive_inputs.pdf} \\caption{Illustration of adaptive input representations. Words are assigned to clusters $\\mathcal{V}_i$ based on their frequency which determines the size of the representations. Embeddings are projected to a common dimension $d$ before being fed to the model. } \\label{fig:adaptive_inputs} \\end{figure*}   The adaptive softmax exploits the fact that the distribution of word types in natural language follows a Zipfian distribution in order to improve the computation of the output probabilities. We apply the same intuition for input word embeddings with the motivation to reduce the number of parameters which frees up capacity for other parts of the model.  We define a number of clusters that partitions the frequency ordered vocabulary $\\mathcal{V} = \\mathcal{V}_1 \\cup \\mathcal{V}_2, \\dots, \\mathcal{V}_{n-1} \\cup \\mathcal{V}_n$ such that $\\mathcal{V}_i \\cap \\mathcal{V}_j = \\emptyset$ for $\\forall i,j$, and $i \\neq j$, where $\\mathcal{V}_1$ contains the most frequent words and $\\mathcal{V}_n$ the least frequent words. We will refer to $\\mathcal{V}_1$ as the \\emph{head} and to any subsequent clusters loosely as \\emph{tail}. We reduce the capacity for each cluster by a factor of $k$. That is, if words in $\\mathcal{V}_1$ have dimension $d$, then words in $\\mathcal{V}_n$ have dimension $\\frac{d}{k^{n-1}}$. We typically set $k=4$ following \\citet{grave2017icml}.  Next, we add linear projections $\\emW_1 \\in \\R^{d\\times d}, \\dots, \\emW_n \\in \\R^{d/k^{n-1} \\times d}$ to map the embeddings of each cluster to dimension $d$ so that the concatenated output of the adaptive input embedding layer can be easily used by the subsequent model (Figure~\\ref{fig:adaptive_inputs}). We also project $\\mathcal{V}_1$ which already has dimension $d$.  When presented with a number of input words, the adaptive input embedding layer partitions the words into the various clusters, performs separate lookups in the embedding tables and then projects to dimension $d$, followed by concatenating the embeddings in the original order.   \\paragraph{Weight sharing.} When the output layer is an adaptive softmax with the same partition of $\\mathcal{V}$, $d$, and $k$ as the adaptive input layer, then we can tie the weights \\citep{inan2016tying,press2016using}. This further reduces the number of parameters and can simultaneously improve performance (\\textsection\\ref{sec:results}). We can share both the parameters for the actual words as well as the projections $\\emW_1, \\dots, \\emW_n$ Sharing the word embeddings is straightforward except for the head where the adaptive softmax has $n-1$ additional embeddings for the remaining clusters which are not shared with the input.  We share all projections, except for the head projection which is not available in the adaptive softmax since the model output is directly multiplied with the output word embeddings for the head band. Performance decreased when we added a head projection to the adaptive softmax in the output, regardless of when it was shared or not. Sharing both the word embeddings as well as the projections performed very well on \\wiki{} but on \\gbw{} we only share the word embeddings as we found that this performed better on the validation set.   "
            },
            {
                "section_name": "Experimental Setup_4",
                "paragraphs": ""
            },
            {
                "section_name": "Model_1",
                "paragraphs": " We follow most of the architectural choices described in \\citet{vaswani2017transformer} but use only a decoder network. We add sinusoidal position embeddings to the input layer and stack $N=16$ blocks for both \\gbw{} and \\wiki{}. Each block contains two sub-blocks: the first is a multi-head self-attention module with $H=16$ heads. The second sub-block is a feed-forward module (FFN) of the form $ReLU(\\mW_1 \\mX + b_1) \\mW_2 + b_2$ where $\\mW_1 \\in \\R^{e \\times e_{ff}}$, $\\mW_1 \\in \\R^{e_{ff} \\times e}$ and $e=1024$, $e_{ff}=4096$ unless otherwise stated. Different to \\citet{vaswani2017transformer} we apply layer normalization before the self-attention and FFN blocks instead of after, as we find it leads to more effective training. Sub-blocks are surrounded by a residual connection \\citep{he2015deep}. \\f\\u  We use a dropout rate of 0.1 and attention dropout of 0.1 for \\gbw{} models, and increase regularization for \\wiki{} by using dropout 0.3, and 0.1 ReLU dropout as well as attention dropout 0.1. We use the same hyperparameters for all models trained on the same dataset in order to enable a like for like comparison. When the dimensionality of the input or output layer differs from $e$, then we add a simple linear projection with no bias.   "
            },
            {
                "section_name": "Datasets_2",
                "paragraphs": " We experiment on the \\gbw{} benchmark and \\wiki{}. \\gbw{} contains 768M word tokens and has a vocabulary of about 800K word types, which corresponds to words with more than $3$ occurrences in the training set~\\citep{chelba:2013:techreport}.  The training data of \\wiki{} comprises about 100M tokens and a vocabulary of around 260K, corresponding to types with more than $3$ occurrences in the training data \\citep{wiki103}. The dataset is composed of shuffled Wikipedia articles where the context carries across sentences.   "
            },
            {
                "section_name": "Batching_3",
                "paragraphs": " For \\gbw{} we batch individual sentences since the corpus does not contain document structure. For \\wiki{} we partition the training data into blocks of 512 contiguous tokens ignoring document boundaries. Evaluation is the same except that we require blocks to contain complete sentences totaling up to 512 tokens.\\footnote{ Respecting document boundaries may lead to better results and we leave this to future work.}  We limit the number of tokens per GPU to a maximum threshold $B$ per GPU. That is, we add examples of similar length until we reach this threshold. When we train on multiple GPUs, each GPU processes $B$ tokens using the same model parameters. This increases the effective batch size to the product of the number of GPUs and $B$. For \\gbw{} models we use $B=2048$ and typically train on 32 GPUs, giving an effective batch size of 65K tokens. The smaller vocabulary of \\wiki{} enables increasing $B$ to 4096 and we train on 8 GPUs. We found that large batch training is beneficial for this dataset and we therefore accumulate gradient updates over two batches before committing a parameter update \\citep{ott:uncertainty:2018}. This gives an effective batch size of 65K tokens for \\wiki{}.   "
            },
            {
                "section_name": "Input and Output layer hyperparameters_4",
                "paragraphs": " \\paragraph{Embedding sizes.} For fixed size word input layers and softmax output layers we generally use embeddings of size 512 for \\wiki{}. When we use an adaptive softmax in the output and fixed size word embeddings for the input, then we use dimension 256 for the input embeddings for \\gbw{} and 64 for \\wiki{}. We tuned this choice on the validation set (Appendix \\ref{app:ablation}). BPE inputs and outputs have embeddings of size 1024.  \\paragraph{Character CNN.} We model character inputs by convolving the representations of all characters in a word following \\citet{charcnn} which applies several filters, then max pooling, a number of highway layers and a projection. Character embeddings have size 128 and we apply seven filters of size 1x128, 2x256, 3x384, 4x512, 5x512, 6x512, 7x512, where 3x128 indicates a filter processing three characters that outputs 128 features. We use a single highway layer for \\wiki{}, and two for \\gbw{}. We do not add start of word and end of word markers as they did not improve validation accuracy. We train on the same pre-processed data as the other models, with unknown tokens in both the inputs and outputs. \\paragraph{Adaptive input representations and adaptive softmax.} We use an adaptive softmax output layer to train models with large word-based vocabularies. For adaptive word inputs and adaptive softmax, we use embeddings of size $d=1024$ for the head and reduce the size of subsequent clusters by a factor of $k=4$. For \\wiki{}, we have three bands of size 20K (d=1024), 40K (d=256) and 200K (d=64). For \\gbw{} the bands are 60K (d=1024), 100K (d=256), and 640K (d=64).  \\paragraph{Sub-word models.} We learn a byte-pair encoding (BPE) of 32K codes on the training data of each benchmark \\citep{bpe}. After applying the code to the training data we obtain a vocabulary of 33,337 tokens for \\wiki{} and 32,347 tokens for \\gbw{}. BPE input/output embeddings have size 1024. The final evaluation is in terms word-level perplexity to be comparable to other models. The probability of a word is the product of the sub-word units.  "
            },
            {
                "section_name": "Optimization_5",
                "paragraphs": " Different to \\citet{vaswani2017transformer} we use Nesterov's accelerated gradient method \\citep{sutskever2013icml} with a momentum of $0.99$ and we renormalize gradients if their norm exceeds $0.1$ \\citep{pascanu2013difficulty}. The learning rate is linearly warmed up from $10^{-7}$ to $1$ for 16K steps and then annealed using a cosine learning rate schedule with $C$ cycles \\citep{cosine}. Each cycle runs for twice the number of updates than the previous cycle and we lower the maximum and minimum learning rates by a rate $M$ compared to the previous cycle. The initial minimum learning rate is $10^{-5}$ and the maximum is $1$.  \\gbw{} models train for a total of 975K updates over $C=3$ cycles, the first cycle takes 137K steps, and we set $M=0.6$. The \\wiki{} models train for 286K steps over $C=4$ cycles, the first cycle takes 18K setps and we set $M=0.75$. We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband. We also use the NCCL2 library and the torch.distributed package for inter-GPU communication. We train models with 16-bit floating point precision, following~\\citet{ott2018scaling}.   "
            },
            {
                "section_name": "Experiments and Results_5",
                "paragraphs": " "
            },
            {
                "section_name": "Main results_6",
                "paragraphs": " \\begin{table*} \\centering \\begin{tabular}{lrrr} \\toprule & \\bf Test & \\bf \\thead{Train Time\\\\ (hours)} & \\bf Parameters \\\\ \\midrule \\citet{dauphin2017convlm} & 31.9 & - & 428M \\\\ \\citet{jozefowicz2016lm} & 30.0 & - & 1,040M \\\\ \\citet{shazeer2017} & 28.0 & - & 4,371M$^\\dagger$ \\\\ \\midrule Char-CNN & 25.88 & 79 & 366M \\\\ Adaptive inputs & 25.22 & 55 & 331M \\\\ Adaptive inputs (large) & 23.91 & 72  & 465M \\\\ Adaptive inputs (very large) & \\bf 23.02 & 145  & 1026M \\\\ \\midrule 10 LSTMs + SNM10-SKIP  \\citep{shazeer2016sparse} & 23.7 & - & - \\\\ \\bottomrule \\end{tabular} \\caption{Test perplexity on \\gbw{}. Adaptive inputs share parameters with an adaptive softmax. Training times of Char-CNN and Adaptive input models are measured when training with 64 GPUs. \\\\ \\small{$^\\dagger$does not include embedding and softmax layers}} \\label{tab:gbw_best} \\end{table*}  \\begin{table*} \\centering \\begin{tabular}{lrrr} \\toprule & \\bf Test & \\bf \\thead{Train Time\\\\ (hours)} & \\bf Parameters \\\\ \\midrule \\citet{grave2016cache} & 40.8 & - & \\\\ \\citet{dauphin2017convlm}  & 37.2 & - & 229M \\\\ \\citet{merity2018lm} & 33.0 & - & 151M  \\\\ \\citet{hebbian} & 29.2 & - & \\\\ \\midrule Adaptive inputs & \\bf 18.7 & 67 & 247M \\\\ \\bottomrule \\end{tabular} \\caption{Test perplexity on \\wiki{} (cf. Table~\\ref{tab:gbw_best}). Training time is based on 8 GPUs. } \\label{tab:wiki_best} \\end{table*} For the main results on \\gbw{}, we doubled the batch size by training on 64 GPUs instead of 32 GPUs.  We also consider two larger setups, one where we added four more blocks ($N=20$) and increased the FFN dimension to $e_{ff} = 6144$ (large), and another where we add another four blocks ($N=24$) with $e_{ff} = 8192$ and $e = 1536$ (very large). All other settings follow \\textsection\\ref{sec:setup} and all models were trained for the same number of steps.  Table~\\ref{tab:gbw_best} compares our models to previous work on \\gbw{}. The adaptive input model outperforms the best previously reported result at an order of magnitude fewer parameters. Our large model performs nearly as well as an ensemble of over ten models and achieves a new state of the art of 24.14 perplexity. Our very large model performs as well as an ensemble of over ten models and achieves 23.02 perplexity. The Char-CNN model performs 0.6 PPL worse than the standard adaptive input model even though it trained for over 40\\% longer.  Table~\\ref{tab:wiki_best} shows our result on \\wiki{} where adaptive inputs achieve 18.7 perplexity. For this result only, we partition the training data into blocks of 3072 contiguous tokens instead of 512 tokens as for other experiments. During evaluation we require blocks to contain complete sentences totaling up to 3072 tokens of which the first 2560 tokens serve as context to score the last 512 tokens; we take care to score all tokens in the test and validation sets. We motivate this choice in \\textsection\\ref{sec:analysis}.      "
            },
            {
                "section_name": "Comparison of input and output layer factorizations_7",
                "paragraphs": " Next, we perform a systematic comparison of different input and output layer factorizations. We consider a word-based setup with fixed size word input embeddings and a standard word softmax (\\sm{}) where embeddings have either dimension 512 (\\wiki{}) or 64 (\\gbw{}). We consider tying the input and output embeddings (\\smt{}). Instead of words, we try less sparse sub-word units, both in the input and output, with embeddings of size 1024 (\\bpe{}) and shared weights (\\bpet{}). Next, we consider replacing the fixed size output representations by an adaptive softmax (\\asm{}) and characters as input (\\cnn{}). Finally, we use both adaptive input word representations as well as an adaptive softmax (\\adp{}) and a tied version (\\adpt{}). All models use the same self-attention architecture described in \\textsection\\ref{sec:model}.  Table~\\ref{tab:wiki_comp} shows results when training all configurations for the same number of updates. Adaptive input representations with tied input and output layers (\\adpt{}) achieve the highest accuracy at the same speed as the BPE models which have a very small vocabulary (33K versus 260K). \\cnn{} is 1 perplexity worse than \\adpt{} and requires well over twice the training time. It is the slowest approach, even though it has a fast adaptive softmax in the output. Fixed word embeddings perform least well (\\sm{}). Sub-word units are fast to train and perform better than word models with fixed sized embeddings. \\asm{} improves over \\sm{} and greatly speeds up training. For \\asm{}, we found that reducing the dimension of the input word embeddings to 64 on \\wiki{} results in better accuracy (Appendix~\\ref{app:ablation}).  Table~\\ref{tab:gbw_comp} shows that adaptive input representations perform equally well on \\gbw{} compared to other factorizations. \\adpt{} is 34\\% faster than \\adp{} because there are fewer parameters to update. Similar to before, \\adpt{} trains more than twice as fast as \\cnn{} at higher accuracy, however, the accuracy gap is narrower than for \\wiki{}. Regularization is more important on \\wiki{} while models for \\gbw{} benefit from additional capacity. \\g\\w Because of this we used input word embeddings of size 256 for \\asm{}.  We also trained \\cnn{} without replacing input words outside the vocabulary by an unknown symbol, however, this only improved validation perplexity by 0.16. \\begin{table*} \\centering \\begin{tabular}{lllrrrr} \\toprule & \\bf Input & \\bf Output & \\bf Valid & \\bf Test & \\bf \\thead{Train Time\\\\ (hours)} & \\bf Params \\\\ \\midrule SM & Embedding & Softmax & 23.87 & 24.92 &57* & 476.8M \\\\ BPE & BPE Embedding & BPE Softmax & 23.13 & 24.25 & 30 & 270M \\\\ BPE-T & BPE Embedding & BPE Softmax (tied) & 22.46 & 23.45 & 30 & 235.7M \\\\ SM-T & Embedding & Softmax (tied) & 22.63 & 23.38 & 56* & 339.7M \\\\ ASM & Embedding & Adaptive & 21.23 & 22.18 & 35 & 263.1M \\\\ CNN & Char-CNN & Adaptive & 20.86 & 21.79 & 70 & 266.3M \\\\ ADP & Adaptive & Adaptive & 20.95 & 21.74 & 34 & 291.3M  \\\\ ADP-T & Adaptive & Adaptive (tied) & \\bf 19.79 & \\bf 20.51 & 30 & 246.9M \\\\ \\bottomrule \\end{tabular} \\caption{Test perplexity on \\wiki{} for various input and output layer factorizations. Training speed was measured on a single 8-GPU machine. (*) indicates a modified training regime because of large memory requirements: the maximum number of tokens per GPU was lowered to 1024 from 4096 but the same number of updates were performed by processing four batches before committing a weight update. } \\label{tab:wiki_comp} \\end{table*}  \\begin{table*} \\centering \\begin{tabular}{lllrrrr} \\toprule & \\bf Input & \\bf Output & \\bf Valid & \\bf Test & \\bf  \\thead{Train time\\\\ (hours)} & \\bf Params \\\\ \\midrule BPE-T & BPE Embedding & BPE Softmax (shared) & 27.44 & 27.51 & 34 & 234.7M \\\\ BPE & BPE Embedding & BPE Softmax & 27.02 & 27.13 & 35 & 267.8M \\\\ ASM & Embedding & Adaptive & 26.97 & 27.06 & 62 & 532.8M \\\\ CNN & Char-CNN & Adaptive & 26.13 & 26.25 & 92 & 365.8M \\\\ ADP & Adaptive & Adaptive & 26.38 & 26.49 & 65 & 458.4M \\\\ ADP-T & Adaptive & Adaptive (shared) & \\bf 25.51 & \\bf 25.58 & 43 & 330.8M \\\\ \\bottomrule \\end{tabular} \\caption{Test perplexity on \\gbw{}. Training speed measured on four 8-GPU machines. } \\label{tab:gbw_comp} \\end{table*}    "
            },
            {
                "section_name": "Analysis_8",
                "paragraphs": " This appendix extends the analysis in \\textsection\\ref{sec:analysis} by showing a breakdown of the test loss when binning by the frequency of the previous word.  \\begin{figure*}[h] \\begin{tikzpicture} \\begin{axis}[ ybar, bar width=.12cm, width=1.0*\\textwidth, height=.5\\textwidth, legend style={at={(0.5,1)}, anchor=north,legend columns=-1},  xticklabels from table={\\gbwdata}{bin}, xticklabel style={text height=1.5ex}, xtick=data,  nodes near coords align={vertical}, ymin=2.3,ymax=4.8, ylabel={Loss on the next word}, xlabel={Word frequency bin} ] \\addplot[black,fill=brown,postaction={pattern=vertical lines}] table[x expr=\\coordindex,y=next_prob_bpe]{\\gbwdata}; \\addplot[black,fill=orange,postaction={pattern=horizontal lines}] table[x expr=\\coordindex,y=next_prob_bpe_shr]{\\gbwdata}; \\addplot[black,fill=green,postaction={pattern=dots}] table[x expr=\\coordindex,y=next_prob_emb]{\\gbwdata}; \\addplot[black,fill=olive,postaction={pattern=crosshatch}] table[x expr=\\coordindex,y=next_prob_cnn]{\\gbwdata}; \\addplot[black,fill=yellow,postaction={pattern=north west lines}] table[x expr=\\coordindex,y=next_prob_adap]{\\gbwdata}; \\addplot[black,fill=gray,postaction={pattern=grid}] table[x expr=\\coordindex,y=next_prob_adap_tie]{\\gbwdata}; \\legend{BPE, BPE-T, ASM, CNN, ADP, ADP-T} \\end{axis} \\end{tikzpicture} \\caption{Loss of models when binning by the frequency of the \\emph{previous} word measured on \\gbw{} (cf. Figure~\\ref{fig:bins_wiki_next}).} \\label{fig:bins_gbw_next} \\end{figure*}  \\end{document}   "
            },
            {
                "section_name": "Adaptive Softmax vs. full Softmax_9",
                "paragraphs": " We also found that adaptive softmax can benefit from additional regularization of rare words. Adaptive softmax first projects the model output to the dimension of a particular cluster and then computes a dot product with the respective word embeddings. We add dropout to the output of the first projection for all clusters, except for the head. This change enables the adaptive softmax to outperform a standard softmax over fixed size output word embeddings on \\wiki{} (Table~\\ref{tab:adaptive}).  However, we found that adding dropout in this way is not helpful for larger datasets such as \\gbw{}. Unfortunately, a standard softmax over 800K words is not tractable and we were unable to make a comparison. It may be possible to achieve better results by tuning dropout for each band of the tail and we leave this for future work.  \\begin{table} \\centering \\begin{tabular}{crr} \\toprule & \\thead{Tail\\\\ dropout} & \\thead{Validation\\\\ perplexity} \\\\ \\midrule Softmax (\\sm{}) & N/A & 23.87 \\\\ Adaptive (\\adp{}) & 0.0 & 24.74 \\\\ Adaptive (\\adp{}) & 0.2 & 21.23 \\\\ \\bottomrule \\end{tabular} \\caption{Perplexity on \\wiki{} when regularizing rare words in adaptive softmax.} \\label{tab:adaptive} \\end{table}   "
            },
            {
                "section_name": "Conclusion_6",
                "paragraphs": " Adaptive input embeddings vary the size of input word embeddings which can improve accuracy while drastically reducing the number of model parameters. When sharing parameters with an adaptive softmax, the number of parameters can be further reduced which improves training speed. We presented a comparison between different input and output layer factorizations including word inputs, character inputs and sub-word units in both the input and output.  Our experiments show that models with adaptive input embeddings train faster compared to character input CNNs while achieving higher accuracy. We achieve new state of the art results on \\wiki{} and \\gbw{}. In future work, we will apply variable sized input embeddings to other tasks.    \\subsubsection*{Acknowledgments} We thank Tom Bosc for fruitful comments and suggestions.  \\bibliography{main} \\bibliographystyle{iclr2019_conference}  \\clearpage \\appendix \\section*{Supplementary Material} "
            },
            {
                "section_name": "Additional experiments on \\wiki{_7",
                "paragraphs": " This appendix shows various ablation. Table~\\ref{tab:comparable} shows that reducing the capacity of fixed size word input embddings is beneficial on \\wiki{}. The next set of results in Table~\\ref{tab:comparable} shows results for various settings of the \\sm{} and \\smt{} models. We also experimented with sharing the head projection but found this to perform less well than not sharing it. Finally, Table~\\ref{tab:cutoffs} shows various band sizes for adaptive input word embbedings.  We also show the performance of \\citet{merity2018lm} who use an adaptive softmax with equally sized word representations and share the input and output embeddings (no dim reduction, tied).  \\begin{table*}[h] \\centering \\begin{tabular}{llrrr} \\toprule \\bf Input & \\bf Output & \\bf Dropout & \\bf Valid PPL & \\bf Parameters \\\\ \\midrule 256d Embedding & Adaptive & 0.3 & 23.39 & 314.7M \\\\ 128d Embedding & Adaptive & 0.3 & 21.51 & 280.3M \\\\ 64d Embedding & Adaptive & 0.3 & 21.23 & 263.1M \\\\ 32d Embedding & Adaptive & 0.3 & 21.78 & 254.5M \\\\ \\midrule 512d Embedding & 512d Softmax (tied) & 0.3 & 22.63 & 339.7M \\\\ 512d Embedding & 512d Softmax (tied) & 0.4 & 28.31 & 339.7M \\\\ 512d Embedding & 512d Softmax & 0.3 & 23.87 & 476.8 \\\\ 512d Embedding & 512d Softmax & 0.4 & 27.64 & 476.8 \\\\ 256d Embedding & 256d Softmax (tied) & 0.3 & 22.65 & 270.6M \\\\ 256d Embedding & 256d Softmax & 0.3 & 24.13 & 339.1M \\\\ 64d Embedding & 512d Softmax & 0.3 & 24.74 & 356.3M \\\\ \\midrule Adaptive & Adaptive (tied emb, not proj) & 0.3 & 20.06 & 247.3M \\\\ Adaptive & Adaptive (tied emb/proj not head) & 0.3 & 19.79 & 246.9M \\\\ Adaptive & Adaptive (tied emb/proj + head) & 0.3 & 20.06 & 246.9M \\\\ \\midrule 512d Embedding & 512d Softmax (tied) & 0.3 & 22.63 & 339.7M \\\\ 512d Embedding & 512d Adaptive (no dim reduction, tied) & 0.3 & 25.48 & 340.2M \\\\ \\bottomrule \\end{tabular} \\caption{Validation perplexity of our models on \\wiki{}. } \\label{tab:comparable} \\end{table*}  \\begin{table*}[h] \\centering \\begin{tabular}{cr} \\toprule \\bf Softmax cutoff & \\bf Valid PPL \\\\ \\midrule 20k/40k/200k & 19.79 \\\\ 20k/140k/100k & 20.26 \\\\ 20k/40k/60k/140k & 20.53 \\\\ 60k/100k/100k & 20.52 \\\\ 5k/155k/100k & 20.06 \\\\ 20k/40k/200k & 19.99 \\\\ 10k/60k/190k & 19.79 \\\\ \\bottomrule \\end{tabular} \\caption{Validation perplexity on \\wiki{} with tied adaptive inputs \\& outputs. The bands signify the number of words belonging to each band. In every case, the first band has dimension 1024, the second band 256, the third band 64 and the fourth band (if it exists) 16.} \\label{tab:cutoffs} \\end{table*}     "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/1809.10853/table1.tex",
                "caption": "Perplexity on \\wiki{} when regularizing rare words in adaptive softmax.",
                "description": ""
            }
        ]
    },
    "1706.03762": {
        "title": "Attention is All you Need",
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "tldr": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": " Recurrent neural networks, long short-term memory \\citep{hochreiter1997} and gated recurrent \\citep{gruEval14} neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation \\citep{sutskever14, bahdanau2014neural, cho2014learning}. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures \\citep{wu2016google,luong2015effective,jozefowicz2016exploring}.  Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input for position $t$. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. \\m Recent work has achieved significant improvements in computational efficiency through factorization tricks \\citep{Kuchaiev2017Factorization} and conditional computation \\citep{shazeer2017outrageously}, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.  \\m  Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences \\citep{bahdanau2014neural, structuredAttentionNetworks}. In all but a few cases \\citep{decomposableAttnModel}, however, such attention mechanisms are used in conjunction with a recurrent network.  \\m \\m  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. \\m   \\c\\c\\c\\c\\c\\c\\c\\c  "
            },
            {
                "section_name": "Background_2",
                "paragraphs": " The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU \\citep{extendedngpu}, ByteNet \\citep{NalBytenet2017} and ConvS2S \\citep{JonasFaceNet2017}, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions \\citep{hochreiter2001gradient}. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section~\\ref{sec:attention}.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations \\citep{cheng2016long, decomposableAttnModel, paulus2017deep, lin2017structured}.  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks \\citep{sukhbaatar2015}.  To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as \\citep{neural_gpu, NalBytenet2017} and \\citep{JonasFaceNet2017}.   \\c  \\c  \\m  \\c\\c\\c  \\c  \\c\\c\\c\\c  \\c\\m          \\b \\c \\l \\b \\v \\s  \\b \\h\\h \\\\ \\\\ \\h \\c\\\\ \\h \\c\\\\ \\h \\c\\c\\\\ \\h \\c\\c\\c\\\\ \\h \\c\\c\\\\ \\h\\h \\e  \\e \\e  "
            },
            {
                "section_name": "Model Architecture_3",
                "paragraphs": " \\begin{figure} \\centering \\includegraphics[scale=0.6]{Figures/ModalNet-21} \\caption{The Transformer - model architecture.} \\label{fig:model-arch} \\end{figure}   \\c\\r  Most competitive neural sequence transduction models have an encoder-decoder structure \\citep{cho2014learning,bahdanau2014neural,sutskever14}. Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive \\citep{graves2013generating}, consuming the previously generated symbols as additional input when generating the next.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure~\\ref{fig:model-arch}, respectively.  "
            },
            {
                "section_name": "Encoder and Decoder Stacks_1",
                "paragraphs": " \\paragraph{Encoder:}The encoder is composed of a stack of $N=6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.   We employ a residual connection \\citep{he2016deep} around each of the two sub-layers, followed by layer normalization \\cite{layernorm2016}.  That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $\\dmodel=512$.  \\paragraph{Decoder:}The decoder is also composed of a stack of $N=6$ identical layers.  In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.  \\r  "
            },
            {
                "section_name": "Attention_2",
                "paragraphs": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  \\subsubsection{Scaled Dot-Product Attention} \\label{sec:scaled-dot-prod}  \\b \\c \\i \\c \\l \\e  We call our particular attention \"Scaled Dot-Product Attention\" (Figure~\\ref{fig:multi-head-att}).   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.  In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:  \\begin{equation} \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\end{equation}  The two most commonly used attention functions are additive attention \\citep{bahdanau2014neural}, and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  \\s   \\i  \\p\\c  While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ \\citep{DBLP:journals/corr/BritzGLL17}. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  \\footnote{To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.}. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.   \\s   \\subsubsection{Multi-Head Attention} \\label{sec:multihead}  \\begin{figure} \\begin{minipage}[t]{0.5\\textwidth} \\centering Scaled Dot-Product Attention \\\\ \\vspace{0.5cm} \\includegraphics[scale=0.6]{Figures/ModalNet-19} \\end{minipage} \\begin{minipage}[t]{0.5\\textwidth} \\centering Multi-Head Attention \\\\ \\vspace{0.1cm} \\includegraphics[scale=0.6]{Figures/ModalNet-20} \\end{minipage}   \\c  \\caption{(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.} \\label{fig:multi-head-att} \\end{figure}  Instead of performing a single attention function with $\\dmodel$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure~\\ref{fig:multi-head-att}.  Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  \\begin{align*} \\mathrm{MultiHead}(Q, K, V) &= \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O\\\\ \\m\\m\\m\\d\\t\\d\\t\\d\\t\\\\ \\text{where}~\\mathrm{head_i} &= \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\\\\ \\end{align*}  Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{\\dmodel \\times d_k}$, $W^K_i \\in \\mathbb{R}^{\\dmodel \\times d_k}$, $W^V_i \\in \\mathbb{R}^{\\dmodel \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times \\dmodel}$.   \\r  In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=\\dmodel/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.  \\subsubsection{Applications of Attention in our Model}  The Transformer uses multi-head attention in three different ways: \\begin{itemize} \\item In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as \\citep{wu2016google, bahdanau2014neural,JonasFaceNet2017}.  \\item The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.  \\item Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.  See Figure~\\ref{fig:multi-head-att}.  \\end{itemize}  "
            },
            {
                "section_name": "Position-wise Feed-Forward Networks_3",
                "paragraphs": " In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.  \\begin{equation} \\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2 \\end{equation}  While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $\\dmodel=512$, and the inner-layer has dimensionality $d_{ff}=2048$.      \\c   \\r\\k\\k\\v\\v \\b\\l \\k\\k\\v\\v\\k\\k \\e \\k\\,\\k\\v\\W\\t\\,\\W\\W\\W\\i  \\m \\m  "
            },
            {
                "section_name": "Embeddings and Softmax_4",
                "paragraphs": "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $\\dmodel$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to \\citep{press2016using}.   In the embedding layers, we multiply those weights by $\\sqrt{\\dmodel}$.   "
            },
            {
                "section_name": "Positional Encoding_5",
                "paragraphs": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $\\dmodel$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed \\citep{JonasFaceNet2017}.  In this work, we use sine and cosine functions of different frequencies:  \\begin{align*} PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel}) \\\\ PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel}) \\end{align*}  where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.  We also experimented with using learned positional embeddings \\citep{JonasFaceNet2017} instead, and found that the two versions produced nearly identical results (see Table~\\ref{tab:variations} row (E)).  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.   "
            },
            {
                "section_name": "Why Self-Attention_4",
                "paragraphs": "\\i\\m\\i\\m\\m  In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations $(x_1, ..., x_n)$ to another sequence of equal length $(z_1, ..., z_n)$, with $x_i, z_i \\in \\mathbb{R}^d$, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.  One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.  The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies \\citep{hochreiter2001gradient}. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.  \\s  \\begin{table}[t] \\caption{ Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. $n$ is the sequence length, $d$ is the representation dimension, $k$ is the kernel size of convolutions and $r$ the size of the neighborhood in restricted self-attention.}  \\label{tab:op_complexities} \\begin{center} \\vspace{-1mm} \\s  \\begin{tabular}{lccc} \\toprule Layer Type & Complexity per Layer & Sequential & Maximum Path Length  \\\\ &             & Operations &   \\\\ \\hline \\rule{0pt}{2.0ex}Self-Attention & $O(n^2 \\cdot d)$ & $O(1)$ & $O(1)$ \\\\ Recurrent & $O(n \\cdot d^2)$ & $O(n)$ & $O(n)$ \\\\  Convolutional & $O(k \\cdot n \\cdot d^2)$ & $O(1)$ & $O(log_k(n))$ \\\\ \\c Self-Attention (restricted)& $O(r \\cdot n \\cdot d)$ & $O(1)$ & $O(n/r)$ \\\\  \\c\\c\\c\\\\  \\c\\i\\\\  \\c\\\\ \\c\\c\\c\\\\  \\c\\i\\\\  \\c\\\\ \\bottomrule \\end{tabular}  \\end{center} \\end{table}   \\b \\c   \\l \\b \\v \\s  \\b \\h \\\\ \\\\ \\h \\c\\\\ \\c\\\\  \\c\\c\\\\ \\h \\c\\c\\\\  \\c\\c\\c\\\\  \\c\\\\  \\c\\\\  \\e  \\e \\e    As noted in Table \\ref{tab:op_complexities}, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece \\citep{wu2016google} and byte-pair \\citep{sennrich2015neural} representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $r$ in the input sequence centered around the respective output position. This would increase the maximum path length to $O(n/r)$. We plan to investigate this approach further in future work.  A single convolutional layer with kernel width $k < n$ does not connect all pairs of input and output positions. Doing so requires a stack of $O(n/k)$ convolutional layers in the case of contiguous kernels, or $O(log_k(n))$ in the case of dilated convolutions \\citep{NalBytenet2017}, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions \\citep{xception2016}, however, decrease the complexity considerably, to $O(k \\cdot n \\cdot d + n \\cdot d^2)$. Even with $k=n$, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.  \\s    \\{\\n\\}\\{\\n\\}  As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.    "
            },
            {
                "section_name": "Training_5",
                "paragraphs": "This section describes the training regime for our models.  \\r  "
            },
            {
                "section_name": "Training Data and Batching_6",
                "paragraphs": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary \\citep{wu2016google}.  Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.  "
            },
            {
                "section_name": "Hardware and Schedule_7",
                "paragraphs": " We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models,(described on the bottom line of table \\ref{tab:variations}), step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days).  "
            },
            {
                "section_name": "Optimizer_8",
                "paragraphs": " \\begin{equation} lrate = \\dmodel^{-0.5} \\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5}) \\end{equation}  This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.  "
            },
            {
                "section_name": "Regularization_9",
                "paragraphs": " We employ three types of regularization during training: \\paragraph{Residual Dropout} We apply dropout \\citep{srivastava2014dropout} to the output of each sub-layer, before it is added to the sub-layer input and normalized.   In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$.  \\p\\c \\b \\m\\m\\m\\f\\s \\e   \\p\\_\\_  \\p\\c \\b \\m\\m\\f\\s \\e  \\_\\_  \\paragraph{Label Smoothing} During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ \\citep{DBLP:journals/corr/SzegedyVISW15}.  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.   "
            },
            {
                "section_name": "Results_6",
                "paragraphs": ""
            },
            {
                "section_name": "Machine Translation_10",
                "paragraphs": "\\begin{table}[t] \\begin{center} \\caption{The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.  } \\label{tab:wmt-results} \\vspace{-2mm} \\s \\begin{tabular}{lccccc} \\toprule \\multirow{2}{*}{\\vspace{-2mm}Model} & \\multicolumn{2}{c}{BLEU} & & \\multicolumn{2}{c}{Training Cost (FLOPs)} \\\\ \\cmidrule{2-3} \\cmidrule{5-6} & EN-DE & EN-FR & & EN-DE & EN-FR \\\\ \\hline ByteNet \\citep{NalBytenet2017} & 23.75 & & & &\\\\ Deep-Att + PosUnk \\citep{DBLP:journals/corr/ZhouCWLX16} & & 39.2 & & & $1.0\\cdot10^{20}$ \\\\ GNMT + RL \\citep{wu2016google} & 24.6 & 39.92 & & $2.3\\cdot10^{19}$  & $1.4\\cdot10^{20}$\\\\ ConvS2S \\citep{JonasFaceNet2017} & 25.16 & 40.46 & & $9.6\\cdot10^{18}$ & $1.5\\cdot10^{20}$\\\\ MoE \\citep{shazeer2017outrageously} & 26.03 & 40.56 & & $2.0\\cdot10^{19}$ & $1.2\\cdot10^{20}$ \\\\ \\hline \\rule{0pt}{2.0ex}Deep-Att + PosUnk Ensemble \\citep{DBLP:journals/corr/ZhouCWLX16} & & 40.4 & & & $8.0\\cdot10^{20}$ \\\\ GNMT + RL Ensemble \\citep{wu2016google} & 26.30 & 41.16 & & $1.8\\cdot10^{20}$  & $1.1\\cdot10^{21}$\\\\ ConvS2S Ensemble \\citep{JonasFaceNet2017} & 26.36 & \\textbf{41.29} & & $7.7\\cdot10^{19}$ & $1.2\\cdot10^{21}$\\\\ \\specialrule{1pt}{-1pt}{0pt} \\rule{0pt}{2.2ex}Transformer (base model) & 27.3 & 38.1 & & \\multicolumn{2}{c}{\\boldmath$3.3\\cdot10^{18}$}\\\\ Transformer (big) & \\textbf{28.4} & \\textbf{41.8} & & \\multicolumn{2}{c}{$2.3\\cdot10^{19}$} \\\\ \\h \\s \\r \\bottomrule \\end{tabular}  \\end{center} \\end{table}   On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table~\\ref{tab:wmt-results}) outperforms the best previously reported models (including ensembles) by more than $2.0$ BLEU, establishing a new state-of-the-art BLEU score of $28.4$.  The configuration of this model is listed in the bottom line of Table~\\ref{tab:variations}.  Training took $3.5$ days on $8$ P100 GPUs.  Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.  On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of $41.0$, outperforming all of the previously published single models, at less than $1/4$ the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate $P_{drop}=0.1$, instead of $0.3$.  For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.  For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of $4$ and length penalty $\\alpha=0.6$ \\citep{wu2016google}.  These hyperparameters were chosen after experimentation on the development set.  We set the maximum output length during inference to input length + $50$, but terminate early when possible \\citep{wu2016google}.  Table \\ref{tab:wmt-results} summarizes our results and compares our translation quality and training costs to other model architectures from the literature.  We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU \\footnote{We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.}.   "
            },
            {
                "section_name": "Model Variations_11",
                "paragraphs": " \\begin{table}[t] \\caption{Variations on the Transformer architecture. Unlisted values are identical to those of the base model.  All metrics are on the English-to-German translation development set, newstest2013.  Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.} \\label{tab:variations} \\begin{center} \\vspace{-2mm} \\s \\begin{tabular}{c|ccccccccc|ccc} \\hline\\rule{0pt}{2.0ex} & \\multirow{2}{*}{$N$} & \\multirow{2}{*}{$\\dmodel$} & \\multirow{2}{*}{$\\dff$} & \\multirow{2}{*}{$h$} & \\multirow{2}{*}{$d_k$} & \\multirow{2}{*}{$d_v$} & \\multirow{2}{*}{$P_{drop}$} & \\multirow{2}{*}{$\\epsilon_{ls}$} & train & PPL & BLEU & params \\\\ & & & & & & & & & steps & (dev) & (dev) & $\\times10^6$ \\\\ \\\\ \\hline\\rule{0pt}{2.0ex} base & 6 & 512 & 2048 & 8 & 64 & 64 & 0.1 & 0.1 & 100K & 4.92 & 25.8 & 65 \\\\ \\hline\\rule{0pt}{2.0ex} \\multirow{4}{*}{(A)} & & & & 1 & 512 & 512 & & & & 5.29 & 24.9 &  \\\\ & & & & 4 & 128 & 128 & & & & 5.00 & 25.5 &  \\\\ & & & & 16 & 32 & 32 & & & & 4.91 & 25.8 &  \\\\ & & & & 32 & 16 & 16 & & & & 5.01 & 25.4 &  \\\\ \\hline\\rule{0pt}{2.0ex} \\multirow{2}{*}{(B)} & & & & & 16 & & & & & 5.16 & 25.1 & 58 \\\\ & & & & & 32 & & & & & 5.01 & 25.4 & 60 \\\\ \\hline\\rule{0pt}{2.0ex} \\multirow{7}{*}{(C)} & 2 & & & & & & & &            & 6.11 & 23.7 & 36 \\\\ & 4 & & & & & & & &            & 5.19 & 25.3 & 50 \\\\ & 8 & & & & & & & &            & 4.88 & 25.5 & 80 \\\\ & & 256 & & & 32 & 32 & & &    & 5.75 & 24.5 & 28 \\\\ & & 1024 & & & 128 & 128 & & & & 4.66 & 26.0 & 168 \\\\ & & & 1024 & & & & & &         & 5.12 & 25.4 & 53 \\\\ & & & 4096 & & & & & &         & 4.75 & 26.2 & 90 \\\\ \\hline\\rule{0pt}{2.0ex} \\multirow{4}{*}{(D)} & & & & & & & 0.0 & & & 5.77 & 24.6 &  \\\\ & & & & & & & 0.2 & & & 4.95 & 25.5 &  \\\\ & & & & & & & & 0.0 & & 4.67 & 25.3 &  \\\\ & & & & & & & & 0.2 & & 5.47 & 25.7 &  \\\\ \\hline\\rule{0pt}{2.0ex} (E) & & \\multicolumn{7}{c}{positional embedding instead of sinusoids} & & 4.92 & 25.7 & \\\\ \\hline\\rule{0pt}{2.0ex} big & 6 & 1024 & 4096 & 16 & & & 0.3 & & 300K & \\textbf{4.33} & \\textbf{26.4} & 213 \\\\ \\hline \\end{tabular}  \\end{center} \\end{table}   \\r\\t\\c\\t  To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging.  We present these results in Table~\\ref{tab:variations}.  In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.  In Table~\\ref{tab:variations} rows (B), we observe that reducing the attention key size $d_k$ hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting.  In row (E) we replace our sinusoidal positional encoding with learned positional embeddings \\citep{JonasFaceNet2017}, and observe nearly identical results to the base model.  \\t\\r   \\m   "
            },
            {
                "section_name": "English Constituency Parsing_12",
                "paragraphs": " \\begin{table}[t] \\begin{center} \\caption{The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)} \\label{tab:parsing-results} \\vspace{-2mm} \\s \\begin{tabular}{c|c|c} \\hline {\\bf Parser}  & {\\bf Training} & {\\bf WSJ 23 F1} \\\\ \\hline Vinyals \\& Kaiser el al. (2014) \\cite{KVparse15} & WSJ only, discriminative & 88.3 \\\\ Petrov et al. (2006) \\cite{petrov-EtAl:2006:ACL} & WSJ only, discriminative & 90.4 \\\\ Zhu et al. (2013) \\cite{zhu-EtAl:2013:ACL} & WSJ only, discriminative & 90.4   \\\\ Dyer et al. (2016) \\cite{dyer-rnng:16} & WSJ only, discriminative & 91.7   \\\\ \\specialrule{1pt}{-1pt}{0pt} Transformer (4 layers)  &  WSJ only, discriminative & 91.3 \\\\ \\specialrule{1pt}{-1pt}{0pt} Zhu et al. (2013) \\cite{zhu-EtAl:2013:ACL} & semi-supervised & 91.3 \\\\ Huang \\& Harper (2009) \\cite{huang-harper:2009:EMNLP} & semi-supervised & 91.3 \\\\ McClosky et al. (2006) \\cite{mcclosky-etAl:2006:NAACL} & semi-supervised & 92.1 \\\\ Vinyals \\& Kaiser el al. (2014) \\cite{KVparse15} & semi-supervised & 92.1 \\\\ \\specialrule{1pt}{-1pt}{0pt} Transformer (4 layers)  & semi-supervised & 92.7 \\\\ \\specialrule{1pt}{-1pt}{0pt} Luong et al. (2015) \\cite{multiseq2seq} & multi-task & 93.0   \\\\ Dyer et al. (2016) \\cite{dyer-rnng:16} & generative & 93.3   \\\\ \\hline \\end{tabular} \\end{center} \\end{table}  To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes \\cite{KVparse15}.  We trained a 4-layer transformer with $d_{model} = 1024$ on the Wall Street Journal (WSJ) portion of the Penn Treebank \\citep{marcus1993building}, about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences \\citep{KVparse15}. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.  We performed only a small number of experiments to select the dropout, both attention and residual (section~\\ref{sec:reg}), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + $300$. We used a beam size of $21$ and $\\alpha=0.3$ for both WSJ only and the semi-supervised setting.  Our results in Table~\\ref{tab:parsing-results} show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar \\cite{dyer-rnng:16}.  In contrast to RNN sequence-to-sequence models \\citep{KVparse15}, the Transformer outperforms the BerkeleyParser \\cite{petrov-EtAl:2006:ACL} even when training only on the WSJ training set of 40K sentences.   "
            },
            {
                "section_name": "Conclusion_7",
                "paragraphs": "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.  For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.  We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.  The code we used to train and evaluate our models is available at \\url{https://github.com/tensorflow/tensor2tensor}.  \\paragraph{Acknowledgements} We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.  \\bibliographystyle{plain} \\b  \\n \\pagebreak \\section*{Attention Visualizations}\\label{sec:viz-att} \\begin{figure*}[h] {\\includegraphics[width=\\textwidth, trim=0 0 0 36, clip]{./vis/making_more_difficult5_new.pdf}} \\caption{An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color.} \\end{figure*}  \\begin{figure*} {\\includegraphics[width=\\textwidth, trim=0 0 0 45, clip]{./vis/anaphora_resolution_new.pdf}} {\\includegraphics[width=\\textwidth, trim=0 0 0 37, clip]{./vis/anaphora_resolution2_new.pdf}} \\caption{Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.} \\end{figure*}  \\begin{figure*} {\\includegraphics[width=\\textwidth, trim=0 0 0 36, clip]{./vis/attending_to_head_new.pdf}} {\\includegraphics[width=\\textwidth, trim=0 0 0 36, clip]{./vis/attending_to_head2_new.pdf}} \\caption{Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.} \\end{figure*}  \\a \\n \\i  \\i  \\end{document}   "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/1706.03762/ModalNet-21.png",
                "caption": "The Transformer - model architecture.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/ModalNet-19.png",
                "caption": "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/ModalNet-20.png",
                "caption": "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/table1.tex",
                "caption": " Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. $n$ is the sequence length, $d$ is the representation dimension, $k$ is the kernel size of convolutions and $r$ the size of the neighborhood in restricted self-attention.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/table2.tex",
                "caption": "The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.  ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/table3.tex",
                "caption": "Variations on the Transformer architecture. Unlisted values are identical to those of the base model.  All metrics are on the English-to-German translation development set, newstest2013.  Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1706.03762/table4.tex",
                "caption": "The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)",
                "description": ""
            }
        ]
    },
    "2108.12409": {
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
        "tldr": "This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "\\label{sec:intro}     When constructing a transformer-based language model, a major design decision is the length of training sequences, denoted $L$ herein, which has to date  been equivalent to the length of inference sequences.  More context, achieved by larger $L$, improves predictions at inference time.  But longer sequences are more expensive to train on.\\footnote{Figure~\\ref{fig:wt103_train_speeds_all} in the appendix plots training speed, in words per second, against $L$.}   Before transformers, RNN language models were trained on shorter-$L$ sequences and assumed to generalize to longer contexts at inference time~\\citep{Mikolov2010RecurrentNN, Mikolov2012ContextDR, zaremba2014recurrent}.  \\citet{vaswani}, introducing the transformer, speculated that it ``may [...] extrapolate to sequence lengths longer than the ones encountered during training.'' We define \\emph{extrapolation} as a model's ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained. We find that transformer language models (LMs)  that use sinusoidal position embeddings have very weak extrapolation abilities; see Figure~\\ref{fig:wt103_extra}.    \\begin{figure}[h] \\centering \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/wt103_extra_512.pdf} \\end{subfigure} \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/wt103_extra_1024.pdf} \\end{subfigure} \\caption{Extrapolation: as the (validation-set's) input sequence gets longer ($x$-axis), current position methods (sinusoidal, rotary, and T5) show degraded perplexity ($y$-axis, lower is better), but our method (\\S\\ref{sec:ourmethod}) does not.  Models were trained on WikiText-103 with sequences of $L=$ 512 (left) or $L=$ 1,024 (right) tokens.  T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables~\\ref{tab:appendix:wt103_extra_512} and~\\ref{tab:appendix:wt103_extra_1024} in the appendix.}  \\label{fig:wt103_extra} \\end{figure}            We demonstrate that this failure to extrapolate is caused by the position embedding method.  As shown in Figure~\\ref{fig:wt103_extra}, recent alternatives to the original sinusoidal position method \\citep{roformer,t5} have improved extrapolation.  However, the better of these, the T5 bias, is considerably slower than the sinusoidal approach and uses extra memory and parameters (Figure~\\ref{fig:wt103_speed_mem}).    We therefore introduce Attention with Linear Biases (ALiBi) to facilitate  efficient extrapolation. ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Our simple approach eliminates position embeddings.    Compared to a sinusoidal model trained on the same input length, our method requires no additional runtime or parameters and incurs a negligible (0--0.7\\%) memory increase.  ALiBi can be implemented by changing only a few lines of existing transformer code.   Using ALiBi, a transformer LM can be trained on short-$L$ sequences and therefore at much lower cost, and it can still be reliably applied to long sequences at runtime. For example, a 1.3 billion parameter LM trained on $L=$ 1024 tokens with ALiBi achieves the same perplexity as a sinusoidal model trained on $L=$ 2048 when both are tested on sequences of 2048 tokens, even though \\textit{our model is 11\\% faster and uses 11\\% less memory. }   Though performance peaks at around two times the number of tokens that the model was trained on, ALiBi maintains strong performance even on sequences of length 10,000. In recently explored settings where NLP training examples are given as context to an LM \\citep{gpt3}, our approach will allow exposure to more examples. Additionally, it enables generation of longer outputs.              "
            },
            {
                "section_name": "Current Approaches Do Not Extrapolate Efficiently _2",
                "paragraphs": "\\label{sec:act2}    We show for the first time that the sinusoidal position method, which technically should be able to extrapolate, in practice has very limited extrapolation capabilities. Though the rotary position method improves over the sinusoidal one, it still does not achieve satisfying results.  Holding everything else constant, we are the first to observe that the T5 bias method leads to better extrapolation than either of these, and so we conclude that extrapolation ability depends heavily on the position embedding.  Unfortunately, the T5 bias is computationally costly (Figure~\\ref{fig:wt103_speed_mem}).   "
            },
            {
                "section_name": "Background and Experimental Setup_1",
                "paragraphs": " A transformer LM receives a list of tokens and outputs a probability distribution representing its prediction for the next token.  We call the input list the \\textit{current input subsequence} since the inputs to language models are typically subsequences from (much longer) training or evaluation sequences.  During both training and perplexity evaluation (i.e., scoring a fixed sequence), many predictions can be calculated at once; this is done using a ``causal mask'' that ensures each position's prediction is influenced only by tokens to its left.  Let $L$ be the length of each input subsequence during training; it includes $L$ predictions, which on average have access to $\\frac{L+1}{2}$ tokens of (left) context. To explore a model's extrapolation abilities, we are interested in cases where sequences of length $L_{\\textit{valid}} > L$ are considered at evaluation time. When $L$ differs between inference and training, we use \\lt to refer to the length of subsequences during training and \\li  to refer to their length at validation.    \\paragraph{Nonoverlapping Inference} To train on or evaluate a sequence longer than $L$ tokens, it is typical to segment the sequence into $L$-length subsequences and train on or evaluate them independently.  Unless otherwise stated, we use nonoverlapping inference to report perplexity scores.     \\paragraph{Extrapolation During Inference} Formally, the functions that define a transformer layer are agnostic to input length;\\footnote{These include the embedding lookup, feedforward sublayer, and softmax layer, which act independently on vector inputs, as well as the attention sublayers, whose parameters do not depend on input length (and which must handle variable-length inputs, e.g., due to causal masking). } they map from some arbitrary, unfixed number of input vectors to the same number of output vectors.  When transformers are applied to data that is inherently sequential, like text, positional information is injected into the inputs in various ways.     \\citet{vaswani} discussed two options for embedding positions into vectors to be added to word embeddings:  learning embeddings for specific positions and unlearned sinusoidal embeddings.  They observed similar performance between these two but preferred the sinusoidal approach, which they argued might extrapolate to longer input sequences during inference. We find that this model cannot extrapolate to more than a few dozen tokens beyond $L$.\\footnote{The learned positional embedding approach does not have a way to encode positions greater than $L$; it therefore has no ability to extrapolate.}       \\paragraph{Experiment Setup} We first test the extrapolation abilities of various position methods on the WikiText-103 corpus~\\citep{pointer} using the transformer language model of~\\cite{baevski}. We use this model because of its prominent role in recent  language modeling developments~\\citep{khandelwal20generalization, shortformer}. The training set is about 103 million tokens from  English Wikipedia (half a gigabyte).  The  model has $16$ transformer layers of dimension $1024$, with $8$ heads, and a feedforward inner dimension of $4096$. This model ties the word embedding and softmax matrices~\\citep{tying, inan2017}. In our experiments, other than varying the position method and training subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs (205).       \\begin{figure} \\centering \\begin{subfigure}{.28\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/wt103_Training-Speed.pdf} \\end{subfigure} \\begin{subfigure}{.28\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/wt103_Inference-Speed.pdf} \\end{subfigure} \\begin{subfigure}{.28\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/wt103_Training-Memory-Usage.pdf} \\end{subfigure} \\begin{subfigure}[]{.14\\textwidth} \\centering \\raisebox{3mm}{\\includegraphics[width=\\linewidth]{figures/legend.pdf}} \\end{subfigure} \\caption{A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1\\% during training and 3\\% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table~\\ref{tab:baselines_speed_mem} in the appendix for exact numbers. } \\label{fig:wt103_speed_mem} \\end{figure}    "
            },
            {
                "section_name": "Measuring Extrapolation_2",
                "paragraphs": "\\paragraph{Sinusoidal Position Embeddings} Sinusoidal position embeddings (\\citealp{vaswani}; \\S 3.5) are constant, non-learned vectors that are added to token embeddings on input to the first layer of the transformer. They are frequently used in transformer language modeling~\\citep{baevski,lewis2021base} and machine translation~\\citep{vaswani,ott2018scaling} models. We first consider the unmodified model of~\\cite{baevski}, which uses sinusoidal position embeddings, and train it on $L=512$ tokens; we then run inference with it on the validation set on $L+k$ tokens, with $k$ ranging from 0 to 15,000. Figure~\\ref{fig:wt103_extra} (left) and the corresponding Table~\\ref{tab:appendix:wt103_extra_512} (in the appendix) show that while the model improves  perplexity up to $k=20$, performance stops improving and stays steady from $k=20$ to $k=50$ and then begins degrading. Similar results are obtained for a model trained with $L=1024$ tokens (Figure~\\ref{fig:wt103_extra} (right) and Table~\\ref{tab:appendix:wt103_extra_1024} in the appendix). That model improves for up to $\\li = \\lt + 50$ tokens, after which performance declines.  \\paragraph{Rotary Position Embeddings} The rotary method was introduced by~\\cite{roformer} and has recently been popularized by the open source GPT-3~\\citep{gpt3} implementation GPT-J~\\citep{gpt-j}. Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.  Unlike the sinusoidal or learned positional embedding approach, the rotary method injects position information into the model at every layer, not just at the initial one.  In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method (\\S\\ref{sec:ourmethod}).  We apply the rotary position embedding method to our Baevski \\& Auli baseline.\\footnote{Our rotary method implementation is based on the code in \\url{https://github.com/JunnYu/RoFormer_pytorch}, which is linked to from the official repository of~\\cite{roformer}: (\\url{https://github.com/ZhuiyiTechnology/roformer}). After we finished running our experiments with the rotary method, we were informed that the runtime of the code linked above could be optimized, making it only 2\\% slower than the sinusoidal approach. This optimization would not change extrapolation performance.} The perplexity results (Figure~\\ref{fig:wt103_extra} and Appendix Tables~\\ref{tab:appendix:wt103_extra_512} and~\\ref{tab:appendix:wt103_extra_1024}) are better than the sinusoidal approach: the model with $L=512$ ($L=1024$) improves perplexity with up to $k=200$ ($k=100$) more tokens than it saw during training, but this comes at the cost of slower training and inference (Figure~\\ref{fig:wt103_speed_mem}).  \\paragraph{T5 Bias} Though most models use trained or sinusoidal position embeddings, the T5 model of~\\cite{t5} uses a relative position method~\\citep{shaw,Huang2019MusicTG} that adds no  position information to word embeddings (as in the previous method). Instead, it modifies the way attention values are computed. We refer to this as the ``T5 bias'' method.\\footnote{This method is similar to the one used in~\\citet[Equation 7]{parikh-etal-2016-decomposable}.} To compute attention values in the unmodified transformer, we compute the dot product of every query with every relevant key and then softmax these attention values. In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key. Therefore, all query-key scores where the query and key distance are zero (i.e., the query and key represent the same token) get a specific learned bias, all scores where the query and key are one word away get a different learned bias, and so on, up to a certain point, from where multiple different distances share the same learned bias (which might be beneficial for extrapolation). As in the rotary method, the T5 bias injects position information into the model at every layer and integrates no explicit position information into the self-attention value vectors.  \\cite{t5} propose that the T5 bias may allow extrapolation, but they did not report experiments testing this.  Here, we show that the T5 bias does allow language models to extrapolate. We do this by again modifying the Baevski \\& Auli model, this time to insert the T5 bias into it.\\footnote{Our T5 bias implementation is based on the one used in HuggingFace Transformers~\\citep{huggingface}, which in turn is based on the official Mesh Tensorflow T5 code. }  As Figure~\\ref{fig:wt103_extra} shows, the T5 bias improves perplexity with longer sequences than the ones it was trained on, i.e.,  $k=600$ ($k=800$) extra tokens for a model trained on $L=512$ ($L=1024$) input tokens.  Unfortunately, this impressive performance comes at a cost: training is at least twice as slow as with the sinusoidal model. Therefore, this model's extrapolation ability provides no efficiency advantage. For example, to do inference on 1024 tokens, we could either train the sinusoidal model with \\lt = 1024 or train the T5 bias model on \\lt = 512 tokens and extrapolate to 1024 for inference. However, the \\lt = 1024 sinusoidal model runs at 28.5k words per second (WPS), while the \\lt = 512 T5 bias model runs at 14.4k WPS (Appendix Table~\\ref{tab:baselines_speed_mem}), so there is no speedup when training on shorter sequences with this method.\\footnote{ \\citet{narang2021transformer} benchmarked the T5 bias as being just 8.7\\% slower than the sinusoidal approach; thus,   while always incurring a runtime penalty, this method's runtime could be faster depending on the choice of hardware and software frameworks used. Narang et al. used the Tensorflow T5 library running on TPUs, while we used the PyTorch Fairseq library running on GPUs. }                       "
            },
            {
                "section_name": "Attention with Linear Biases (ALiBi)_3",
                "paragraphs": "\\label{sec:ourmethod}   \\begin{figure} \\begin{center} \\includegraphics[width=.48\\textwidth]{figures/fig1.pdf} \\end{center} \\caption{When computing attention scores for each head, our linearly biased attention method, ALiBi,  adds a constant bias (right) to each attention score ($\\mathbf{q}_i \\cdot \\mathbf{k}_j$, left). As in the unmodified attention sublayer, the softmax function is then applied to these scores, and the rest of the computation is unmodified. \\textbf{$\\textbf{m}$ is a head-specific scalar} that is set and not learned throughout training. We show that our method for setting $m$ values generalizes to multiple text domains, models and training compute budgets. When using ALiBi, we do \\emph{not} add positional embeddings at the bottom of the network.  } \\label{fig:1} \\end{figure}      In the transformer model of \\citet{vaswani}, position embeddings are added to the word embeddings at the bottom of the network. For an input subsequence of length $L$, the attention sublayer computes the attention scores for the $i$th query $\\mathbf{q}_i \\in \\mathbb{R}^{1\\times d}$, ($1 \\leq i \\leq L$) in each head, given the first $i$ keys $\\mathbf{K} \\in \\mathbb{R}^{i\\times d}$, where $d$ is the head dimension: \\begin{equation*} \\text{softmax}(\\mathbf{q}_i \\mathbf{K}^\\top) \\end{equation*} These attention scores are then multiplied by the values to return the output of the attention sublayer.\\footnote{For simplicity we omit the key, query, value and final output projections, dropout, and the scaling factor.}  When using ALiBi, we do not add position embeddings at any point in the network. The only modification we apply is after the query-key dot product, where we add a static, non-learned bias:\\footnote{The ALiBi bias is not multiplied by the $\\sqrt{d_k}$ scaling factor from Equation 1 of~\\citet{vaswani}.} \\begin{equation*} \\text{softmax}(\\mathbf{q}_i \\mathbf{K}^\\top + m \\cdot [-(i-1), ..., -2, -1, 0]), \\end{equation*} where scalar $m$ is a head-specific slope fixed before training. Figure~\\ref{fig:1} offers a visualization.  For our models with 8 heads, the slopes that we used are the geometric sequence: ${\\frac{1}{2^1}, \\frac{1}{2^2}, ..., \\frac{1}{2^8}}$. For models that require 16 heads, we interpolate those 8 slopes by geometrically averaging every consecutive pair, resulting in the geometric sequence that starts at $\\frac{1}{\\sqrt{2}}$ and has the ratio of $\\frac{1}{\\sqrt{2}}$: ${\\frac{1}{2^{0.5}}, \\frac{1}{2^1}, \\frac{1}{2^{1.5}}, ..., \\frac{1}{2^{8}}}$. In general, for $n$ heads, our set of slopes is the geometric sequence that starts at $2^{\\frac{-8}{n}}$ and uses that same value as its ratio.   In \\S\\ref{sec:results}, we observe that this set of slopes works on a wide variety of text domains and model sizes. Therefore, we do not believe that it is necessary to tune these slope values every time a new model is trained on a new dataset. This makes our method similar to the sinusoidal approach, where the hyperparameters (the start and end of the geometric progression of wavelengths) were set once by~\\citet{vaswani} and then reused in different models of different sizes on different datasets.   \\al has an inductive bias towards recency; it penalizes attention scores between distant query-key pairs, with the penalty increasing as the distance between a key and a query grows. The different heads increase their penalties at different rates, depending on the slope magnitude.  We initially experimented with making the slopes trainable, but this did not yield strong extrapolation results.\\footnote{In our experiments, trainable slopes also slowed down the training speed by 3\\%.} A brief manual exploration of around ten slope sets led us to discover the set of slopes that we finally picked. Our main insight from this exploration is that the slope sets that work best are those with slopes in the $(0,1)$ range, with the slopes' density increasing as we get closer to $0$. We also found our method to be robust to slope choice. Even randomly sampling from the exponential distribution worked well in some cases (although that method had high variance).  Since ALiBi is a relative position method, we add position information at every layer to the keys and queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that these properties might be beneficial for extrapolation.  \\paragraph{Implementation.} ALiBi is easy to implement, with all changes accomplished in a few lines of code. We implement it by modifying the mask matrix by adding the linear biases to it (in practice, when training a transformer LM, query $\\mathbf{q}_i$ attends only to keys $1$ to $i$; this is implemented by adding a mask matrix to the query-key dot product before the softmax operation is applied). This means that there is no runtime penalty when using our method since we add no operations to the network.  Compared to the sinusoidal model trained on the same input lengths, AliBi incurs a memory increase (up to 100MB in some of our experiments): in the unmodified transformer, the mask is of size $L\\times L$; when using ALiBi, the mask is a slightly larger $n \\times L\\times L$ (where $n$ is the number of heads) since the linear biases added for each head uses a different slope. But, as we show, ALiBi enables training on much smaller sequences while still achieving (and occasionally surpassing) results obtained using sinusoidal embeddings on longer sequences, which saves multiple gigabytes of memory.  "
            },
            {
                "section_name": "Results_4",
                "paragraphs": "\\label{sec:results} We first show that on WikiText103 ALiBi is efficient and enables training models with short input subsequences that outperform strong baselines even when the ALiBi models extrapolate to more than six times the number of tokens that they were trained on. We then take the same hyperparameters for our method (the set of slopes) that worked on WikiText-103 and show that -- with no modification -- they provide strong results on a dataset in a very different domain: books. Finally, we show that a 1.3B parameter model trained with AliBi on a much larger (461 GB) dataset with much more compute provides a superior alternative to the sinusoidal method since it achieves similar perplexity scores while running faster and using less memory (since it is trained on shorter inputs).  While multiple alternatives to the position methods presented in~\\cite{vaswani} have been proposed, few have been adopted in large (1B or more parameter) LMs since  that setting is much more challenging than the smaller scale experiments. GPT-3 and Jurassic-1~\\citep{J1WhitePaper} use the learned position embedding method from Vaswani et al., and GPT-J uses the rotary method. Our results on the 1.3B parameter model show our method's ability to generalize to larger models, dataset sizes and training durations without retuning the hyperparameter.   "
            },
            {
                "section_name": "Results on WikiText-103 and Toronto BookCorpus_3",
                "paragraphs": " \\begin{figure}[h] \\begin{center} \\includegraphics[width=.65\\textwidth]{figures/wt103_extra_lsb_all.pdf} \\end{center} \\caption{\\al models trained and evaluated on varying sequence lengths on the WikiText-103 validation set and the sinusoidal baseline (not evaluated on longer sequences). All of our models outperform the sinusoidal ones even when trained on fewer tokens. Appendix Table~\\ref{tab:LSB_wt103} has exact perplexities, more \\al models (trained on fewer tokens), and results for rotary and T5 bias models. } \\label{fig:wt103_extra_lsb_all} \\end{figure}   We first develop our method on the WikiText-103 corpus~\\citep{pointer}, replacing the sinusoidal position embeddings in the language model of~\\cite{baevski} with \\al.   Figure~\\ref{fig:wt103_extra_lsb_all} (and the corresponding Appendix Table~\\ref{tab:LSB_wt103}) show our results for models trained with varying numbers of input subsequence tokens ($L$), extrapolating to longer subsequence lengths on the validation dataset. Our first observation is that, without extrapolation, for every $L$, our models outperform  those using the sinusoidal method, sometimes by a  significant amount. For example, the Baevski \\& Auli model achieves 18.67$\\pm$0.24 (std.~dev.) perplexity when trained with $L = 3072$ input tokens, but our $L=3072$ model achieves 17.60 perplexity (when both models evaluate with \\li = 3072).  Our second observation is that all of our models can extrapolate, and they obtain improved perplexity scores when handling more tokens than they observed during training. For example, our model trained on 512 tokens (which achieves 19.73 perplexity when evaluating subsequences of length 512 in the development set) achieves a perplexity score of 18.40 on the development set when extrapolating to subsequences of length 3072. Surprisingly, this surpasses the score that the $L=3072$ sinusoidal model obtains on the development set by a statistically significant margin. Note that all our models trained on $L=512$ to $L=2048$ outperform the sinusoidal baseline trained on $L=3072$ when extrapolating to \\li = 3072 even though those models all take much less time to train since they train on shorter subsequences (Appendix Figure~\\ref{fig:wt103_train_speed_vs_ppl} compares training speed to perplexity for these models)! The $L=512$ model is 1.84 times faster to train and yet still outperforms the $L=3072$ sinusoidal model when extrapolating to $\\li = 3072$. In addition, training the $L=3072$ sinusoidal model requires a GPU with more than 16 GB of memory to fit the large attention matrices, which our $L=512$ outperforms even though it can be trained on a GPU with much less memory due to much smaller attention matrices.  Additionally,  Table~\\ref{tab:LSB_wt103} (in the appendix) also shows that, for $L$s  of 1024 and 3072, our method performs better than the rotary and T5 bias models even when \\li = $L$ (i.e., no extrapolation is occurring). Figure~\\ref{fig:wt103_extra} (and the corresponding Appendix Tables~\\ref{tab:appendix:wt103_extra_512} and~\\ref{tab:appendix:wt103_extra_1024}) more broadly explore our method vs.~the other position methods. They show that the T5 bias (the best of the baselines) improves perplexity until \\li is around $2L$, but on the WikiText-103 dataset our method continually improves perplexity until at least around $3L$, with the $L=512$ model improving perplexity even when \\li exceeds 12k tokens. Even when unable to improve perplexity given longer sequences, \\al always maintains strong performance as more tokens are added.    Appendix Table~\\ref{tab:wt103_test} shows that our results on the validation set also transfer to the test set of WikiText-103. Currently, almost all models that present results on WikiText-103 use sliding window evaluation  (defined in \\S\\ref{sec:analysis}) to compute perplexities. We apply that method to our (and to the sinusoidal, rotary and T5 bias) models in Appendix Table~\\ref{tab:wt103_test_sota}. We find that our L = 3072 model surpasses the performance of Transformer-XL~\\citep{transformer-xl}, the Sandwich~\\citep{sandwich}, and Shortformer~\\citep{shortformer} models. Our results are similar to the ones obtained with staged training~\\citep{shortformer} but fall short of results obtained by Routing Transformer~\\citep{roy2020efficient} and kNN-LM~\\citep{khandelwal20generalization}. The methods used in those models are orthogonal to ours, and we hypothesize that combining them with ours might lead to even larger performance increases.   After developing our method on WikiText-103, in Appendix Section~\\ref{subsec:tbc}, we run one set of experiments on a  different domain (books) using a similar model architecture and without modifying any of the \\al hyperparameters (the slopes) and show that our results fully transfer to this new domain. Our models are able to both surpass the sinusoidal baseline when not extrapolating while also outperforming it when extrapolating to longer sequences.   "
            },
            {
                "section_name": "Results on the CC100+RoBERTa Corpus_4",
                "paragraphs": " Our final set of experiments investigates whether \\al transfers to a larger model trained with a larger computational budget on a larger dataset than the ones we previously used. We show that our method achieves strong results in this more challenging setting, obtaining similar performance to the sinusoidal baseline while using significantly less memory, since we train on shorter subsequences.  The dataset we choose is a combination of the datasets used to train the RoBERTa~\\citep{roberta} implementation of BERT~\\citep{bert} and the English part of the CC-100 corpus introduced in~\\cite{cc-100}, for a total of 461 GB. The RoBERTa training corpus---i.e., the Toronto Book Corpus~\\citep{zhu2015aligning}, English Wikipedia, CC-News~\\citep{ccnews}, OpenWebText~\\citep{openwebtext} and Stories~\\citep{stories})---is 161 gigabytes, and the English part of the CC-100 corpus is 300 gigabytes. The validation set contains 649K tokens.        Our models for this dataset have 25 transformer layers with 16 heads and a dimension of 2048, with an 8192 hidden dimension of the feedforward sublayers. These models have 1.3B parameters. We train our models for one epoch, which is 50k updates on 128 V100 GPUs.    \\begin{figure} \\centering \\begin{subfigure}{.45\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/ccr-train-512.pdf}  \\end{subfigure} \\begin{subfigure}{.45\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/ccr-train-1k.pdf} \\end{subfigure} \\caption{ On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens. The \\al models obtain strong results even though they use 6\\%-11\\% less memory since they train on shorter sequences. Appendix Table \\ref{tab:ccr} shows memory use and end-of-training perplexities.} \\label{fig:ccr_train_all} \\end{figure}   In Figure~\\ref{fig:ccr_train_all} (left), we compare the validation perplexity for \\li = 1024 throughout the training process for an \\al model trained with \\lt = 512 compared to the sinusoidal model trained with \\lt = 1024. Since our model is trained on shorter sequences, it is 7\\% faster and uses 1.6 GB less memory. We halt training of the sinusoidal baseline when our model reaches the end of its training (one epoch). At that time, our model is just 0.06 perplexity away from the baseline even though it was trained on sequences that are half the length of those the baseline used and requires less memory.  In Figure~\\ref{fig:ccr_train_all} (right), results become even more impressive, showing that our model trained on \\lt = 1024 outperforms by 0.09 perplexity the sinusoidal model trained on \\lt = 2048 (when evaluating with \\li = 2048) even though our model uses 3.1 GB less memory. Our model maintains a lead in perplexity over the sinusoidal model during the entire training process. By sampling five evenly distributed points across the training process, we compute that our \\lt = 1024 model reaches a given perplexity value, on average, 11\\% faster than the sinusoidal model does.  Since our models in these comparisons use much less memory, they allow for stacking more layers, which would further improve performance (with negligible, if any, runtime cost). To keep our experiments as straightforward as possible, however, we do not add layers to our models.  Appendix Table~\\ref{tab:ccr_all_50k} presents additional results comparing our models to the sinusoidal baseline when both are trained on the same $L$, showing that \\al performs similarly to the sinusoidal baseline when not extrapolating. This contrasts with the  results presented on the smaller datasets, where \\al consistently outperforms other position methods even when not extrapolating, suggesting that ALiBi's inductive bias provides additional benefits for lower-resource language modeling.   \\begin{figure}[h] \\centering \\begin{subfigure}{.45\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/ccr_extra_512.pdf} \\end{subfigure} \\begin{subfigure}{.45\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/ccr_extra_1024.pdf} \\end{subfigure} \\caption{The ALiBi and sinusoidal models (with both $L$ = 512 and 1024) trained for 50k updates (1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. \\al achieves the best results at around $2L$ but maintains strong performance even up to 10000 tokens in these experiments.} \\label{fig:ccr_extra} \\end{figure}   Figure~\\ref{fig:ccr_extra} shows that our models trained on $L=512$ and $L=1024$ achieve the best results when extrapolating to about double the tokens that they were trained on. Specifically, the \\lt = 512 model (that obtains 9.79 perplexity when \\li = 512) achieves its best score (9.3) when extrapolating to 1012 tokens, and the \\lt = 1024 model (that obtains 9.16 perplexity when \\li = 1024) achieves its best score (8.9) when extrapolating to 2024 tokens.   One possible explanation is that the subsequences the model observes during training are up to $L$ tokens long. When performing inference on subsequences of length $2L$, half of the subsequences the model consumes are as long as the examples seen during training. When inference is performed on subsequences of length $2L+1$ or longer, less than half of the predictions the model makes are on subsequences of lengths seen during training, and that might degrade performance.  The sinusoidal model cannot extrapolate at all in this setting, with its performance degrading for both the \\lt = 512 and 1024 models as soon as one token more than $L$ is added during evaluation.   In Appendix \\ref{sec:analysis}, we find that \\al's  edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse.  We posit  that future work building on \\al might achieve further gains by more efficiently  exploiting longer histories.    "
            },
            {
                "section_name": "Related Work_5",
                "paragraphs": "In parallel with our work,~\\citet{wennberg2021case} introduce a relative position method that, like our method, adds a bias to attention scores that is a function of the distance between the key and query elements. Unlike our \\al method, which uses a  non-learned linear function, their method uses a radial-basis function, with multiple trainable parameters (in our experiments, this led to a slight decrease in runtime). In addition, they present experiments on text classification, not on language modeling.  They do not explore extrapolation. The Distance Aware Transformer~\\citep{da-transformer} multiplies attention scores by a bias that is a function of the distance between the key and query. This function uses a different, learned parameter in every head. They show results only on text classification. In our experiments (not presented), multiplying attention scores by the bias (instead of adding, as in \\al) degraded performance.  Transformer-XL~\\citep{transformer-xl}  presented a language model that uses a cache and can attend to more tokens during inference than it was trained on (by increasing the length of the cache). However, this work presents results only where output length is limited to the $L$ (the training length), and their relative position method is very slow~\\citep{shortformer}. The Longformer~\\citep{longformer} adapts models trained on shorter sequences to document-level tasks. However, to achieve this they had to partially train their models on longer sequences. Our \\al method enables extrapolation without any additional training on longer sequences.  To our knowledge, extrapolation has not been previously explored in transformer language modeling, but it has been investigated previously and concurrently with transformers on other tasks, such as machine translation~\\citep{rosendahl2019:pos_enc, neishi-yoshinaga-2019-relation, newman2020extrapolation, Kiyono2021SHAPESA}, sequence-to-sequence models trained on an artificial dataset~\\citep{Hupkes2020}, pretrained sequence-to-sequence models tested on arithmetic tasks~\\citep[Appendix C]{Nogueira2021InvestigatingTL}, models trained with reinforcement learning~\\citep{lampinen2021towards}, image, speech recognition, and machine translation models~\\citep{likhomanenko2021cape}, and protein structure prediction~\\citep[Appendix 1.5]{Jumper2021HighlyAP}.        "
            },
            {
                "section_name": "Conclusion_6",
                "paragraphs": " We showed that the sinusoidal position embedding approach does not enable transformers to extrapolate to inputs longer than the ones they were trained on. We then established that extrapolation in transformers can be enabled by just changing the position method. We showed that our \\al method offers an extremely simple replacement for existing position approaches and allow models to extrapolate. In addition, when not extrapolating, our method achieves either better perplexity than the sinusoidal method (in models smaller than 1B parameters, trained on less data) or similar perplexity (in larger, billion parameter models trained on much more data). \\al is simple to implement and does not slow down runtime or require extra parameters (but does occasionally require a negligible amount of extra memory). Using our method, we sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048).  \\subsubsection*{Acknowledgments} We thank Tim Dettmers, Gabriel Ilharco, Jungo Kasai, Hao Peng, Sewon Min, Sofia Serrano, Sam Shleifer, Luke Zettlemoyer, Julian Michael, Nikolaos Pappas, Yizhong Wang, and the anonymous reviewers for their valuable feedback and fruitful discussions.  \\clearpage \\bibliography{iclr2022_conference} \\bibliographystyle{iclr2022_conference}  \\clearpage \\appendix "
            },
            {
                "section_name": "Appendix_7",
                "paragraphs": "\\label{sec:appendix}   "
            },
            {
                "section_name": "Introduction_5",
                "paragraphs": " The training speed of transformer LMs gets slower as the input subsequence length $L$ increases. Figure~\\ref{fig:wt103_train_speeds_all} visualizes this. \\begin{figure}[h] \\begin{center} \\includegraphics[width=1\\textwidth]{figures/wt103_train_speeds_all.pdf} \\end{center} \\caption{ Training speed of our model and the sinusoidal baseline trained on different amounts of input subsequence tokens $L$.} \\label{fig:wt103_train_speeds_all} \\end{figure}    Table~\\ref{tab:baselines_speed_mem} contains the runtimes and memory use statistics for models using the various position methods discussed in this work. \\begin{table}[h]  \\caption{ The speed (during training and evaluation, in words per second) and memory usage (during training) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline  on WikiText-103. Training and inference are batched, and speeds are shown for one V100 GPU. } \\label{tab:baselines_speed_mem} \\begin{center} \\begin{tabular}{@{}lrccc@{}} \\toprule \\multirow{2}{*}[0pt]{Position Method}& \\multirow{2}{*}[0pt]{Train Length}&  \\multicolumn{2}{c}{Speed ($\\uparrow$)} & \\multirow{2}{*}[0pt]{Memory ($\\downarrow$)} \\\\ &  & Train  & Eval. &   \\\\\\midrule          & 512           &28.5k &82.1k  & 15.3 GB  \\\\ Sinusoidal&1024 &26.0k &77.8k  & 19.2 GB  \\\\ & 3072          &15.3k &42.4k  & 15.1 GB \\\\\\midrule &512            &20.0k &43.4k  & 17.8 GB  \\\\ Rotary & 1024   &17.7k &39.4k  & 22.8 GB  \\\\ & 3072          &11.5k &29.5k  & 17.8 GB  \\\\\\midrule & 512           &14.4k &21.8k  & 16.9 GB  \\\\ T5 Bias& 1024   &13.0k &20.2k  & 20.9 GB  \\\\ & 3072          & 4.3k & 4.9k  & 15.9 GB  \\\\\\midrule & 512           &28.3k &85.8k  & 15.3 GB \\\\ ALiBi & 1024    &25.8k &76.4k  & 19.3 GB \\\\ & 3072          &15.5k &42.2k  & 15.2 GB \\\\  \\bottomrule \\end{tabular} \\end{center} \\end{table}    Tables~\\ref{tab:appendix:wt103_extra_512}, \\ref{tab:appendix:wt103_extra_1024}, and \\ref{tab:appendix:wt103_extra_3072} show the perplexity and runtime of models using the sinusoidal, rotary T5 bias, and \\al position methods when extrapolating to sequences longer than the ones they were trained on. The models used in these tables were trained on $L = 512, 1024$ and $3072$ tokens. \\begin{table}[] \\small \\caption{The sinusoidal, rotary, T5 bias and \\al models trained on \\lt = \\textbf{512} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.} \\label{tab:appendix:wt103_extra_512} \\begin{center}  \\begin{tabular}{@{}lcccccccc@{}} \\toprule &  \\multicolumn{2}{c}{Sinusoidal} & \\multicolumn{2}{c}{Rotary} &\\multicolumn{2}{c}{T5 Bias} & \\multicolumn{2}{c}{ALiBi} \\\\ Inputs & PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$) \\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-9}  512    & 20.05                           & 15046 & 20.07                           & 10839 & 19.65                           & 11724 & 19.73                           & 14726 \\\\ 513    & 19.98                           & 14925 & 20.01                           & 10806 & 19.57                           & 10491 & 19.62                           & 14965 \\\\ 522    & 19.93                           & 15116 & 20.02                           & 11295 & 19.57                           & 9970  & 19.64                           & 15316 \\\\ 532    & \\textbf{19.91} & 15358 & 19.98                           & 10854 & 19.53                           & 10382 & 19.61                           & 15383 \\\\ 542    & \\textbf{19.91} & 15076 & 19.94                           & 10795 & 19.47                           & 12270 & 19.57                           & 15301 \\\\ 552    & \\textbf{19.91} & 16394 & 19.93                           & 12267 & 19.47                           & 13000 & 19.54                           & 16540 \\\\ 562    & \\textbf{19.91} & 16646 & 19.87                           & 12481 & 19.39                           & 12201 & 19.49                           & 16385 \\\\ 572    & 19.95                           & 16934 & 19.83                           & 12668 & 19.36                           & 12851 & 19.46                           & 16881 \\\\ 582    & 20.13                           & 16961 & 19.88                           & 12594 & 19.41                           & 13904 & 19.48                           & 17064 \\\\ 592    & 20.18                           & 17243 & 19.84                           & 13007 & 19.36                           & 13706 & 19.43                           & 17289 \\\\ 602    & 20.40                           & 17502 & 19.81                           & 12788 & 19.33                           & 14102 & 19.38                           & 17141 \\\\ 612    & 20.59                           & 17637 & 19.81                           & 12601 & 19.27                           & 14573 & 19.38                           & 17661 \\\\ 712    & 24.86                           & 15614 & \\textbf{19.79} & 12676 & 19.10                           & 13818 & 19.14                           & 15637 \\\\ 812    & 30.82                           & 17151 & 20.17                           & 13954 & 18.94                           & 14377 & 18.99                           & 17210 \\\\ 912    & 37.42                           & 17200 & 20.73                           & 13887 & 18.86                           & 15345 & 18.88                           & 17619 \\\\ 1012   & 43.54                           & 16304 & 21.37                           & 13759 & 18.79                           & 14240 & 18.73                           & 16059 \\\\ 1112   & 50.36                           & 16424 & 22.01                           & 13891 & \\textbf{18.77} & 14014 & 18.68                           & 16659 \\\\ 1212   & 58.01                           & 17294 & 23.02                           & 15245 & 18.87                           & 14589 & 18.67                           & 17372 \\\\ 1312   & 63.62                           & 15314 & 23.93                           & 13698 & 18.84                           & 13138 & 18.60                           & 15698 \\\\ 1412   & 70.75                           & 15663 & 24.81                           & 13928 & 18.87                           & 12857 & 18.59                           & 15860 \\\\ 1512   & 76.23                           & 15812 & 25.99                           & 14248 & 18.91                           & 13752 & 18.52                           & 16225 \\\\ 2512   & 132.41                          & 15254 & 31.58                           & 13456 & 20.41                           & 9948  & 18.41                           & 15204 \\\\ 3512   & 178.97                          & 13293 & 35.54                           & 11850 & 22.91                           & 7847  & 18.40                           & 13329 \\\\ 4512   & 209.37                          & 11767 & 39.15                           & 10485 & 25.91                           & 6146  & 18.41                           & 11738 \\\\ 5512   & 240.44                          & 10168 & 43.14                           & 9020  & 29.54                           & 5309  & 18.36                           & 9986  \\\\ 6512   & 271.40                          & 9052  & 47.81                           & 8108  & 34.48                           & 4680  & 18.35                           & 9022  \\\\ 7512   & 293.02                          & 8315  & 51.12                           & 7483  & 39.29                           & 4102  & 18.33                           & 8324  \\\\ 8512   & 305.65                          & 7259  & 54.98                           & 6718  & 43.08                           & 3660  & 18.34                           & 7366  \\\\ 9512   & 336.02                          & 6672  & 57.85                           & 6211  & 48.90                           & 3370  & 18.34                           & 6555  \\\\ 10512  & 341.53                          & 6126  & 60.77                           & 5575  & 52.95                           & 3010  & 18.32                           & 6030  \\\\ 11512  & 362.74                          & 5994  & 66.62                           & 5445  & 61.38                           & 2873  & 18.32                           & 5882  \\\\ 12512  & 373.17                          & 5421  & 69.70                           & 4988  & 64.94                           & 2602  & \\textbf{18.31} & 5287  \\\\ 13512  & 382.91                          & 5174  & 73.27                           & 4692  & OOM                             & -     & \\textbf{18.31} & 4962  \\\\ 14512  & 399.98                          & 4351  & 75.52                           & 4103  & OOM                             & -     & \\textbf{18.31} & 4352  \\\\ 15512  & 406.01                          & 4291  & 79.25                           & 3969  & OOM                             & -     & \\textbf{18.31} & 4289  \\\\  \\bottomrule \\end{tabular} \\end{center} \\end{table}    \\begin{table}[] \\small \\caption{The sinusoidal, rotary, T5 bias and \\al models  trained on \\lt = \\textbf{1024} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.} \\label{tab:appendix:wt103_extra_1024} \\begin{center}  \\begin{tabular}{@{}lcccccccc@{}} \\toprule &  \\multicolumn{2}{c}{Sinusoidal} & \\multicolumn{2}{c}{Rotary} &\\multicolumn{2}{c}{T5 Bias} & \\multicolumn{2}{c}{ALiBi} \\\\ Inputs & PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$) \\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-9} 1024   & 19.34      & 17002 & 19.33  & 14690 & 18.80   & 14973 & 18.66 & 16951 \\\\ 1025   & 19.33      & 16630 & 19.34  & 14423 & 18.82   & 14635 & 18.67 & 16690 \\\\ 1034   & 19.27      & 16589 & 19.28  & 14351 & 18.74   & 14435 & 18.60 & 16707 \\\\ 1044   & 19.26      & 16760 & 19.27  & 14491 & 18.72   & 14644 & 18.60 & 16667 \\\\ 1054   & 19.23      & 16747 & 19.26  & 14503 & 18.71   & 14800 & 18.58 & 16833 \\\\ 1064   & 19.21      & 16676 & 19.22  & 14623 & 18.70   & 14498 & 18.55 & 16941 \\\\ 1074   &\\textbf{ 19.19}      & 16879 & 19.19  & 14464 & 18.65   & 14670 & 18.49 & 16936 \\\\ 1084   & 19.22      & 16942 & 19.23  & 14650 & 18.70   & 14607 & 18.56 & 17090 \\\\ 1094   & 19.24      & 16771 & 19.22  & 14629 & 18.69   & 14517 & 18.54 & 16880 \\\\ 1104   & 19.28      & 16870 & 19.27  & 14837 & 18.69   & 14635 & 18.52 & 17009 \\\\ 1114   & 19.29      & 16795 & 19.27  & 14879 & 18.69   & 14540 & 18.52 & 17050 \\\\ 1124   & 19.26      & 17312 & \\textbf{19.18}  & 15121 & 18.62   & 14480 & 18.46 & 17571 \\\\ 1224   & 20.54      & 17901 & 19.38  & 15584 & 18.58   & 14956 & 18.40 & 18013 \\\\ 1324   & 23.13      & 16308 & 19.96  & 14386 & 18.52   & 13726 & 18.33 & 16422 \\\\ 1424   & 26.45      & 16217 & 21.27  & 14385 & 18.48   & 13516 & 18.28 & 16121 \\\\ 1524   & 29.82      & 16377 & 22.59  & 14693 & 18.42   & 13587 & 18.22 & 16659 \\\\ 1624   & 34.27      & 15928 & 24.34  & 14228 & 18.40   & 12979 & 18.17 & 16053 \\\\ 1724   & 38.24      & 16640 & 25.66  & 14686 & 18.35   & 12976 & 18.15 & 16607 \\\\ 1824   & 42.23      & 16840 & 27.63  & 14918 & \\textbf{18.30}   & 13071 & 18.08 & 16846 \\\\ 1924   & 46.46      & 15071 & 29.64  & 13452 & 18.31   & 11843 & 18.08 & 15118 \\\\ 2024   & 51.09      & 15591 & 31.17  & 13706 & 18.34   & 11906 & 18.05 & 15557 \\\\ 3024   & 96.46      & 13639 & 35.67  & 12256 & 18.62   & 8480  & \\textbf{17.92} & 13668 \\\\ 4024   & 144.00     & 12441 & 44.30  & 11203 & 19.44   & 7443  & 17.95 & 12402 \\\\ 5024   & 182.31     & 11431 & 48.31  & 10324 & 20.47   & 6384  &\\textbf{ 17.92} & 11394 \\\\ 6024   & 214.02     & 10238 & 54.78  & 9117  & 21.76   & 5577  & 18.01 & 10119 \\\\ 7024   & 261.86     & 8785  & 62.83  & 7950  & 23.64   & 4867  & 17.93 & 8779  \\\\ 8024   & 284.88     & 8132  & 64.91  & 7355  & 25.79   & 4377  & 17.96 & 8086  \\\\ 9024   & 310.04     & 7045  & 71.91  & 6380  & 27.54   & 3787  & 17.98 & 7001  \\\\ 10024  & 337.48     & 6633  & 77.70  & 6016  & 29.54   & 3582  & 17.97 & 6583  \\\\ 11024  & 358.43     & 5722  & 81.15  & 5219  & 31.94   & 3170  & 18.02 & 5641  \\\\ 12024  & 375.95     & 5560  & 87.51  & 5072  & 33.35   & 2940  & 18.01 & 5294  \\\\ 13024  & 393.57     & 4691  & 94.74  & 4383  & OOM     & -      & 17.98 & 4621  \\\\ 14024  & 403.52     & 4905  & 96.10  & 4546  & OOM     &  -     & 18.01 & 4827  \\\\ 15024  & 431.66     & 4518  & 99.78  & 4170  & OOM     &   -    & 17.96 & 4447  \\\\ 16024  & 453.32     & 4239  & 106.99 & 3878  & OOM     &   -    & 17.98 & 4153  \\\\ \\bottomrule \\end{tabular} \\end{center} \\end{table}  \\begin{table}[] \\small \\caption{The sinusoidal, rotary, T5 bias and \\al models  trained on \\lt = \\textbf{3072} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.} \\label{tab:appendix:wt103_extra_3072} \\begin{center}  \\begin{tabular}{@{}lcccccccc@{}} \\toprule &  \\multicolumn{2}{c}{Sinusoidal} & \\multicolumn{2}{c}{Rotary} &\\multicolumn{2}{c}{T5 Bias} & \\multicolumn{2}{c}{ALiBi} \\\\ Inputs & PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$)& PPL ($\\downarrow$) & WPS ($\\uparrow$) \\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-9} 3072  & 18.67  & 13380 & 18.57 & 12548 & 18.01 & 8828 & 17.60 & 13866 \\\\ 3073  & 18.67  & 13773 & 18.57 & 12474 & 18.01 & 8483 & 17.59 & 13793 \\\\ 3082  & 18.62  & 13741 & 18.54 & 12388 & 17.95 & 8698 & 17.59 & 13778 \\\\ 3092  &\\textbf{ 18.60 } & 13742 & 18.48 & 12458 & 17.92 & 8361 & 17.55 & 13783 \\\\ 3102  & 18.65  & 13701 & 18.52 & 12365 & 17.94 & 8764 & 17.59 & 13747 \\\\ 3112  & 18.64  & 13809 & 18.51 & 12449 & 17.96 & 8665 & 17.59 & 13827 \\\\ 3122  & 18.68  & 13722 & 18.52 & 12432 & 17.98 & 8437 & 17.58 & 13795 \\\\ 3132  & 18.67  & 13825 & 18.54 & 12490 & 17.97 & 8653 & 17.58 & 13784 \\\\ 3142  & 18.69  & 13543 & 18.52 & 12230 & 17.97 & 8282 & 17.61 & 13572 \\\\ 3152  & 18.66  & 13520 & 18.56 & 12240 & 17.98 & 8608 & 17.59 & 13523 \\\\ 3162  & 18.71  & 13501 & 18.56 & 12253 & 18.04 & 8589 & 17.62 & 13598 \\\\ 3172  & 18.72  & 13563 & 18.55 & 12297 & 17.99 & 8583 & 17.59 & 13625 \\\\ 3272  & 18.87  & 13453 & 18.55 & 12148 & 17.93 & 8144 & 17.59 & 13482 \\\\ 3372  & 19.46  & 13533 & 18.50 & 12254 & 17.88 & 8442 & 17.52 & 13565 \\\\ 3472  & 20.55  & 13047 & 18.52 & 11868 & 17.95 & 7857 & 17.54 & 13107 \\\\ 3572  & 21.84  & 13128 & 18.50 & 11882 & 17.86 & 7814 & 17.50 & 13170 \\\\ 3672  & 23.04  & 13106 & 18.49 & 11859 & 17.87 & 7719 & 17.48 & 13196 \\\\ 3772  & 24.47  & 13287 & 18.54 & 11942 & 17.85 & 7579 & 17.49 & 13312 \\\\ 3872  & 25.85  & 12621 & \\textbf{18.40} & 11272 & 17.82 & 7581 & 17.41 & 12566 \\\\ 3972  & 27.21  & 12379 & 18.48 & 11151 & 17.84 & 7483 & 17.41 & 12324 \\\\ 4072  & 28.59  & 12178 & 18.59 & 11019 & 17.88 & 6974 & 17.48 & 12212 \\\\ 5072  & 45.53  & 11076 & 18.80 & 9887  & 17.76 & 6230 & 17.33 & 10938 \\\\ 6072  & 65.01  & 10114 & 19.50 & 9049  &\\textbf{ 17.68} & 5554 & 17.26 & 10133 \\\\ 7072  & 85.96  & 8647  & 20.60 & 7861  & 17.83 & 4820 & 17.22 & 8670  \\\\ 8072  & 102.74 & 7755  & 21.60 & 6991  & 18.06 & 4281 & 17.30 & 7729  \\\\ 9072  & 125.99 & 6953  & 22.14 & 6360  & 18.12 & 3823 & 17.26 & 6939  \\\\ 10072 & 133.68 & 6646  & 23.21 & 6068  & 18.37 & 3579 & 17.28 & 6597  \\\\ 11072 & 161.29 & 5663  & 24.39 & 5158  & 18.64 & 3119 & 17.26 & 5585  \\\\ 12072 & 169.55 & 5567  & 26.70 & 5111  & 18.93 & 2920 & 17.24 & 5397  \\\\ 13072 & 189.43 & 5044  & 29.33 & 4658  & 19.10 & 2735 & \\textbf{17.15} & 4809  \\\\ 14072 & 203.86 & 4915  & 32.21 & 4616  & OOM   & -    & 17.22 & 4866  \\\\ 15072 & 221.14 & 4561  & 33.47 & 4292  & OOM   & -    & 17.23 & 4491  \\\\ 16072 & 231.29 & 4382  & 34.51 & 4099  & OOM   & -    & 17.22 & 4312  \\\\  \\bottomrule \\end{tabular} \\end{center} \\end{table}     \\clearpage "
            },
            {
                "section_name": "\\al Results on WikiText-103_6",
                "paragraphs": "\\begin{figure}[h] \\begin{center} \\includegraphics[width=.5\\textwidth]{figures/wt103-speed-vs-ppl.pdf}  \\end{center} \\caption{The training speed and validation perplexity (with \\li = 3072) for \\al models and the sinusoidal model trained with \\lt = 3072. All our models trained on 512 or more tokens achieve better perplexity than the sinusoidal model even though all of them (except the \\lt = 3072) require less time and memory to train.  } \\label{fig:wt103_train_speed_vs_ppl} \\end{figure}  Figure~\\ref{fig:wt103_train_speed_vs_ppl} depicts a cross section of Figure~\\ref{fig:wt103_extra_lsb_all}, showing our models with different train lengths and the sinusoidal baseline, all evaluated on \\li = 3072 tokens. We observe that all our models with 512 $\\leq$  \\lt $<$ 3072 are faster to train than the sinusoidal model with \\lt = 3072, but they all achieve greater perplexity scores on the validation set. Our model with \\lt = 3072 trains just as fast as the sinusoidal one but bests its score by more than one perplexity point; (the standard deviation for the the sinusoidal model with \\lt = 3072 is 0.24).   Table~\\ref{tab:LSB_wt103} shows the perplexity values obtained when 8 different \\al models, trained on $L$ values between 64 and 3072, extrapolating to \\li values longer than the ones they were trained on. In addition, we present results for the sinusoidal, rotary and T5 bias models, with \\li = \\lt. \\begin{table}[h] \\caption{Perplexity when ALiBi extrapolates on the WikiText-103 development set. $^*$For results we present for the sinusoidal, rotary and T5 bias models, \\lt = \\li (so we do not test the extrapolation abilities of those baselines here). } \\label{tab:LSB_wt103} \\begin{center} \\begin{tabular}{@{}rcccccccc@{}} \\toprule \\multicolumn{1}{c}{ALiBi } &  \\multicolumn{8}{c}{Evaluation Length} \\\\ Train Length & 64 & 128 & 256 & 512 & 1024 & 1536 & 2048 & 3072 \\\\\\cmidrule(r){1-1} \\cmidrule(l){2-9} 64   &28.46&24.70&22.88&22.09&21.73&21.63&21.59&21.53\\\\ 128  &-&23.98&21.70&20.67&20.36&20.29&20.31&20.28\\\\ 256  &-&-&\\textbf{21.29}&19.89&19.29&19.13&19.10&19.03\\\\ 512  &-&-&-&19.73&18.81&18.50&18.48    &18.40\\\\ 1024 &-&-&-&-&\\textbf{18.66}&18.20&18.05        &17.96\\\\ 1536 &-  &  -&-  &  -&-  &\\textbf{18.12 }& 17.90&17.72  \\\\ 2048 & - & - & - &  -& - & -&\\textbf{17.91 }    &17.64 \\\\ 3072 &  -&-  &  -&  -&  -&- &-         &\\textbf{17.60} \\\\  \\midrule \\midrule \\multicolumn{1}{c}{Sinusoidal$^*$ }&\\textbf{ 28.03}&\\textbf{ 23.81}& 21.45& 20.05&19.34&19.05&18.87&18.67\\\\ \\multicolumn{1}{c}{Rotary$^*$ } &- &-&-&20.07&19.33&-&-&18.57\\\\ \\multicolumn{1}{c}{T5 Bias$^*$ }&- &-&-&\\textbf{19.65}&18.80&-&-&18.01\\\\  \\bottomrule \\end{tabular} \\end{center} \\end{table}      Table~\\ref{tab:wt103_test} compares \\al to the sinusoidal, rotary and T5 bias baselines on the test set of WikiText-103, and Table~\\ref{tab:wt103_test_sota} compares \\al to the current state of the art models on that test set.   \\definecolor{mg}{gray}{0.5}   \\begin{table}[t]   \\caption{Test perplexity and runtime on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods. } \\label{tab:wt103_test} \\begin{center} \\centering   \\begin{tabular}{@{}lccccc@{}} \\toprule \\multirow{2}{*}[-2pt]{Model}  &\\multirow{2}{*}[-2pt]{Param. $\\downarrow$}&Train&\\multicolumn{3}{c}{Inference  } \\\\  \\cmidrule(lr){3-3} \\cmidrule(lr){4-6}  & &Speed$\\uparrow$ & Speed $\\uparrow$& Valid $\\downarrow$ & Test $\\downarrow$ \\\\ \\midrule   Sinusoidal, $L$ = 3072 &\\textbf{247M}  & 15.3k &\\textbf{13.6k}&18.67&19.38 \\\\ Rotary, $L$ = 3072     &\\textbf{247M} &\\textbf{11.5k} &12.2k &18.57 & 19.28 \\\\ T5 Bias, $L$ = 3072    &\\textbf{247M} &4.3k &7.3k&18.01&18.73 \\\\  \\midrule &&&&& \\\\ \\addlinespace[-0.75em]  \\multirow{2}{*}[3pt]{\\rotatebox[origin=c]{90}{ALiBi}} $L$ = 512, $L_{valid}$ = 3072 &\\textbf{247M}&28.3k&\\textbf{13.6k}&18.40& 19.08\\\\  ~~~~$L$ = 3072, $L_{valid}$ = 3072 &\\textbf{247M}&15.5k&\\textbf{13.6k}&\\textbf{17.60}&\\textbf{18.30} \\\\  \\addlinespace[-0.75em] &&&&& \\\\ \\bottomrule   \\end{tabular}     \\end{center} \\end{table}           \\begin{table}[!htbp]   \\caption{Valid and test perplexity scores on WikiText-103 for two of our ALiBi  models and models that use the sinusoidal, rotary and T5 bias methods with sliding window evaluation (\\S\\ref{sec:analysis} and S=512 following~\\citep{baevski,khandelwal20generalization, shortformer}). The sinusoidal model presents our results from training and inference with the model of Baevski \\& Auli. }   \\label{tab:wt103_test_sota} \\begin{center} \\centering   \\begin{tabular}{@{}lccc@{}} \\toprule  {Model}  &{Param. $\\downarrow$}& Valid $\\downarrow$ & Test $\\downarrow$ \\\\   \\midrule Adaptive Inputs~\\citep{baevski} &\\textbf{247M}   &17.97 &18.70\\\\ {Transformer-XL~\\citep{transformer-xl}} &{257M }& - & 18.3\\\\ {Shortformer~\\citep{shortformer}}&\\textbf{247M}& 17.47&18.15  \\\\ {Sandwich Transformer~\\citep{sandwich}}&\\textbf{247M}&- &17.96 \\\\ Staged Training~\\citep{shortformer}&\\textbf{247M}&- &17.56 \\\\ {Compressive Transformer~\\citep{Rae2020Compressive}}&329M         &-&17.1\\\\ Routing Transformer~\\citep{roy2020efficient}& - &  - & 15.8\\\\ {kNN-LM~\\citep{khandelwal20generalization}     }&\\textbf{247M}&  15.81  & \\textbf{15.79}\\\\ \\midrule  Sinusoidal, $L$ = 3072 &\\textbf{247M}  & 17.95&18.67 \\\\ Rotary, $L$ = 3072     &\\textbf{247M} & 17.98&18.72\\\\ T5 Bias, $L$ = 3072    &\\textbf{247M} &17.37& 18.12\\\\  \\midrule &&& \\\\ \\addlinespace[-0.75em]  \\multirow{2}{*}[3pt]{\\rotatebox[origin=c]{90}{ALiBi}} $L$ = 512, $L_{valid}$ = 3072 &\\textbf{247M}&18.30&19.01\\\\  ~~~~$L$ = 3072, $L_{valid}$ = 3072 &\\textbf{247M}&16.97&17.66 \\\\  \\addlinespace[-0.75em] &&& \\\\ \\bottomrule   \\end{tabular}     \\end{center} \\end{table}    "
            },
            {
                "section_name": "Results on the Toronto Book Corpus_7",
                "paragraphs": "\\label{subsec:tbc}  To ensure that our results are not specific to the WikiText-103 corpus, we next apply our model and the baselines to a different domain while using a similar model architecture and the same ALiBi slopes as those used in the previous subsection.  We emphasize that our set of slopes was chosen by running experiments on the WikiText-103 corpus, and here we apply that set of slopes to a model trained on a very different text domain. Throughout the entire process of developing this method, we ran only one set of experiments on this domain using the previously selected set of slopes.   Specifically, we use the Toronto BooksCorpus~\\citep{zhu2015aligning}, which has been used to train BERT~\\citep{bert} (in conjuction with the English Wikipedia). The corpus is about 700M tokens (2.9 GB).  We use the same train/validation/test split as~\\citet{khandelwal20generalization} and their tokenization, which uses BERT's vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103's, we replace the adaptive word embedding and softmax of \\citet{baevski} with a tied word embedding and softmax matrix \\citep{tying,inan2017}.  \\begin{figure}[h] \\begin{center} \\includegraphics[width=.75\\textwidth]{figures/tbc-extra-lsb-all.pdf} \\end{center} \\caption{ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus. Our models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even when trained on much shorter sequences. } \\label{fig:tbc_extra_lsb_all} \\end{figure}       Our results in Figure~\\ref{fig:tbc_extra_lsb_all} (and Table~\\ref{tab:books_lsb})  replicate our success on the WikiText-103 dataset. Our model surpasses the sinusoidal baseline when trained on the same amount of input tokens ($L$) and, in addition, our model is able to extrapolate to longer sequences at inference. This occurs even though our set of slopes was \\emph{not} tuned on this dataset. This result establishes the generality of ALiBi and the particular set of slopes we found and suggests that they may be used on different text domains without further hyperparameter tuning.  Tables~\\ref{tab:books_test} and \\ref{tab:books_test_sota} present the perplexities for our \\al models, the baselines, and the current state of the art on the Toronto BookCorpus validation and test sets. Our results here mirror our results on WikiText-103: we improve over the sinusoidal baseline even when AliBi is trained on fewer tokens.    \\begin{table}[!htbp] \\caption{\\al   models extrapolating on the Toronto BookCorpus development set. $^*$For the results of the sinusoidal models, \\lt = \\li (so we do not test the extrapolation abilities of those models here).} \\label{tab:books_lsb} \\begin{center} \\begin{tabular}{@{}rccc@{}} \\toprule &  \\multicolumn{3}{c}{Evaluation Length} \\\\ Train Length               & 512 & 1024  & 3072 \\\\\\cmidrule(r){1-1} \\cmidrule(l){2-4}  512 & 14.29 &13.64 &13.55  \\\\ 1024 & -&13.86 & 13.52 \\\\ 3072 & -& -& 13.15\\\\ \\midrule \\midrule Sinusoidal$^*$ &14.80 &14.73 & 14.46 \\\\ \\bottomrule \\end{tabular} \\end{center} \\end{table}   \\begin{table}[!htbp] \\caption{Validation and test perplexities on the Toronto Book Corpus dataset. } \\label{tab:books_test} \\begin{center} \\centering  \\begin{tabular}{@{}lccc@{}} \\toprule {Model}  &{Param. $\\downarrow$}& Valid $\\downarrow$ & Test $\\downarrow$ \\\\  \\midrule Sinusoidal, L = 3072 &\\textbf{247M} & 14.46&11.67  \\\\  \\midrule   &&& \\\\ \\addlinespace[-0.75em]  \\multirow{2}{*}[3pt]{\\rotatebox[origin=c]{90}{ALiBi}} $L_{train}$ = 512, $L_{valid}$ = 3072 &\\textbf{247M}&13.55 & 10.98 \\\\ ~~~~$L_{train}$ = 3072, $L_{valid}$ = 3072 &\\textbf{247M}&\\textbf{13.15} &\\textbf{10.73} \\\\  \\addlinespace[-0.75em] &&& \\\\ \\bottomrule   \\end{tabular}  \\end{center} \\end{table}       \\begin{table}[t] \\caption{Validation and test perplexities on the Toronto Book Corpus dataset with a sliding window (\\S\\ref{sec:analysis}). Following~\\citep{baevski,khandelwal20generalization,sandwich,shortformer}, we set the sliding window stride $S$=512. } \\label{tab:books_test_sota} \\begin{center} \\centering  \\begin{tabular}{@{}lccc@{}} \\toprule {Model}  &{Param. $\\downarrow$}& Valid $\\downarrow$ & Test $\\downarrow$ \\\\ \\midrule kNN-LM~\\citep{khandelwal20generalization} &\\textbf{247M} &14.20&10.89 \\\\ Shortformer~\\citep{shortformer} &\\textbf{247M} &13.40&10.88 \\\\ Sandwich~\\citep{sandwich} &\\textbf{247M} &-&10.83 \\\\ Staged Training~\\citep{shortformer}&\\textbf{247M}  &12.80&10.48 \\\\   \\midrule Sinusoidal, L = 3072 &\\textbf{247M} & 14.06 & 11.40   \\\\  \\midrule   &&& \\\\ \\addlinespace[-0.75em]  \\multirow{2}{*}[3pt]{\\rotatebox[origin=c]{90}{ALiBi}} $L$ = 512, $L_{valid}$ = 3072 &\\textbf{247M}&13.76 & 11.11 \\\\ ~~~~$L$ = 3072, $L_{valid}$ = 3072 &\\textbf{247M}&12.70 & 10.40 \\\\  \\addlinespace[-0.75em] &&& \\\\ \\bottomrule   \\end{tabular}  \\end{center} \\end{table}     \\begin{table}[!htbp]    \\caption{Perplexity, memory, and train time on the CC100+RoBERTa corpus for our \\al models and the sinusoidal baseline. We run our \\lt = 512 (1024) model and the sinusoidal model with \\lt = 1024 (2048) for the same amount of time. We show that our models achieve strong results even though they use 6--11\\% less memory. } \\label{tab:ccr} \\begin{center} \\centering   \\begin{tabular}{@{}lccccc@{}} \\toprule &  \\multicolumn{3}{c}{Training}&  \\multicolumn{2}{c}{Valid PPL $\\downarrow$}\\\\ \\cmidrule(l){2-4} \\cmidrule(l){5-6} &   { Memory $\\downarrow$} & {Updates} &{ Hours $\\downarrow$} &\\li = 1024  & \\li = 2048   \\\\ \\midrule Sinusoidal, $L_{train}$ = 1024  &26.2 GB &46.7k &5.5k &  \\textbf{9.24}  & -   \\\\ ALiBi, $L_{train}$ = 512        &\\textbf{24.6 GB}& 50.0k&5.5k&  9.30  & -   \\\\  \\midrule Sinusoidal, $L_{train}$ = 2048  &29.3 GB &44.2k &  5.9k&-&   9.01\\\\ ALiBi, $L_{train}$ = 1024        &\\textbf{26.2 GB}&50.0k & 5.9k &-&  \\textbf{8.92}  \\\\ \\bottomrule \\end{tabular}   \\end{center} \\end{table}                       "
            },
            {
                "section_name": "Results on the CC100+RoBERTa Corpus_8",
                "paragraphs": "Table~\\ref{tab:ccr} compares our 1.3 billion parameter \\al models when extrapolating to two times the number of tokens that they were trained on. We use the sinusoidal model as our baseline, and train it for the same amount of time as we train the \\al model that we compare it to (and so since our \\al models run faster in this setting, the sinusoidal models complete less updates).  Table~\\ref{tab:ccr_all_50k} compares our 1.3 billion parameter \\al models to the sinusoidal baselines, with and without extrapolation, with all models completing 50,000 updates. \\begin{table}[!htbp]    \\caption{Perplexity, train time and memory use of the sinusoidal and ALiBi models on the CC100+RoBERTa corpus when all models are trained with 50k updates.} \\label{tab:ccr_all_50k} \\begin{center} \\centering \\small \\setlength{\\tabcolsep}{2pt}   \\begin{tabular}{@{}lcccccc@{}} \\toprule &  \\multicolumn{3}{c}{Training}&  \\multicolumn{3}{c}{Valid PPL $\\downarrow$}\\\\ \\cmidrule(l){2-4} \\cmidrule(l){5-7} &   { Memory $\\downarrow$} & {Updates} &{ Hours $\\downarrow$} &\\li = 512  &\\li = 1024  & \\li = 2048   \\\\ \\midrule Sinusoidal, $L_{train}$ = 512  &24.6 GB & 50.0k & 5.5k &9.71&37.05 &105.42   \\\\  ALiBi, $L_{train}$ = 512        &24.6 GB& 50.0k& 5.5k & 9.79 &9.30&9.54   \\\\    \\midrule Sinusoidal, $L_{train}$ = 1024  &  26.2 GB &50.0k & 5.9k&-&9.15  &48.85\\\\ ALiBi, $L_{train}$ = 1024        &  26.2 GB &50.0k & 5.9k &-&9.16&8.92  \\\\   \\midrule Sinusoidal, $L_{train}$ = 2048  & 29.3 GB &50.0k &6.7k  & -&-&8.83\\\\ ALiBi, $L_{train}$ = 2048        & 29.4 GB&50.0k & 6.7k  & -&-&8.84  \\\\ \\bottomrule \\end{tabular}   \\end{center} \\end{table}          "
            },
            {
                "section_name": "Analysis_8",
                "paragraphs": "\\label{sec:analysis}     In this section we investigate why \\al works so effectively. We find that ALiBi's decrease in perplexity when given longer sequences is largely explained by its improved avoidance of the early token curse.  We hypothesize that future work building on \\al might achieve further gains by more efficiently  exploiting longer histories.   "
            },
            {
                "section_name": "Defining sliding window evaluation and the early token curse_9",
                "paragraphs": "  \\begin{figure}[h] \\begin{center} \\includegraphics[width=.5\\textwidth]{figures/bigcat.pdf} \\end{center} \\caption{Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom; red) on a sequence of 8 words using a  model with \\li = 4. Nonoverlapping evaluation is much faster since it requires just two inference passes (as opposed to the five passes required by the siding window approach). But the sliding window approach provides more context for each prediction.} \\label{fig:bigcat} \\end{figure}   \\paragraph{Sliding Window Inference} As mentioned in Section~\\ref{sec:act2}, nonoverlapping inference is commonly used to evaluate sequences longer than $L$ (the number of tokens in each training subsequence). An alternative is to use a sliding window during evaluation~\\citep{baevski}.  A stride $S$ is picked between 1 and $L-1$, and the window is advanced by $S$ tokens after each forward pass.\\footnote{Nonoverlapping inference can be viewed as sliding window inference with stride $L$.} This means that $L-S$ tokens from the previous subsequence are re-encoded, and only $S$ new tokens are output. The advantage is that all outputs in each subsequence after the first have at least $L-S$ previous tokens to condition on. However, since tokens must be re-encoded multiple times, this approach is much slower than the nonoverlapping one. When $S=1$, we output one token every inference pass, each using the maximal context window that the model can handle; however, this is the slowest approach. Figure~\\ref{fig:bigcat} is a visualization of the nonoverlapping and sliding window evaluation approaches.  We use sliding window inference as a tool to analyze our models, but we note that it is normally prohibitively slow in practice~\\citep{shortformer}.  \\paragraph{Early Token Curse} Splitting an evaluation set into subsequences means that predictions occuring early in each subsequence cannot access many previous context tokens (appearing at the end of the previous subsequence). The result, referred to as the \\emph{early token curse}~\\citep{shortformer}, increases (i.e., degrades) perplexity scores. A workaround is to evaluate the model using a sliding window, giving each prediction more context. This solution is slow since it requires many more forward passes of the model.   "
            },
            {
                "section_name": "Extrapolation reduces the early token curse_10",
                "paragraphs": "     We presented results showing that our ALiBi method (and, to a lesser extent, the T5 bias) allows LMs to extrapolate during inference. Two reasons could explain why these methods enable LMs to achieve better perplexity given longer input subsequences: \\begin{enumerate} \\item Performance improves because the models can use longer contexts to make more accurate predictions. For example, the average article length in the WikiText-103 corpus is about 3600 tokens; therefore, if a model trained on $L=512$ tokens extrapolates to \\li = 3072 tokens during inference and achieves better results, that might be because it can spot patterns occurring across more than 512 tokens. \\item Performance improves because longer input sequences mean the early token curse is reduced. For example, during nonoverlapping evaluation on sequences of length \\li $=$ 1000, 10\\% of predictions have 100 tokens of context or less. If we rerun nonoverlapping evaluation on that model with \\li $=$ 2000 tokens, now only 5\\% of predictions have 100 tokens of context or less. So, by simply being able to handle longer sequences, a model can substantially reduce the early token curse and improve performance.\\footnote{100 tokens is an arbitrary small number used here to represent a short history context, i.e., one in which making predictions for the next output token would be harder.} \\end{enumerate}  To better understand what might be occurring, we re-evaluate the development set of WikiText-103 with our models and the sinusoidal baseline with $L = 512, 1024, 3072$. However, this time we use sliding window evaluation with a stride of $S=1$, meaning that we move the sliding window just one token after every inference pass, giving each prediction the maximum number of context tokens that the model can use.  \\begin{figure}[h] \\begin{center} \\includegraphics[width=.75\\textwidth]{figures/wt103-extra-lsb-sin-cw.pdf}  \\end{center} \\caption{ALiBi models evaluated on different input lengths on WikiText-103 with sliding window evaluation (with stride $S=1$). Unlike results shown in Figure~\\ref{fig:wt103_extra_lsb_all}, where performance improves in each of our models as we increase the validation sequence length, here performance stays relatively flat as we increase \\li. This might mean that \\al increases performance when \\li $>$ \\lt not because it uses longer contexts, but because fewer tokens suffer from the early token curse. Note that as in \\S\\ref{sec:act2}, the perplexity of the sinusoidal model explodes when \\li $>$ \\lt even when using sliding window evaluation.} \\label{fig:wt103_extra_lsb_cw} \\end{figure}   The results are shown in Figure~\\ref{fig:wt103_extra_lsb_cw} and in the corresponding Tables~\\ref{tab:sin_extrapolating_cw} (sinusoidal) and \\ref{tab:lab_extrapolating_cw} (ALiBi).  Unsurprisingly, for the sinusoidal model, as in \\S\\ref{sec:act2}, increasing \\li causes an explosion in perplexity even when using sliding window evaluation. Our ALiBi models cannot improve perplexity when looking at longer sequences in this setting, but they keep perplexity flat when \\li increases.  This leads us to believe that our perplexity improvement when increasing \\li and using nonoverlapping evaluation is caused by explanation 2, not explanation 1. Because sliding window evaluation provides long context windows for \\emph{every} prediction made, it curtails the early token curse. In this setting, \\al's performance remains flat when \\li increases, leading us to hypothesize that the gains seen while increasing \\li in \\S\\ref{sec:results} were the result of larger \\li values mitigating the early token curse.  Our \\al results mirror what occurs in the model using the T5 bias: when using sliding window evaluation, perplexity remains relatively flat when evaluating longer sequences (see Table~\\ref{tab:t5_extrapolating_cw}).      Our analysis reveals that when $\\li > \\lt$, \\al might not be using contexts longer than the ones it was trained on. This highlights a research direction that could be pursued in future work.  These findings do not lessen the value of \\al.  When \\li = \\lt, \\al achieves either superior or similar results to the sinusoidal method and other alternatives even though it is simpler and requires no learned parameters. When evaluating $\\li > \\lt$ tokens, even if \\al does not attend to more than \\lt tokens, it yields better results than the other alternatives that can be used in this case, i.e., standard nonoverlapping inference (which is cheap, but does not perform as well) and the more accurate sliding window approach (which is very slow).             \\begin{table}[t] \\caption{Perplexities of the \\textbf{sinusoidal} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset.} \\label{tab:sin_extrapolating_cw} \\begin{center} \\begin{tabular}{@{}rccccc@{}} \\toprule &  \\multicolumn{5}{c}{Evaluation Length ($S=1$)} \\\\ Train Length               & 512 & 1024 & 1536 & 2048 & 3072 \\\\\\cmidrule(r){1-1} \\cmidrule(l){2-6}  512& 18.35& 204.42& 264.74& 306.19& 360.12\\\\ 1024 &-& 18.05& 206.55& 302.6& 393.71\\\\ 3072 & -&- & -& -& 18.03 \\\\ \\bottomrule \\end{tabular} \\end{center} \\end{table}    \\begin{table}[t] \\caption{Perplexities of the \\textbf{T5 bias} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset. } \\label{tab:t5_extrapolating_cw} \\begin{center}  \\begin{tabular}{@{}rccccc@{}} \\toprule &  \\multicolumn{5}{c}{Evaluation Length ($S=1$)} \\\\ Train Length               & 512 & 1024 & 1536 & 2048 & 3072 \\\\\\cmidrule(r){1-1} \\cmidrule(l){2-6}  512&  17.92 & 18.51 & 20.36 & 22.62 & 30.77 \\\\ 1024 &-& 17.65 & 17.87 & 18.51 & 20.66  \\\\ 3072 & -&- & -& -&  17.41\\\\ \\bottomrule \\end{tabular}  \\end{center} \\end{table}    \\begin{table}[t] \\caption{Perplexities of the \\textbf{ALiBi} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset.} \\label{tab:lab_extrapolating_cw} \\begin{center} \\begin{tabular}{@{}rccccc@{}} \\toprule &  \\multicolumn{5}{c}{Evaluation Length ($S=1$)} \\\\ Train Length               & 512 & 1024 & 1536 & 2048 & 3072 \\\\\\cmidrule(r){1-1} \\cmidrule(l){2-6}  512& 17.98 & 17.92 & 18.2 & 18.28 & 18.3 \\\\ 1024 &-& 17.46 & 17.47 & 17.62 & 17.92  \\\\ 3072 & -&- & -& -& 16.96\\\\ \\bottomrule \\end{tabular} \\end{center} \\end{table}       \\end{document}   "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2108.12409/wt103_extra_512.png",
                "caption": "Extrapolation: as the (validation-set's) input sequence gets longer ($x$-axis), current position methods (sinusoidal, rotary, and T5) show degraded perplexity ($y$-axis, lower is better), but our method (\\S\\ref{sec:ourmethod}) does not.  Models were trained on WikiText-103 with sequences of $L=$ 512 (left) or $L=$ 1,024 (right) tokens.  T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables~\\ref{tab:appendix:wt103_extra_512} and~\\ref{tab:appendix:wt103_extra_1024} in the appendix.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_extra_1024.png",
                "caption": "Extrapolation: as the (validation-set's) input sequence gets longer ($x$-axis), current position methods (sinusoidal, rotary, and T5) show degraded perplexity ($y$-axis, lower is better), but our method (\\S\\ref{sec:ourmethod}) does not.  Models were trained on WikiText-103 with sequences of $L=$ 512 (left) or $L=$ 1,024 (right) tokens.  T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables~\\ref{tab:appendix:wt103_extra_512} and~\\ref{tab:appendix:wt103_extra_1024} in the appendix.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_Training-Speed.png",
                "caption": "A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1\\% during training and 3\\% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table~\\ref{tab:baselines_speed_mem} in the appendix for exact numbers. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_Inference-Speed.png",
                "caption": "A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1\\% during training and 3\\% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table~\\ref{tab:baselines_speed_mem} in the appendix for exact numbers. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_Training-Memory-Usage.png",
                "caption": "A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1\\% during training and 3\\% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table~\\ref{tab:baselines_speed_mem} in the appendix for exact numbers. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/legend.png",
                "caption": "A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1\\% during training and 3\\% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table~\\ref{tab:baselines_speed_mem} in the appendix for exact numbers. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/fig1.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_extra_lsb_all.png",
                "caption": "\\al models trained and evaluated on varying sequence lengths on the WikiText-103 validation set and the sinusoidal baseline (not evaluated on longer sequences). All of our models outperform the sinusoidal ones even when trained on fewer tokens. Appendix Table~\\ref{tab:LSB_wt103} has exact perplexities, more \\al models (trained on fewer tokens), and results for rotary and T5 bias models. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/ccr-train-512.png",
                "caption": " On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens. The \\al models obtain strong results even though they use 6\\%-11\\% less memory since they train on shorter sequences. Appendix Table \\ref{tab:ccr} shows memory use and end-of-training perplexities.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/ccr-train-1k.png",
                "caption": " On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens. The \\al models obtain strong results even though they use 6\\%-11\\% less memory since they train on shorter sequences. Appendix Table \\ref{tab:ccr} shows memory use and end-of-training perplexities.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/ccr_extra_512.png",
                "caption": "The ALiBi and sinusoidal models (with both $L$ = 512 and 1024) trained for 50k updates (1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. \\al achieves the best results at around $2L$ but maintains strong performance even up to 10000 tokens in these experiments.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/ccr_extra_1024.png",
                "caption": "The ALiBi and sinusoidal models (with both $L$ = 512 and 1024) trained for 50k updates (1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. \\al achieves the best results at around $2L$ but maintains strong performance even up to 10000 tokens in these experiments.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103_train_speeds_all.png",
                "caption": " Training speed of our model and the sinusoidal baseline trained on different amounts of input subsequence tokens $L$.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103-speed-vs-ppl.png",
                "caption": "The training speed and validation perplexity (with \\li = 3072) for \\al models and the sinusoidal model trained with \\lt = 3072. All our models trained on 512 or more tokens achieve better perplexity than the sinusoidal model even though all of them (except the \\lt = 3072) require less time and memory to train.  ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/tbc-extra-lsb-all.png",
                "caption": "ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus. Our models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even when trained on much shorter sequences. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/bigcat.png",
                "caption": "Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom; red) on a sequence of 8 words using a  model with \\li = 4. Nonoverlapping evaluation is much faster since it requires just two inference passes (as opposed to the five passes required by the siding window approach). But the sliding window approach provides more context for each prediction.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/wt103-extra-lsb-sin-cw.png",
                "caption": "ALiBi models evaluated on different input lengths on WikiText-103 with sliding window evaluation (with stride $S=1$). Unlike results shown in Figure~\\ref{fig:wt103_extra_lsb_all}, where performance improves in each of our models as we increase the validation sequence length, here performance stays relatively flat as we increase \\li. This might mean that \\al increases performance when \\li $>$ \\lt not because it uses longer contexts, but because fewer tokens suffer from the early token curse. Note that as in \\S\\ref{sec:act2}, the perplexity of the sinusoidal model explodes when \\li $>$ \\lt even when using sliding window evaluation.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table1.tex",
                "caption": " The speed (during training and evaluation, in words per second) and memory usage (during training) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline  on WikiText-103. Training and inference are batched, and speeds are shown for one V100 GPU. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table2.tex",
                "caption": "The sinusoidal, rotary, T5 bias and \\al models trained on \\lt = \\textbf{512} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table3.tex",
                "caption": "The sinusoidal, rotary, T5 bias and \\al models  trained on \\lt = \\textbf{1024} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table4.tex",
                "caption": "The sinusoidal, rotary, T5 bias and \\al models  trained on \\lt = \\textbf{3072} on WikiText-103 and evaluated with different values of \\li on the validation set. \\textbf{Bold} shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table5.tex",
                "caption": "Perplexity when ALiBi extrapolates on the WikiText-103 development set. $^*$For results we present for the sinusoidal, rotary and T5 bias models, \\lt = \\li (so we do not test the extrapolation abilities of those baselines here). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table6.tex",
                "caption": "Test perplexity and runtime on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table7.tex",
                "caption": "Valid and test perplexity scores on WikiText-103 for two of our ALiBi  models and models that use the sinusoidal, rotary and T5 bias methods with sliding window evaluation (\\S\\ref{sec:analysis} and S=512 following~\\citep{baevski,khandelwal20generalization, shortformer}). The sinusoidal model presents our results from training and inference with the model of Baevski \\& Auli. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table8.tex",
                "caption": "\\al   models extrapolating on the Toronto BookCorpus development set. $^*$For the results of the sinusoidal models, \\lt = \\li (so we do not test the extrapolation abilities of those models here).",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table9.tex",
                "caption": "Validation and test perplexities on the Toronto Book Corpus dataset. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table10.tex",
                "caption": "Validation and test perplexities on the Toronto Book Corpus dataset with a sliding window (\\S\\ref{sec:analysis}). Following~\\citep{baevski,khandelwal20generalization,sandwich,shortformer}, we set the sliding window stride $S$=512. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table11.tex",
                "caption": "Perplexity, memory, and train time on the CC100+RoBERTa corpus for our \\al models and the sinusoidal baseline. We run our \\lt = 512 (1024) model and the sinusoidal model with \\lt = 1024 (2048) for the same amount of time. We show that our models achieve strong results even though they use 6--11\\% less memory. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table12.tex",
                "caption": "Perplexity, train time and memory use of the sinusoidal and ALiBi models on the CC100+RoBERTa corpus when all models are trained with 50k updates.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table13.tex",
                "caption": "Perplexities of the \\textbf{sinusoidal} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table14.tex",
                "caption": "Perplexities of the \\textbf{T5 bias} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2108.12409/table15.tex",
                "caption": "Perplexities of the \\textbf{ALiBi} models evaluated with sliding window evaluation with stride $S=1$ on the WikiText-103 validation dataset.",
                "description": ""
            }
        ]
    },
    "1910.10683": {
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
        "tldr": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": " Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to ``understand'' text. This knowledge can range from low-level (e.g.\\ the spelling or meaning of words) to high-level (e.g.\\ that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors \\citep{mikolov2013distributed,mikolov2013efficient,pennington2014glove} to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space \\citep{mikolov2013distributed}.  Recently, it has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks. In applications of transfer learning to computer vision \\citep{oquab2014learning,jia2014caffe,huh2016makes,yosinski2014transferable}, pre-training is typically done via supervised learning on a large labeled data set like ImageNet \\citep{russakovsky2015imagenet,deng2009imagenet}. In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data. This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks \\citep{devlin2018bert,yang2019xlnet,dong2019unified,liu2019roberta,lan2019albert}. Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet---for example, the Common Crawl project\\footnote{\\url{http://commoncrawl.org}} produces about 20TB of text data extracted from web pages each month. This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e.\\ it is often possible to achieve better performance simply by training a larger model on a larger data set \\citep{hestness2017deep,shazeer2017outrageously,jozefowicz2016exploring,mahajan2018exploring,radford2019language,shazeer2018mesh,huang2018gpipe,keskar2019ctrl}.  This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives \\citep{howard2018universal,devlin2018bert,yang2019xlnet,dong2019unified}, unlabeled data sets \\citep{yang2019xlnet,liu2019roberta,zellers2019defending}, benchmarks \\citep{wang2019superglue,wang2018glue,conneau2018senteval}, fine-tuning methods \\citep{howard2018universal,houlsby2019parameter,peters2019tune}, and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field.  \\begin{figure}[t] \\centering \\includegraphics[width=\\textwidth]{text_to_text.pdf} \\caption{ A diagram of our text-to-text framework. Every task we consider---including translation, question answering, and classification---is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc.\\ across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. ``T5'' refers to our model, which we dub the ``\\textbf{T}ext-\\textbf{t}o-\\textbf{T}ext \\textbf{T}ransfer \\textbf{T}ransformer''. } \\label{fig:text_to_text} \\end{figure}  The basic idea underlying our work is to treat every text processing problem as a ``text-to-text'' problem, i.e.\\ taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering  \\citep{mccann2018natural}, language modeling \\citep{radford2019language}, or span extraction \\cite{keskar2019unifying} tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document summarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.  We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.   We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to $11$ billion parameters) to obtain state-of-the-art results in many of the tasks we consider. In order to perform experiments at this scale, we introduce the ``Colossal Clean Crawled Corpus'' (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.\\footnoteref{fn:oss}  The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In \\cref{sec:experiments}, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (\\cref{sec:together}), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in \\cref{sec:conclusion}.  "
            },
            {
                "section_name": "Setup_2",
                "paragraphs": "\\label{sec:setup}  Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our ``Colossal Clean Crawled Corpus'' (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the ``\\textbf{T}ext-\\textbf{t}o-\\textbf{T}ext \\textbf{T}ransfer \\textbf{T}ransformer'' (T5).  "
            },
            {
                "section_name": "Model_1",
                "paragraphs": "\\label{sec:model}  Early results on transfer learning for NLP leveraged recurrent neural networks \\citep{peters2018deep,howard2018universal}, but it has recently become more common to use models based on the ``Transformer'' architecture \\citep{vaswani2017attention}. The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings \\citep{radford2018improving,devlin2018bert,mccann2018natural,yu2018qanet}. Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in \\cref{sec:architectures}, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper \\citep{vaswani2017attention} or follow-up tutorials\\footnote{\\url{http://nlp.seas.harvard.edu/2018/04/03/attention.html}}\\textsuperscript{,}\\footnote{\\url{http://jalammar.github.io/illustrated-transformer/}} for a more detailed introduction.  The primary building block of the Transformer is self-attention \\citep{cheng2016long}. Self-attention is a variant of attention \\citep{graves2013generating,bahdanau2014neural} that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence \\citep{sutskever2014sequence,kalchbrenner2014convolutional} tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling \\citep{radford2018improving,al2019character} or classification and span prediction tasks \\citep{devlin2018bert,yang2019xlnet}. We empirically explore these architectural variants in \\cref{sec:architectures}.  Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output. Dropout \\citep{srivastava2014dropout} is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal self-attention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent ``heads'' whose outputs are concatenated before being further processed.  Since self-attention is order-independent (i.e.\\ it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings \\citep{shaw2018self,huang2018music}. Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the ``key'' and ``query'' being compared in the self-attention mechanism. We use a simplified form of position embeddings where each ``embedding'' is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use $32$ embeddings for all of our models with ranges that increase in size logarithmically up to an offset of $128$ beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond $128$ tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by \\citet{vaswani2017attention} with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.  As part of our study, we experiment with the scalability of these models, i.e.\\ how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on ``slices'' of Cloud TPU Pods.\\footnote{\\url{https://cloud.google.com/tpu/}} TPU pods are are multi-rack ML supercomputers that contain $1{,}024$ TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library \\citep{shazeer2018mesh} for ease of implementation of both model parallelism and data parallelism \\citep{krizhevsky2014one}.  "
            },
            {
                "section_name": "The Colossal Clean Crawled Corpus_2",
                "paragraphs": "\\label{sec:dataset}  Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common Crawl has previously been used as a source of text data for NLP, for example to train an n-gram language model \\citep{buck2014n}, as training data for commonsense reasoning \\citep{trinh2018simple}, for mining parallel texts for machine translation \\citep{smith2013dirt}, as a pre-training data set \\citep{grave2018learning,zellers2019defending,liu2019roberta}, and even simply as a giant text corpus for testing optimizers \\citep{anil2019memory}.  Common Crawl is a publicly-available web archive that provides ``web extracted text'' by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl's web extracted text: \\begin{itemize} \\item We only retained lines that ended in a terminal punctuation mark (i.e.\\ a period, exclamation mark, question mark, or end quotation mark). \\item We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words. \\item We removed any page that contained any word on the ``List of Dirty, Naughty, Obscene or Otherwise Bad Words''.\\footnote{\\url{https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words}} \\item Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript. \\item Some pages had placeholder ``lorem ipsum'' text; we removed any page where the phrase ``lorem ipsum'' appeared. \\item Some pages inadvertently contained code. Since the curly bracket ``\\{'' appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket. \\item Since some of the scraped pages were sourced from Wikipedia and had citation markers (e.g.\\ [1], [citation needed], etc.), we removed any such markers. \\item Many pages had boilerplate policy notices, so we removed any lines containing the strings ``terms of use'', ``privacy policy'', ``cookie policy'', ``uses cookies'', ``use of cookies'', or ``use cookies''. \\item To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set. \\end{itemize} Additionally, since most of our downstream tasks are focused on English-language text, we used \\texttt{langdetect}\\footnote{\\url{https://pypi.org/project/langdetect/}} to filter out any pages that were not classified as English with a probability of at least 0.99. Our heuristics are inspired by past work on using Common Crawl as a source of data for NLP: For example, \\citet{grave2018learning} also filter text using an automatic language detector and discard short lines and \\citet{smith2013dirt,grave2018learning} both perform line-level deduplication. However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g.\\ are limited to News data \\citep{zellers2019defending,liu2019roberta}, comprise only Creative Commons content \\citep{habernal2016c4corpus}, or are focused on parallel training data for machine translation \\citep{smith2013dirt}).  To assemble our base data set, we downloaded the web extracted text from April 2019 and applied the aforementioned filtering. This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the ``\\textbf{C}olossal \\textbf{C}lean \\textbf{C}rawled \\textbf{C}orpus'' (or C4 for short) and release it as part of TensorFlow Datasets.\\footnote{\\url{https://www.tensorflow.org/datasets/catalog/c4}} We consider the impact of using various alternative versions of this data set in \\cref{sec:datasets}.  "
            },
            {
                "section_name": "Downstream Tasks_3",
                "paragraphs": "\\label{sec:tasks}  Our goal in this paper is to measure general language learning abilities. As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. All data was sourced from TensorFlow Datasets.\\footnote{\\url{https://www.tensorflow.org/datasets}}  GLUE \\citep{wang2018glue} and SuperGLUE \\citep{wang2019superglue} each comprise a collection of text classification tasks meant to test general language understanding abilities: \\begin{itemize} \\item Sentence acceptability judgment (CoLA \\citep{warstadt2018neural}) \\item Sentiment analysis (SST-2 \\citep{socher2013recursive}) \\item Paraphrasing/sentence similarity (MRPC \\citep{dolan2005automatically}, STS-B \\citep{cer2017semeval}, QQP \\citep{shankar2017first}) \\item Natural language inference (MNLI \\citep{williams2017broad}, QNLI \\citep{rajpurkar2016squad}, RTE \\citep{dagan2005pascal}, CB \\citep{de2019commitmentbank}) \\item Coreference resolution (WNLI and WSC \\citep{levesque2012winograd}) \\item Sentence completion (COPA \\citep{roemmele2011choice}) \\item Word sense disambiguation (WIC \\citep{pilehvar2018wic}) \\item Question answering (MultiRC \\citep{khashabi2018looking}, ReCoRD \\citep{zhang2018record}, BoolQ \\citep{clark2019boolq}) \\end{itemize} We use the data sets as distributed by the GLUE and SuperGLUE benchmarks. For simplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly for SuperGLUE) as a single task by concatenating all of the constituent data sets. As suggested by \\cite{kocijan2019surprisingly} we also include the Definite Pronoun Resolution (DPR) data set \\citep{rahman2012resolving} in the combined SuperGLUE task.  The CNN/Daily Mail \\citep{hermann2015teaching} data set was introduced as a question-answering task but was adapted for text summarization by  \\citet{nallapati2016abstractive}; we use the non-anonymized version from \\citet{see2017get} as an abstractive summarization task. SQuAD \\citep{rajpurkar2016squad} is a common question-answering benchmark. In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token. For WMT English to German, we use the same training data as \\citep{vaswani2017attention} (i.e.\\ News Commentary v13, Common Crawl, Europarl v7) and \\texttt{newstest2013} as a validation set \\citep{bojar2014findings}. For English to French, we use the standard training data from 2015 and \\texttt{newstest2014} as a validation set \\citep{bojar2015findings}. For English to Romanian, which is a standard lower-resource machine translation benchmark, we use the train and validation sets from WMT 2016 \\citep{bojar2016findings}. Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language.  "
            },
            {
                "section_name": "Input and Output Format_4",
                "paragraphs": "\\label{sec:format}  In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a ``text-to-text'' format---that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using ``teacher forcing'' \\citep{williams1989learning}) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model.  As an example, to ask the model to translate the sentence ``That is good.''\\ from English to German, the model would be fed the sequence ``translate English to German: That is good.''\\ and would be trained to output ``Das ist gut.'' For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark \\citep{williams2017broad} the goal is to predict whether a premise implies (``entailment''), contradicts (``contradiction''), or neither (``neutral'') a hypothesis. With our preprocessing, the input sequence becomes ``mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.'' with the corresponding target word ``entailment''. Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs ``hamburger'' when the only possible labels for a task were ``entailment'', ``neutral'', or ``contradiction''). In this case, we always count the model's output as wrong, though we never observed this behavior in any of our trained models. Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. A diagram of our text-to-text framework with a few input/output examples is shown in \\cref{fig:text_to_text}. We provide full examples of preprocessed inputs for every task we studied in \\cref{sec:preprocessing}.  Our text-to-text framework follows previous work that casts multiple NLP tasks into a common format: \\citet{mccann2018natural} propose the ``Natural Language Decathlon'', a benchmark that uses a consistent question-answering format for a suite of ten NLP tasks. The Natural Language Decathlon also stipulates that all models must be multi-task, i.e.\\ are able to simultaneously tackle all of the tasks at once. We instead allow for separately fine-tuning the model on each individual task and use short task prefixes instead of an explicit question-answer format. \\citet{radford2019language} evaluate the zero-shot learning capabilities of language models by feeding some input to the model as a prefix and then autoregressively sampling an output. For example, automatic summarization is done by feeding in a document followed by the text ``TL;DR:'' (short for ``too long, didn't read'', a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, \\citet{keskar2019unifying} unify many NLP tasks as ``span extraction'', where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. In contrast, our framework also allows for generative tasks like machine translation and abstractive summarization where it is not possible to enumerate all possible output choices.  We were able to straightforwardly cast all of the tasks we considered into a text-to-text format with the exception of STS-B, which is a regression task where the goal is to predict a similarity score between $1$ and $5$. We found that \\text{most} of these scores were annotated in increments of $0.2$, so we simply rounded any score to the nearest increment of $0.2$ and converted the result to a literal string representation of the number (e.g.\\ the floating-point value $2.57$ would be mapped to the string ``2.6''). At test time, if the model outputs a string corresponding to a number between $1$ and $5$, we convert it to a floating-point value; otherwise, we treat the model's prediction as incorrect. This effectively recasts the STS-B regression problem as a 21-class classification problem.  Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more amenable to the text-to-text framework. Examples from the Winograd tasks consist of a text passage containing an ambiguous pronoun that could refer to more than one of the noun phrases in the passage. For example, the passage might be ``The city councilmen refused the demonstrators a permit because they feared violence.'', which contains the ambiguous pronoun ``they'' that could refer to ``city councilmen'' or ``demonstrators''. We cast the WNLI, WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in the text passage and asking the model to predict the noun that it refers to. The example mentioned above would be transformed to the input ``The city councilmen refused the demonstrators a permit because *they* feared violence.'' and the model would be trained to predict the target text ``The city councilmen''.  For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun, and a True/False label reflecting whether the candidate matches the pronoun (ignoring any articles). We only train on examples with a ``True'' label since we do not know the correct noun targets for examples with a ``False'' label. For evaluation, we assign a ``True'' label if the words in the model's output are a subset of the words in the candidate noun phrase (or vice versa) and assign a ``False'' label otherwise. This removes roughly half of the WSC training set, but the DPR data set adds about $1{,}000$ pronoun resolution examples. Examples from DPR are annotated with the correct referent noun, making it easy to use this data set in the format listed above.  The WNLI training and validation sets have a significant overlap with the WSC training set. To avoid leaking validation examples into our training data (a particular issue in the multi-task experiments of \\cref{sec:multitask}), we therefore never train on WNLI and never report results on the WNLI validation set. Omitting results on the WNLI validation set is standard practice \\citep{devlin2018bert} due to the fact that it is ``adversarial'' with respect to the training set, i.e.\\ validation examples are all slightly-perturbed versions of training examples with the opposite label. As such, we do not include WNLI in the average GLUE score whenever we report on the validation set (all sections except \\cref{sec:together} where results are presented on the test sets). Converting examples from WNLI to the ``referent noun prediction'' variant described above is a little more involved; we describe this process in \\cref{sec:wnli_preprocessing}.  "
            },
            {
                "section_name": "Experiments_3",
                "paragraphs": "\\label{sec:experiments}  Recent advances in transfer learning for NLP have come from a wide variety of developments, such as new pre-training objectives, model architectures, unlabeled data sets, and more. In this section, we carry out an empirical survey of these techniques in hopes of teasing apart their contribution and significance. We then combine the insights gained to attain state-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly growing area of research, it is not feasible for us to cover every possible technique or idea in our empirical study. For a broader literature review, we recommend a recent survey by \\cite{ruder2019transfer}.  We systematically study these contributions by taking a reasonable baseline (described in \\cref{sec:baseline}) and altering one aspect of the setup at a time. For example, in \\cref{sec:objectives} we measure the performance of different unsupervised objectives while keeping the rest of our experimental pipeline fixed. This ``coordinate ascent'' approach might miss second-order effects (for example, some particular unsupervised objective may work best on a model larger than our baseline setting), but performing a combinatorial exploration of all of the factors in our study would be prohibitively expensive. In future work, we expect it could be fruitful to more thoroughly consider combinations of the approaches we study.  Our goal is to compare a variety of different approaches on a diverse set of tasks while keeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do not exactly replicate existing approaches. For example, ``encoder-only'' models like BERT \\citep{devlin2018bert} are designed to produce a single prediction per input token or a single prediction for an entire input sequence. This makes them applicable for classification or span prediction tasks but not for generative tasks like translation or abstractive summarization. As such, none of the model architectures we consider are identical to BERT or consist of an encoder-only structure. Instead, we test approaches that are similar in spirit---for example, we consider an analogous objective to BERT's ``masked language modeling'' objective in \\cref{sec:objectives} and we consider a model architecture that behaves similarly to BERT on text classification tasks in \\cref{sec:architectures}.  After outlining our baseline experimental setup in the following subsection, we undertake an empirical comparison of model architectures (\\cref{sec:architectures}), unsupervised objectives (\\cref{sec:objectives}), pre-training data sets (\\cref{sec:datasets}), transfer approaches (\\cref{sec:transfer}), and scaling (\\cref{sec:scaling}). At the culmination of this section, we combine insights from our study with scale to obtain state-of-the-art results in many tasks we consider (\\cref{sec:together}).  "
            },
            {
                "section_name": "Baseline_5",
                "paragraphs": "\\label{sec:baseline}  Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard Transformer (described in \\cref{sec:model}) using a simple denoising objective and then separately fine-tune on each of our downstream tasks. We describe the details of this experimental setup in the following subsections.  \\subsubsection{Model} \\label{sec:model_hparams}  For our model, we use a standard encoder-decoder Transformer as proposed by \\cite{vaswani2017attention}. While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single ``stack'' (e.g.\\ for language modeling \\citep{radford2018improving,dong2019unified} or classification and span prediction \\citep{devlin2018bert,yang2019xlnet}), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. We explore the performance of different model architectures in \\cref{sec:architectures}.  Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a ``BERT\\textsubscript{BASE}'' \\citep{devlin2018bert} stack. Specifically, both the encoder and decoder consist of $12$ blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of $d_{\\mathrm{ff}} = 3072$ followed by a ReLU nonlinearity and another dense layer. The ``key'' and ``value'' matrices of all attention mechanisms have an inner dimensionality of $d_{\\mathrm{kv}} = 64$ and all attention mechanisms have $12$ heads. All other sub-layers and embeddings have a dimensionality of $d_{\\mathrm{model}} = 768$. In total, this results in a model with about $220$ million parameters. This is roughly twice the number of parameters of BERT\\textsubscript{BASE} since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of $0.1$ everywhere dropout is applied in the model.  \\subsubsection{Training} \\label{sec:training}  As described in \\cref{sec:format}, all tasks are formulated as text-to-text tasks. This allows us to always train using standard maximum likelihood, i.e.\\ using teacher forcing \\citep{williams1989learning} and a cross-entropy loss. For optimization, we use AdaFactor \\citep{shazeer2018adafactor}. At test time, we use greedy decoding (i.e.\\ choosing the highest-probability logit at every timestep).  We pre-train each model for $2^{19} = 524{,}288$ steps on C4 before fine-tuning. We use a maximum sequence length of $512$ and a batch size of $128$ sequences. Whenever possible, we ``pack'' multiple sequences into each entry of the batch\\footnote{\\url{https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/index.html\\#data_generators.generator_utils.pack_examples}} so that our batches contain roughly $2^{16} = 65{,}536$ tokens. In total, this batch size and number of steps corresponds to pre-training on $2^{35} \\approx 34\\mathrm{B}$ tokens. This is considerably less than BERT \\citep{devlin2018bert}, which used roughly $137\\mathrm{B}$ tokens, or RoBERTa \\citep{liu2019roberta}, which used roughly $2.2\\mathrm{T}$ tokens. Using only $2^{35}$ tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in \\cref{sec:scaling,sec:together}. Note that $2^{35}$ tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.  During pre-training, we use an ``inverse square root'' learning rate schedule: $1 \\big/ \\sqrt{\\max(n, k)}$ where $n$ is the current training iteration and $k$ is the number of warm-up steps (set to $10^{4}$ in all of our experiments). This sets a constant learning rate of $0.01$ for the first $10^{4}$ steps, then exponentially decays the learning rate until pre-training is over. We also experimented with using a triangular learning rate \\citep{howard2018universal}, which produced slightly better results but requires knowing the total number of training steps ahead of time. Since we will be varying the number of training steps in some of our experiments, we opt for the more generic inverse square root schedule.  Our models are fine-tuned for $2^{18} = 262{,}144$ steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e.\\ those with large data sets), which benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit quickly. During fine-tuning, we continue using batches with $128$ length-$512$ sequences (i.e.\\ $2^{16}$ tokens per batch). We use a constant learning rate of $0.001$ when fine-tuning. We save a checkpoint every $5{,}000$ steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, we choose the best checkpoint for each task independently. For all of the experiments except those in \\cref{sec:together}, we report results in the validation set to avoid performing model selection on the test set.  \\subsubsection{Vocabulary} We use SentencePiece \\citep{kudo2018sentencepiece} to encode text as WordPiece tokens \\citep{sennrich2015neural,kudo2018subword}. For all experiments, we use a vocabulary of $32{,}000$ wordpieces. Since we ultimately fine-tune our model on English to German, French, and Romanian translation, we also require that our vocabulary covers these non-English languages. To address this, we classified pages from the Common Crawl scrape used in C4 as German, French, and Romanian. Then, we trained our SentencePiece model on a mixture of $10$ parts of English C4 data with $1$ part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of our model. Note that our vocabulary makes it so that our model can only process a predetermined, fixed set of languages.  \\subsubsection{Unsupervised Objective} \\label{sec:baseline_objective}  \\begin{figure}[t] \\centering \\includegraphics[width=0.6\\textwidth]{objective.pdf} \\caption{Schematic of the objective we use in our baseline model. In this example, we process the sentence ``Thank you for inviting me to your party last week.'' The words ``for'', ``inviting'' and ``last'' (marked with an $\\times$) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as \\texttt{<X>} and \\texttt{<Y>}) that is unique over the example. Since ``for'' and ``inviting'' occur consecutively, they are replaced by a single sentinel \\texttt{<X>}. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token \\texttt{<Z>}.} \\label{fig:objective} \\end{figure}  Leveraging unlabeled data to pre-train our model necessitates an objective that does not require labels but (loosely speaking) teaches the model generalizable knowledge that will be useful in downstream tasks. Preliminary work that applied the transfer learning paradigm of pre-training and fine-tuning all of the model's parameters to NLP problems used a causal language modeling objective for pre-training \\citep{dai2015semi,peters2018deep,radford2018improving,howard2018universal}. However, it has recently been shown that ``denoising'' objectives \\citep{devlin2018bert,taylor1953cloze} (also called ``masked language modeling'') produce better performance and as a result they have quickly become standard. In a denoising objective, the model is trained to predict missing or otherwise corrupted tokens in the input. Inspired by BERT's ``masked language modeling'' objective and the ``word dropout'' regularization technique \\citep{bowman2015generating}, we design an objective that randomly samples and then drops out $15\\%$ of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens which are added to our vocabulary and do not correspond to any wordpiece. The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. Our choices to mask consecutive spans of tokens and only predict dropped-out tokens were made to reduce the computational cost of pre-training. We perform thorough investigation into pre-training objectives in \\cref{sec:objectives}. An example of the transformation resulting from applying this objective is shown in \\cref{fig:objective}. We empirically compare this objective to many other variants in \\cref{sec:objectives}.  \\subsubsection{Baseline Performance}  In this section, we present results using the baseline experimental procedure described above to get a sense of what kind of performance to expect on our suite of downstream tasks. Ideally, we would repeat every experiment in our study multiple times to get a confidence interval on our results. Unfortunately, this would be prohibitively expensive due to the large number of experiments we run. As a cheaper alternative, we train our baseline model $10$ times from scratch (i.e.\\ with different random initializations and data set shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don't expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes. Separately, we also measure the performance of training our model for $2^{18}$ steps (the same number we use for fine-tuning) on all downstream tasks without pre-training. This gives us an idea of how much pre-training benefits our model in the baseline setting.  When reporting results in the main text, we only report a subset of the scores across all the benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we report the average score across all subtasks (as stipulated by the official benchmarks) under the headings ``GLUE'' and ``SGLUE''. For all translation tasks, we report the BLEU score \\citep{papineni2002bleu} as provided by SacreBLEU v1.3.0 \\citep{post2018call} with ``exp'' smoothing and ``intl'' tokenization. We refer to scores for WMT English to German, English to French, and English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics \\citep{lin2004rouge} to be highly correlated so we report the ROUGE-2-F score alone under the heading ``CNNDM''. Similarly, for SQuAD we find the performance of the ``exact match'' and ``F1'' scores to be highly correlated so we report the ``exact match'' score alone. We provide every score achieved on every task for all experiments in \\cref{tab:giant}, \\cref{sec:giant}.  Our results tables are all formatted so that each row corresponds to a particular experimental configuration with columns giving the scores for each benchmark. We will include the mean performance of the baseline configuration in most tables. Wherever a baseline configuration appears, we will mark it with a $\\bigstar$ (as in the first row of \\cref{tab:baseline}). We also will \\textbf{boldface} any score that is within two standard deviations of the maximum (best) in a given experiment.  \\begin{table} \\footnotesize \\centering \\begin{tabular}{l c c c c c c c} \\toprule & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Baseline average       & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ Baseline standard deviation & $0.235$     & $0.065$     & $0.343$     & $0.416$     & $0.112$     & $0.090$     & $0.108$ \\\\ No pre-training             & $66.22$     & $17.60$     & $50.31$     & $53.04$     & $25.86$     & $\\*{39.77}$ & $24.04$ \\\\ \\bottomrule \\end{tabular} \\caption{ Average and standard deviation of scores achieved by our baseline model and training procedure. For comparison, we also report performance when training on each task from scratch (i.e.\\ without any pre-training) for the same number of steps used to fine-tune the baseline model. All scores in this table (and every table in our paper except \\cref{tab:final}) are reported on the validation sets of each data set. } \\label{tab:baseline} \\end{table}  Our baseline results are shown in \\cref{tab:baseline}. Overall, our results are comparable to existing models of similar size. For example, BERT\\textsubscript{BASE} achieved an exact match score of $80.8$ on SQuAD and an accuracy of $84.4$ on MNLI-matched, whereas we achieve $80.88$ and $84.24$, respectively (see \\cref{tab:giant}). Note that we cannot directly compare our baseline to BERT\\textsubscript{BASE} because ours is an encoder-decoder model and was pre-trained for roughly \\sfrac{1}{4} as many steps. Unsurprisingly, we find that pre-training provides significant gains across almost all benchmarks. The only exception is WMT English to French, which is a large enough data set that gains from pre-training tend to be marginal. We include this task in our experiments to test the behavior of transfer learning in the high-resource regime. Since we perform early stopping by selecting the best-performing checkpoint, the large disparity between our baseline and ``no pre-training'' emphasize how much pre-training improves performance on tasks with limited data. While we do not explicitly measure improvements in data efficiency in this paper, we emphasize that this is one of the primary benefits of the transfer learning paradigm.  As for inter-run variance, we find that for most tasks the standard deviation across runs is smaller than $1\\%$ of the task's baseline score. Exceptions to this rule include CoLA, CB, and COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks. For example, on CB our baseline model had an average F1 score of $91.22$ with a standard deviation of $3.237$ (see \\cref{tab:giant}), which may be partly due to the fact that CB's validation set contains only $56$ examples. Note that the GLUE and SuperGLUE scores are computed as the average of scores across the tasks comprising each benchmark. As a result, we caution that the high inter-run variance of CoLA, CB, and COPA can make it harder to compare models using the GLUE and SuperGLUE scores alone.  "
            },
            {
                "section_name": "Architectures_6",
                "paragraphs": "\\label{sec:architectures}  While the Transformer was originally introduced with an encoder-decoder architecture, much modern work on transfer learning for NLP uses alternative architectures. In this section, we review and compare these architectural variants.  \\subsubsection{Model Structures} \\label{sec:structures}  \\begin{figure}[t] \\centering \\includegraphics[width=0.8\\textwidth]{attention_masks.pdf} \\caption{ Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted $x$ and $y$ respectively. A dark cell at row $i$ and column $j$ indicates that the self-attention mechanism is allowed to attend to input element $j$ at output timestep $i$. A light cell indicates that the self-attention mechanism is \\textit{not} allowed to attend to the corresponding $i$ and $j$ combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the $i$th output element from depending on any input elements from ``the future''. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence. } \\label{fig:attention_masks} \\end{figure}  A major distinguishing factor for different architectures is the ``mask'' used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let $y_i$ refer to the $i$th element of the output sequence and $x_j$ refer to the $j$th entry of the input sequence. $y_i$ is computed as $\\sum_j w_{i, j} x_j$, where $w_{i, j}$ is the scalar weight produced by the self-attention mechanism as a function of $x_i$ and $x_j$. The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in \\cref{fig:attention_masks}. For example, the causal mask (\\cref{fig:attention_masks}, middle) sets any $w_{i, j}$ to zero if $j > i$.  \\begin{figure}[t] \\centering \\includegraphics[width=0.8\\textwidth]{architectures.pdf} \\caption{ Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use ``.''\\ to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as $x$ and $y$ respectively. Left: A standard encoder-decoder architecture uses fully-visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input. } \\label{fig:architectures} \\end{figure}  The first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of \\cref{fig:architectures}.  The encoder uses a ``fully-visible'' attention mask. Fully-visible masking allows a self-attention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in \\cref{fig:attention_masks}, left. This form of masking is appropriate when attending over a ``prefix'', i.e.\\ some context provided to the model that is later used when making predictions. BERT \\citep{devlin2018bert} also uses a fully-visible masking pattern and appends a special ``classification'' token to the input. BERT's output at the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence.  The self-attention operations in the Transformer's decoder use a ``causal'' masking pattern. When producing the $i$th entry of the output sequence, causal masking prevents the model from attending to the $j$th entry of the input sequence for $j > i$. This is used during training so that the model can't ``see into the future'' as it produces its output. An attention matrix for this masking pattern is shown in \\cref{fig:attention_masks}, middle.  The decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e.\\ a model trained solely for next-step prediction \\citep{liu2018generating,radford2018improving,al2019character}. This constitutes the second model structure we consider. A schematic of this architecture is shown in \\cref{fig:architectures}, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method \\citep{radford2018improving}.  Language models are typically used for compression or sequence generation \\citep{graves2013generating}. However, they can also be used in the text-to-text framework simply by concatenating the inputs and targets. As an example, consider the case of English to German translation: If we have a training datapoint with input sentence ``That is good.'' and target ``Das ist gut.'', we would simply train the model on next-step prediction over the concatenated input sequence ``translate English to German: That is good. target: Das ist gut.'' If we wanted to obtain the model's prediction for this example, the model would be fed the prefix ``translate English to German: That is good. target:'' and would be asked to generate the remainder of the sequence autoregressively. In this way, the model can predict an output sequence given an input, which satisfies the needs of text-to-text tasks. This approach was recently used to show that language models can learn to perform some text-to-text tasks without supervision \\citep{radford2019language}.  A fundamental and frequently cited drawback of using a language model in the text-to-text setting is that causal masking forces the model's representation of the $i$th entry of the input sequence to only depend on the entries up until $i$. To see why this is potentially disadvantageous, consider the text-to-text framework where the model is provided with a prefix/context before being asked to make predictions (e.g., the prefix is an English sentence and the model is asked to predict the German translation). With fully causal masking, the model's representation of a prefix state can only depend on prior entries of the prefix. So, when predicting an entry of the output, the model will attend to a representation of the prefix that is unnecessarily limited. Similar arguments have been made against using a unidirectional recurrent neural network encoder in sequence-to-sequence models \\citep{bahdanau2014neural}.  This issue can be avoided in a Transformer-based language model simply by changing the masking pattern. Instead of using a causal mask, we use fully-visible masking during the prefix portion of the sequence. This masking pattern and a schematic of the resulting ``prefix LM'' (the third model structure we consider) are illustrated in the rightmost panels of \\cref{fig:attention_masks,fig:architectures}, respectively. In the English to German translation example mentioned above, fully-visible masking would be applied to the prefix ``translate English to German: That is good. target:'' and causal masking would be used during training for predicting the target ``Das ist gut.'' Using a prefix LM in the text-to-text framework was originally proposed by \\cite{liu2018generating}. More recently, \\cite{dong2019unified} showed that this architecture is effective on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder model with parameters shared across the encoder and decoder and with the encoder-decoder attention replaced with full attention across the input and target sequence.  We note that when following our text-to-text framework, the prefix LM architecture closely resembles BERT \\citep{devlin2018bert} for classification tasks. To see why, consider an example from the MNLI benchmark where the premise is ``I hate pigeons.'', the hypothesis is ``My feelings towards pigeons are filled with animosity.'' and the correct label is ``entailment''. To feed this example into a language model, we would transform it into the sequence ``mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity. target: entailment''. In this case, the fully-visible prefix would correspond to the entire input sequence up to the word ``target:'', which can be seen as being analogous to the ``classification'' token used in BERT. So, our model would have full visibility over the entire input, and then would be tasked with making a classification by outputting the word ``entailment''. It is easy for the model to learn to output one of the valid class labels given the task prefix (``mnli'' in this case). As such, the main difference between a prefix LM and the BERT architecture is that the classifier is simply integrated into the output layer of the Transformer decoder in the prefix LM.  \\subsubsection{Comparing Different Model Structures} \\label{sec:architecture_variants}  In the interest of experimentally comparing these architectural variants, we would like each model we consider to be equivalent in some meaningful way. We might say that two models are equivalent if they either have the same number of parameters or they require roughly the same amount of computation to process a given (input-sequence, target-sequence) pair. Unfortunately, it is not possible to compare an encoder-decoder model to a language model architecture (comprising a single Transformer stack) according to both of these criteria at the same time. To see why, first note an encoder-decoder model with $L$ layers in the encoder and $L$ layers in the decoder has approximately the same number of parameters as a language model with $2L$ layers. However, the same $L + L$ encoder-decoder model will have approximately the same computational cost as a language model with \\textit{only} $L$ layers. This is a consequence of the fact that the $L$ layers in the language model must be applied to \\textit{both} the input and output sequence, while the encoder is only applied to the input sequence and the decoder is only applied to the output sequence. Note that these equivalences are approximate---there are some extra parameters in the decoder due to the encoder-decoder attention and there are also some computational costs in the attention layers that are quadratic in the sequence lengths. In practice, however, we observed nearly identical step times for $L$-layer language models versus $L + L$-layer encoder-decoder models, suggesting a roughly equivalent computational cost. Further, for the model sizes we consider, the number of parameters in the encoder-decoder attention layers is about 10\\% of the total parameter count, so we make the simplifying assumption that an $L + L$-layer encoder-decoder model has the same number of parameters as an $2L$-layer language model.  To provide a reasonable means of comparison, we consider multiple configurations for our encoder-decoder model. We will refer to the number of layers and parameters in a BERT\\textsubscript{BASE}-sized layer stack as $L$ and $P$, respectively. We will use $M$ to refer to the number of FLOPs required for an $L + L$-layer encoder-decoder model or $L$-layer decoder-only model to process a given input-target pair. In total, we will compare: \\begin{itemize} \\item An encoder-decoder model with $L$ layers in the encoder and $L$ layers in the decoder. This model has $2P$ parameters and a computation cost of $M$ FLOPs. \\item An equivalent model, but with parameters shared across the encoder and decoder, resulting in $P$ parameters and an $M$-FLOP computational cost. \\item An encoder-decoder model with $L/2$ layers each in the encoder and decoder, giving $P$ parameters and an $M/2$-FLOP cost. \\item A decoder-only language model with $L$ layers and $P$ parameters and a resulting computational cost of $M$ FLOPs. \\item A decoder-only prefix LM with the same architecture (and thus the same number of parameters and computational cost), but with fully-visible self-attention over the input. \\end{itemize}  \\subsubsection{Objectives} \\label{sec:architecture_objectives}  As an unsupervised objective, we will consider both a basic language modeling objective as well as our baseline denoising objective described in \\cref{sec:baseline_objective}. We include the language modeling objective due to its historic use as a pre-training objective \\citep{dai2015semi,ramachandran2016unsupervised,howard2018universal,radford2018improving,peters2018deep} as well as its natural fit for the language model architectures we consider. For models that ingest a prefix before making predictions (the encoder-decoder model and prefix LM), we sample a span of text from our unlabeled data set and choose a random point to split it into prefix and target portions. For the standard language model, we train the model to predict the entire span from beginning to end. Our unsupervised denoising objective is designed for text-to-text models; to adapt it for use with a language model we concatenate the inputs and targets as described in \\cref{sec:structures}.  \\subsubsection{Results}  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Architecture         & Objective & Params & Cost  & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Encoder-decoder & Denoising & $2P$   & $M$   & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ Enc-dec, shared      & Denoising & $P$    & $M$   & $82.81$     & $18.78$     & $\\*{80.63}$ & $\\*{70.73}$ & $26.72$     & $39.03$     & $\\*{27.46}$ \\\\ Enc-dec, 6 layers    & Denoising & $P$    & $M/2$ & $80.88$     & $18.97$     & $77.59$     & $68.42$     & $26.38$     & $38.40$     & $26.95$ \\\\ Language model       & Denoising & $P$    & $M$   & $74.70$     & $17.93$     & $61.14$     & $55.02$     & $25.09$     & $35.28$     & $25.86$ \\\\ Prefix LM            & Denoising & $P$    & $M$   & $81.82$     & $18.61$     & $78.94$     & $68.11$     & $26.43$     & $37.98$     & $27.39$ \\\\ \\midrule Encoder-decoder      & LM        & $2P$   & $M$   & $79.56$     & $18.59$     & $76.02$     & $64.29$     & $26.27$     & $39.17$     & $26.86$ \\\\ Enc-dec, shared      & LM        & $P$    & $M$   & $79.60$     & $18.13$     & $76.35$     & $63.50$     & $26.62$     & $39.17$     & $27.05$ \\\\ Enc-dec, 6 layers    & LM        & $P$    & $M/2$ & $78.67$     & $18.26$     & $75.32$     & $64.06$     & $26.13$     & $38.42$     & $26.89$ \\\\ Language model       & LM        & $P$    & $M$   & $73.78$     & $17.54$     & $53.81$     & $56.51$     & $25.23$     & $34.31$     & $25.38$ \\\\ Prefix LM            & LM        & $P$    & $M$   & $79.68$     & $17.84$     & $76.87$     & $64.86$     & $26.28$     & $37.51$     & $26.76$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance of the different architectural variants described in \\cref{sec:architecture_variants}. We use $P$ to refer to the number of parameters in a 12-layer base Transformer layer stack and $M$ to refer to the FLOPs required to process a sequence using the encoder-decoder model. We evaluate each architectural variant using a denoising objective (described in \\cref{sec:baseline_objective}) and an autoregressive objective (as is commonly used to train language models). } \\label{tab:architectures_results}  \\end{table}  The scores achieved by each of the architectures we compare are shown in \\cref{tab:architectures_results}. For all tasks, the encoder-decoder architecture with the denoising objective performed best. This variant has the highest parameter count ($2P$) but the same computational cost as the $P$-parameter decoder-only models. Surprisingly, we found that sharing parameters across the encoder and decoder performed nearly as well. In contrast, halving the number of layers in the encoder and decoder stacks significantly hurt performance. Concurrent work \\citep{lan2019albert} also found that sharing parameters across Transformer blocks can be an effective means of lowering the total parameter count without sacrificing much performance. XLNet also bears some resemblance to the shared encoder-decoder approach with a denoising objective \\citep{yang2019xlnet}. We also note that the shared parameter encoder-decoder outperforms the decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder attention is beneficial. Finally, we confirm the widely-held conception that using a denoising objective always results in better downstream task performance compared to a language modeling objective. This observation has been previously made by \\citet{devlin2018bert}, \\citet{voita2019bottom}, and \\citet{lample2019cross} among others. We undertake a more detailed exploration of unsupervised objectives in the following section.  "
            },
            {
                "section_name": "Unsupervised Objectives_7",
                "paragraphs": "\\label{sec:objectives}  The choice of unsupervised objective is of central importance as it provides the mechanism through which the model gains general-purpose knowledge to apply to downstream tasks. This has led to the development of a wide variety of pre-training objectives \\citep{dai2015semi,ramachandran2016unsupervised,radford2018improving,devlin2018bert,yang2019xlnet,liu2019multi,wang2019can,song2019mass,dong2019unified,joshi2019spanbert}. In this section, we perform a procedural exploration of the space of unsupervised objectives. In many cases, we will not replicate an existing objective exactly---some will be modified to fit our text-to-text encoder-decoder framework and, in other cases, we will use objectives that combine concepts from multiple common approaches.  Overall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized span of text from our unlabeled text data set. The token sequence is processed to produce a (corrupted) input sequence and a corresponding target. Then, the model is trained as usual with maximum likelihood to predict the target sequence. We provide illustrative examples of many of the objectives we consider in \\cref{tab:objectives}.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l l l} \\toprule Objective & Inputs & Targets   \\\\ \\midrule Prefix language modeling & Thank you for inviting & me to your party last week . \\\\ BERT-style \\cite{devlin2018bert} & Thank you \\texttt{<M>} \\texttt{<M>} me to your party \\textcolor{gray}{apple} week . & \\textit{(original text)}  \\\\ Deshuffling & party me for your to . last fun you inviting week Thank & \\textit{(original text)} \\\\ MASS-style \\cite{song2019mass} & Thank you \\texttt{<M>} \\texttt{<M>} me to your party \\texttt{<M>} week . & \\textit{(original text)} \\\\ I.i.d.\\ noise, replace spans & Thank you \\texttt{<X>} me to your party \\texttt{<Y>} week . & \\texttt{<X>} for inviting \\texttt{<Y>} last \\texttt{<Z>} \\\\ I.i.d.\\ noise, drop tokens & Thank you me to your party week . & for inviting last \\\\ Random spans & Thank you \\texttt{<X>} to \\texttt{<Y>} week . & \\texttt{<X>} for inviting me \\texttt{<Y>} your party last \\texttt{<Z>} \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text ``Thank you for inviting me to your party last week .'' Note that all of our objectives process \\textit{tokenized} text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write \\textit{(original text)} as a target to denote that the model is tasked with reconstructing the entire input text. \\texttt{<M>} denotes a shared mask token and \\texttt{<X>}, \\texttt{<Y>}, and \\texttt{<Z>} denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word \\textcolor{gray}{apple}. } \\label{tab:objectives} \\end{table}  \\subsubsection{Disparate High-Level Approaches} \\label{sec:objectives_highlevel}  To begin with, we compare three techniques that are inspired by commonly-used objectives but differ significantly in their approach. First, we include a basic ``prefix language modeling'' objective as was used in \\cref{sec:architecture_objectives}. This technique splits a span of text into two components, one to use as inputs to the encoder and the other to use as a target sequence to be predicted by the decoder. Second, we consider an objective inspired by the ``masked language modeling'' (MLM) objective used in BERT \\citep{devlin2018bert}. MLM takes a span of text and corrupts $15\\%$ of the tokens. $90\\%$ of the corrupted tokens are replaced with a special mask token and $10\\%$ are replaced with a random token. Since BERT is an encoder-only model, its goal during pre-training is to reconstruct masked tokens at the output of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted sequence as the target. Note that this differs from our baseline objective, which uses only the corrupted tokens as targets; we compare these two approaches in \\cref{sec:simplifying_bert}. Finally, we also consider a basic deshuffling objective as used e.g.\\ in \\citep{liu2019summae} where it was applied to a denoising sequential autoencoder. This approach takes a sequence of tokens, shuffles it, and then uses the original deshuffled sequence as a target. We provide examples of the inputs and targets for these three methods in the first three rows of \\cref{tab:objectives}.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Objective                         & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo        \\\\ \\midrule Prefix language modeling          & $80.69$     & $18.94$     & $77.99$     & $65.27$     & $\\*{26.86}$ & $39.73$     & $\\*{27.49}$ \\\\ BERT-style \\citep{devlin2018bert} & $\\*{82.96}$ & $\\*{19.17}$ & $\\*{80.65}$ & $\\*{69.85}$ & $\\*{26.78}$ & $\\*{40.03}$ & $\\*{27.41}$ \\\\ Deshuffling                       & $73.17$     & $18.59$     & $67.61$     & $58.47$     & $26.11$     & $39.30$     & $25.62$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance of the three disparate pre-training objectives described in \\cref{sec:objectives_highlevel}. } \\label{tab:objectives_highlevel} \\end{table}  The performance of these three objectives is shown in \\cref{tab:objectives_highlevel}. Overall, we find that the BERT-style objective performs best, though the prefix language modeling objective attains similar performance on the translation tasks. Indeed, the motivation for the BERT objective was to outperform language model-based pre-training. The deshuffling objective performs considerably worse than both prefix language modeling and the BERT-style objective.  \\subsubsection{Simplifying the BERT Objective} \\label{sec:simplifying_bert}  Based on the results in the prior section, we will now focus on exploring modifications to the BERT-style denoising objective. This objective was originally proposed as a pre-training technique for an encoder-only model trained for classification and span prediction. As such, it may be possible to modify it so that it performs better or is more efficient in our encoder-decoder text-to-text setup.  First, we consider a simple variant of the BERT-style objective where we don't include the random token swapping step. The resulting objective simply replaces $15\\%$ of the tokens in the input with a mask token and the model is trained to reconstruct the original uncorrupted sequence. A similar masking objective was used by \\cite{song2019mass} where it was referred to as ``MASS'', so we call this variant the ``MASS-style'' objective. Second, we were interested to see if it was possible to avoid predicting the entire uncorrupted text span since this requires self-attention over long sequences in the decoder. We consider two strategies to achieve this: First, instead of replacing each corrupted token with a mask token, we replace the entirety of each consecutive span of corrupted tokens with a unique mask token. Then, the target sequence becomes the concatenation of the ``corrupted'' spans, each prefixed by the mask token used to replace it in the input. This is the pre-training objective we use in our baseline, described in \\cref{sec:baseline_objective}. Second, we also consider a variant where we simply drop the corrupted tokens from the input sequence completely and task the model with reconstructing the dropped tokens in order. Examples of these approaches are shown in the fifth and sixth rows of \\cref{tab:objectives}.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Objective                         & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule BERT-style \\citep{devlin2018bert} & $82.96$     & $19.17$     & $\\*{80.65}$ & $69.85$     & $26.78$     & $\\*{40.03}$ & $27.41$ \\\\ MASS-style \\citep{song2019mass}   & $82.32$     & $19.16$     & $80.10$     & $69.28$     & $26.79$     & $\\*{39.89}$ & $27.55$ \\\\ \\bsl Replace corrupted spans      & $83.28$     & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $39.82$     & $\\*{27.65}$ \\\\ Drop corrupted tokens             & $\\*{84.44}$ & $\\*{19.31}$ & $\\*{80.52}$ & $68.67$     & $\\*{27.07}$ & $39.76$     & $\\*{27.82}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Comparison of variants of the BERT-style pre-training objective. In the first two variants, the model is trained to reconstruct the original uncorrupted text segment. In the latter two, the model only predicts the sequence of corrupted tokens. } \\label{tab:objectives_bert} \\end{table}  An empirical comparison of the original BERT-style objective to these three alternatives is shown in \\cref{tab:objectives_bert}. We find that in our setting, all of these variants perform similarly. The only exception was that dropping corrupted tokens completely produced a small improvement in the GLUE score thanks to a significantly higher score on CoLA ($60.04$, compared to our baseline average of $53.84$, see \\cref{tab:giant}). This may be due to the fact that CoLA involves classifying whether a given sentence is grammatically and syntactically acceptable, and being able to determine when tokens are missing is closely related to detecting acceptability. However, dropping tokens completely performed worse than replacing them with sentinel tokens on SuperGLUE. The two variants that do not require predicting the full original sequence (``replace corrupted spans'' and ``drop corrupted spans'') are both potentially attractive since they make the target sequences shorter and consequently make training faster. Going forward, we will explore variants where we replace corrupted spans with sentinel tokens and only predict the corrupted tokens (as in our baseline objective).  \\subsubsection{Varying the Corruption Rate} \\label{sec:corruption_rate}  So far, we have been corrupting 15\\% of the tokens, the value used in BERT \\citep{devlin2018bert}. Again, since our text-to-text framework differs from BERT's, we are interested to see if a different corruption rate works better for us. We compare corruption rates of $10\\%$, $15\\%$, $25\\%$, and $50\\%$ in \\cref{tab:objectives_rate}. Overall, we find that the corruption rate had a limited effect on the model's performance. The only exception is that the largest corruption rate we consider ($50\\%$) results in a significant degradation of performance on GLUE and SQuAD. Using a larger corruption rate also results in longer targets, which can potentially slow down training. Based on these results and the historical precedent set by BERT, we will use a corruption rate of $15\\%$ going forward.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Corruption rate & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule $10\\%$          & $\\*{82.82}$ & $19.00$     & $\\*{80.38}$ & $69.55$     & $\\*{26.87}$ & $39.28$     & $\\*{27.44}$ \\\\ \\bsl $15\\%$     & $\\*{83.28}$ & $19.24$     & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ $25\\%$          & $\\*{83.00}$ & $\\*{19.54}$ & $\\*{80.96}$ & $70.48$     & $\\*{27.04}$ & $\\*{39.83}$ & $\\*{27.47}$ \\\\ $50\\%$          & $81.27$     & $19.32$     & $79.80$     & $70.33$     & $\\*{27.01}$ & $\\*{39.90}$ & $\\*{27.49}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance of the i.i.d.\\ corruption objective with different corruption rates. } \\label{tab:objectives_rate} \\end{table}  \\subsubsection{Corrupting Spans} \\label{sec:objective_spans}  We now turn towards the goal of speeding up training by predicting shorter targets. The approach we have used so far makes an i.i.d.\\ decision for each input token as to whether to corrupt it or not. When multiple consecutive tokens have been corrupted, they are treated as a ``span'' and a single unique mask token is used to replace the entire span. Replacing entire spans with a single token results in unlabeled text data being processed into shorter sequences. Since we are using an i.i.d.\\ corruption strategy, it is not always the case that a significant number of corrupted tokens appear consecutively. As a result, we might obtain additional speedup by specifically corrupting spans of tokens rather than corrupting individual tokens in an i.i.d.\\ manner. Corrupting spans was also previously considered as a pre-training objective for BERT, where it was found to improve performance \\citep{joshi2019spanbert}.  To test this idea, we consider an objective that specifically corrupts contiguous, randomly-spaced spans of tokens. This objective can be parametrized by the proportion of tokens to be corrupted and the total number of corrupted spans. The span lengths are then chosen randomly to satisfy these specified parameters. For example, if we are processing a sequence of $500$ tokens and we have specified that $15\\%$ of tokens should be corrupted and that there should be $25$ total spans, then the total number of corrupted tokens would be $500 \\times 0.15 = 75$ and the average span length would be $75 / 25 = 3$. Note that given the original sequence length and corruption rate, we can equivalently parametrize this objective either by the average span length or the total number of spans. \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Span length            & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Baseline (i.i.d.) & $\\*{83.28}$ & $19.24$     & $80.88$     & $71.36$     & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ $2$                    & $\\*{83.54}$ & $19.39$     & $\\*{82.09}$ & $\\*{72.20}$ & $\\*{26.76}$ & $\\*{39.99}$ & $\\*{27.63}$ \\\\ $3$                    & $\\*{83.49}$ & $\\*{19.62}$ & $\\*{81.84}$ & $\\*{72.53}$ & $\\*{26.86}$ & $39.65$     & $\\*{27.62}$ \\\\ $5$                    & $\\*{83.40}$ & $19.24$     & $\\*{82.05}$ & $\\*{72.23}$ & $\\*{26.88}$ & $39.40$     & $\\*{27.53}$ \\\\ $10$                   & $82.85$     & $19.33$     & $\\*{81.84}$ & $70.44$     & $\\*{26.79}$ & $39.49$     & $\\*{27.69}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance of the span-corruption objective (inspired by \\cite{joshi2019spanbert}) for different average span lengths. In all cases, we corrupt 15\\% of the original text sequence. } \\label{tab:objectives_span} \\end{table}  We compare the span-corruption objective to the i.i.d-corruption objective in \\cref{tab:objectives_span}. We use a corruption rate of $15\\%$ in all cases and compare using average span lengths of $2$, $3$, $5$ and $10$. Again, we find a limited difference between these objectives, though the version with an average span length of $10$ slightly underperforms the other values in some cases. We also find in particular that using an average span length of $3$ slightly (but significantly) outperforms the i.i.d.\\ objective on most non-translation benchmarks. Fortunately, the span-corruption objective also provides some speedup during training compared to the i.i.d.\\ noise approach because span corruption produces shorter sequences on average.  \\begin{figure}[t] \\centering \\includegraphics[width=0.6\\textwidth]{objectives_flow.pdf} \\caption{ A flow chart of our exploration of unsupervised objectives. We first consider a few disparate approaches in \\cref{sec:objectives_highlevel} and find that a BERT-style denoising objective performs best. Then, we consider various methods for simplifying the BERT objective so that it produces shorter target sequences in \\cref{sec:simplifying_bert}. Given that replacing dropped-out spans with sentinel tokens performs well and results in short target sequences, in \\cref{sec:corruption_rate} we experiment with different corruption rates. Finally, we evaluate an objective that intentionally corrupts contiguous spans of tokens in \\cref{sec:objective_spans}. } \\label{fig:objectives_flow} \\end{figure}  \\subsubsection{Discussion}  \\Cref{fig:objectives_flow} shows a flow chart of the choices made during our exploration of unsupervised objectives. Overall, the most significant difference in performance we observed was that denoising objectives outperformed language modeling and deshuffling for pre-training. We did not observe a remarkable difference across the many variants of the denoising objectives we explored. However, different objectives (or parameterizations of objectives) can lead to different sequence lengths and thus different training speeds. This implies that choosing among the denoising objectives we considered here should mainly be done according to their computational cost. Our results also suggest that additional exploration of objectives similar to the ones we consider here may not lead to significant gains for the tasks and model we consider. Instead, it may be fortuitous to explore entirely different ways of leveraging unlabeled data.  "
            },
            {
                "section_name": "Pre-training Data set_8",
                "paragraphs": "\\label{sec:datasets}  Like the unsupervised objective, the pre-training data set itself is a crucial component of the transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training data sets are usually not treated as significant contributions on their own and are often not released alongside pre-trained models and code. Instead, they are typically introduced in the course of presenting a new method or model. As a result, there has been relatively little comparison of different pre-training data sets as well as a lack of a ``standard'' data set used for pre-training. Some recent notable exceptions \\citep{baevski2019cloze,liu2019roberta,yang2019xlnet} have compared pre-training on a new large (often Common Crawl-sourced) data set to using a smaller preexisting data set (often Wikipedia). To probe more deeply into the impact of the pre-training data set on performance, in this section we compare variants of our C4 data set and other potential sources of pre-training data. We release all of the C4 data set variants we consider as part of TensorFlow Datasets.\\footnote{\\url{https://www.tensorflow.org/datasets/catalog/c4}}  \\subsubsection{Unlabeled Data Sets} \\label{sec:datasets_comparison}  In creating C4, we developed various heuristics to filter the web-extracted text from Common Crawl (see \\cref{sec:dataset} for a description). We are interested in measuring whether this filtering results in improved performance on downstream tasks, in addition to comparing it to other filtering approaches and common pre-training data sets. Towards this end, we compare the performance of our baseline model after pre-training on the following data sets:  \\begin{description} \\item[C4] As a baseline, we first consider pre-training on our proposed unlabeled data set as described in \\cref{sec:dataset}.  \\item[Unfiltered C4] To measure the effect of the heuristic filtering we used in creating C4 (deduplication, removing bad words, only retaining sentences, etc.), we also generate an alternate version of C4 that forgoes this filtering. Note that we still use \\texttt{langdetect} to extract English text. As a result, our ``unfiltered'' variant still includes some filtering because \\texttt{langdetect} sometimes assigns a low probability to non-natural English text.  \\item[RealNews-like] Recent work has used text data extracted from news websites \\citep{zellers2019defending,baevski2019cloze}. To compare to this approach, we generate another unlabeled data set by additionally filtering C4 to only include content from one of the domains used in the ``RealNews'' data set \\citep{zellers2019defending}. Note that for ease of comparison, we retain the heuristic filtering methods used in C4; the only difference is that we have ostensibly omitted any non-news content.  \\item[WebText-like] Similarly, the WebText data set \\citep{radford2019language} only uses content from webpages that were submitted to the content aggregation website Reddit and received a ``score'' of at least 3. The score for a webpage submitted to Reddit is computed based on the proportion of users who endorse (upvote) or oppose (downvote) the webpage. The idea behind using the Reddit score as a quality signal is that users of the site would only upvote high-quality text content. To generate a comparable data set, we first tried removing all content from C4 that did not originate from a URL that appeared in the list prepared by the OpenWebText effort.\\footnote{\\url{https://github.com/jcpeterson/openwebtext}} However, this resulted in comparatively little content---only about 2 GB---because most pages never appear on Reddit. Recall that C4 was created based on a single month of Common Crawl data. To avoid using a prohibitively small data set, we therefore downloaded 12 months of data from Common Crawl from August 2018 to July 2019, applied our heuristic filtering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like data set, which is of comparable size to the original 40GB WebText data set \\citep{radford2019language}.  \\item[Wikipedia] The website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from TensorFlow Datasets,\\footnote{\\url{https://www.tensorflow.org/datasets/catalog/wikipedia}} which omits any markup or reference sections from the articles.  \\item[Wikipedia + Toronto Books Corpus] A drawback of using pre-training data from Wikipedia is that it represents only one possible domain of natural text (encyclopedia articles). To mitigate this, BERT \\citep{devlin2018bert} combined data from Wikipedia with the Toronto Books Corpus (TBC) \\citep{zhu2015aligning}. TBC contains text extracted from eBooks, which represents a different domain of natural language. BERT's popularity has led to the Wikipedia + TBC combination being used in many subsequent works.  \\end{description}  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Data set         & Size  & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl C4         & 745GB & $83.28$     & $\\*{19.24}$ & $80.88$     & $71.36$     & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ C4, unfiltered  & 6.1TB & $81.46$     & $19.14$     & $78.78$     & $68.04$     & $26.55$     & $39.34$     & $27.21$ \\\\ RealNews-like   & 35GB  & $\\*{83.83}$ & $\\*{19.23}$ & $80.39$     & $72.38$     & $\\*{26.75}$ & $\\*{39.90}$ & $\\*{27.48}$ \\\\ WebText-like    & 17GB  & $\\*{84.03}$ & $\\*{19.31}$ & $\\*{81.42}$ & $71.40$     & $\\*{26.80}$ & $\\*{39.74}$ & $\\*{27.59}$ \\\\ Wikipedia       & 16GB  & $81.85$     & $\\*{19.31}$ & $81.29$     & $68.01$     & $\\*{26.94}$ & $39.69$     & $\\*{27.67}$ \\\\ Wikipedia + TBC & 20GB  & $83.65$     & $\\*{19.28}$ & $\\*{82.08}$ & $\\*{73.24}$ & $\\*{26.77}$ & $39.63$     & $\\*{27.57}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance resulting from pre-training on different data sets. The first four variants are based on our new C4 data set. } \\label{tab:datasets} \\end{table}  The results achieved after pre-training on each of these data sets is shown in \\cref{tab:datasets}. A first obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades performance and makes the unfiltered variant perform the worst in every task. Beyond this, we found that in some cases a pre-training data set with a more constrained domain outperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus produced a SuperGLUE score of $73.24$, beating our baseline's score (using C4) of $71.36$. This is almost entirely attributable to a boost in performance from $25.78$ (baseline, C4) to $50.93$ (Wikipedia + TBC) on the Exact Match score for MultiRC (see \\cref{tab:giant}). MultiRC is a reading comprehension data set whose largest source of data comes from fiction books, which is exactly the domain covered by TBC. Similarly, using the RealNews-like data set for pre-training conferred an increase from $68.16$ to $73.72$ on the Exact Match score for ReCoRD, a data set that measures reading comprehension on news articles. As a final example, using data from Wikipedia produced significant (but less dramatic) gains on SQuAD, which is a question-answering data set with passages sourced from Wikipedia. Similar observations have been made in prior work, e.g.\\ \\citet{beltagy2019scibert} found that pre-training BERT on text from research papers improved its performance on scientific tasks. The main lesson behind these findings is that \\textit{pre-training on in-domain unlabeled data can improve performance on downstream tasks}. This is unsurprising but also unsatisfying if our goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains. \\citet{liu2019roberta} also observed that pre-training on a more diverse data set yielded improvements on downstream tasks. This observation also motivates the parallel line of research on domain adaptation for natural language processing; for surveys of this field see e.g.\\ \\cite{ruder2019neural,li2012literature}.  A drawback to only pre-training on a single domain is that the resulting data sets are often substantially smaller. Similarly, while the WebText-like variant performed as well or better than the C4 data set in our baseline setting, the Reddit-based filtering produced a data set that was about $40\\times$ smaller than C4 despite being based on $12\\times$ more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on $2^{35} \\approx 34\\mathrm{B}$ tokens, which is only about $8$ times larger than the smallest pre-training data set we consider. We investigate at what point using a smaller pre-training data sets poses an issue in the following section.  \\subsubsection{Pre-training Data set Size} \\label{sec:datasets_take}  The pipeline we use to create C4 was designed to be able to create extremely large pre-training data sets. The access to so much data allows us to pre-train our models without repeating examples. It is not clear whether repeating examples during pre-training would be helpful or harmful to downstream performance because our pre-training objective is itself stochastic and can help prevent the model from seeing the same exact data multiple times.  To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model on artificially truncated versions of C4. Recall that we pre-train our baseline model on $2^{35} \\approx 34\\mathrm{B}$ tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of $2^{29}$, $2^{27}$, $2^{25}$ and $2^{23}$ tokens. These sizes correspond to repeating the data set $64$, $256$, $1{,}024$, and $4{,}096$ times respectively over the course of pre-training.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Number of tokens   & Repeats   & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Full data set  & $0$       & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ $2^{29}$           & $64$      & $\\*{82.87}$ & $\\*{19.19}$ & $\\*{80.97}$ & $\\*{72.03}$ & $\\*{26.83}$ & $\\*{39.74}$ & $\\*{27.63}$ \\\\ $2^{27}$           & $256$     & $82.62$     & $\\*{19.20}$ & $79.78$     & $69.97$     & $\\*{27.02}$ & $\\*{39.71}$ & $27.33$ \\\\ $2^{25}$           & $1{,}024$ & $79.55$     & $18.57$     & $76.27$     & $64.76$     & $26.38$     & $39.56$     & $26.80$ \\\\ $2^{23}$           & $4{,}096$ & $76.34$     & $18.33$     & $70.92$     & $59.29$     & $26.37$     & $38.84$     & $25.81$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Measuring the effect of repeating data during pre-training. In these experiments, we only use the first $N$ tokens from C4 (with varying values of $N$ shown in the first column) but still pre-train over $2^{35}$ tokens. This results in the data set being repeated over the course of pre-training (with the number of repeats for each experiment shown in the second column), which may result in memorization (see \\cref{fig:datasets_take_loss}). } \\label{tab:datasets_take} \\end{table}  The resulting downstream performance is shown in \\cref{tab:datasets_take}. As expected, performance degrades as the data set size shrinks. We suspect this may be due to the fact that the model begins to memorize the pre-training data set. To measure if this is true, we plot the training loss for each of these data set sizes in \\cref{fig:datasets_take_loss}. Indeed, the model attains significantly smaller training losses as the size of the pre-training data set shrinks, suggesting possible memorization. \\citet{baevski2019cloze} similarly observed that truncating the pre-training data set size can degrade downstream task performance.  We note that these effects are limited when the pre-training data set is repeated only $64$ times. This suggests that some amount of repetition of pre-training data might not be harmful. However, given that additional pre-training can be beneficial (as we will show in \\cref{sec:scaling}) and that obtaining additional unlabeled data is cheap and easy, we suggest using large pre-training data sets whenever possible. We also note that this effect may be more pronounced for larger model sizes, i.e.\\ a bigger model may be more prone to overfitting to a smaller pre-training data set.  \\begin{figure}[t] \\centering \\includegraphics[width=0.6\\textwidth]{datasets_take_loss.pdf} \\caption{ Pre-training loss for our original C4 data set as well as $4$ artificially truncated versions. The sizes listed refer to the number of tokens in each data set. The four sizes considered correspond to repeating the data set between $64$ and $4{,}096$ times over the course of pre-training. Using a smaller data set size results in smaller training loss values, which may suggest some memorization of the unlabeled data set. } \\label{fig:datasets_take_loss} \\end{figure}  "
            },
            {
                "section_name": "Training Strategy_9",
                "paragraphs": "\\label{sec:transfer}  So far we have considered the setting where all parameters of a model are pre-trained on an unsupervised task before being fine-tuned on individual supervised tasks. While this approach is straightforward, various alternative methods for training the model on downstream/supervised tasks have been proposed. In this section, we compare different schemes for fine-tuning the model in addition to the approach of training the model simultaneously on multiple tasks.  \\subsubsection{Fine-tuning Methods}  It has been argued that fine-tuning all of the model's parameters can lead to suboptimal results, particularly on low-resource tasks \\citep{peters2019tune}. Early results on transfer learning for text classification tasks advocated fine-tuning only the parameters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model \\citep{subramanian2018learning,kiros2015skip,logeswaran2018efficient,hill2016learning,conneau2017supervised}. This approach is less applicable to our encoder-decoder model because the entire decoder must be trained to output the target sequences for a given task. Instead, we focus on two alternative fine-tuning approaches that update only a subset of the parameters of our encoder-decoder model.  The first, ``adapter layers'' \\citep{houlsby2019parameter,bapna2019simple}, is motivated by the goal of keeping most of the original model fixed while fine-tuning. Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer. These new feed-forward networks are designed so that their output dimensionality matches their input. This allows them to be inserted into the network with no additional changes to the structure or parameters. When fine-tuning, only the adapter layer and layer normalization parameters are updated. The main hyperparameter of this approach is the inner dimensionality $d$ of the feed-forward network, which changes the number of new parameters added to the model. We experiment with various values for $d$.  The second alternative fine-tuning method we consider is ``gradual unfreezing'' \\citep{howard2018universal}. In gradual unfreezing, more and more of the model's parameters are fine-tuned over time. Gradual unfreezing was originally applied to a language model architecture consisting of a single stack of layers. In this setting, at the start of fine-tuning only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network's parameters are being fine-tuned. To adapt this approach to our encoder-decoder model, we gradually unfreeze layers in the encoder and decoder in parallel, starting from the top in both cases. Since the parameters of our input embedding matrix and output classification matrix are shared, we update them throughout fine-tuning. Recall that our baseline model consists of $12$ layers each in the encoder and decoder and is fine-tuned for $2^{18}$ steps. As such, we subdivide the fine-tuning process into $12$ episodes of $\\sfrac{2^{18}}{12}$ steps each and train from layers $12 - n$ to $12$ in the $n$th episode. We note that \\cite{howard2018universal} suggested fine-tuning an additional layer after each epoch of training. However, since our supervised data sets vary so much in size and since some of our downstream tasks are actually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler strategy of fine-tuning an additional layer after every $\\sfrac{2^{18}}{12}$ steps.  A comparison of the performance of these fine-tuning approaches is shown in \\cref{tab:finetuning}. For adapter layers, we report the performance using an inner dimensionality $d$ of $32$, $128$, $512$, $2048$. Pursuant with past results \\citep{houlsby2019parameter,bapna2019simple} we find that lower-resource tasks like SQuAD work well with a small value of $d$ whereas higher resource tasks require a large dimensionality to achieve reasonable performance. This suggests that adapter layers could be a promising technique for fine-tuning on fewer parameters as long as the dimensionality is scaled appropriately to the task size. Note that in our case we treat GLUE and SuperGLUE each as a single ``task'' by concatenating their constituent data sets, so although they comprise some low-resource data sets the combined data set is large enough that it necessitates a large value of $d$. We found that gradual unfreezing caused a minor degradation in performance across all tasks, though it did provide some speedup during fine-tuning. Better results may be attainable by more carefully tuning the unfreezing schedule.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Fine-tuning method       & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl All parameters      & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ Adapter layers, $d=32$    & $80.52$ & $15.08$ & $79.32$ & $60.40$ & $13.84$ & $17.88$ & $15.54$ \\\\ Adapter layers, $d=128$   & $81.51$ & $16.62$ & $79.47$ & $63.03$ & $19.83$ & $27.50$ & $22.63$ \\\\ Adapter layers, $d=512$   & $81.54$ & $17.78$ & $79.18$ & $64.30$ & $23.45$ & $33.98$ & $25.81$ \\\\ Adapter layers, $d=2048$  & $81.51$ & $16.62$ & $79.47$ & $63.03$ & $19.83$ & $27.50$ & $22.63$ \\\\ Gradual unfreezing       & $82.50$ & $18.95$ & $79.17$ & $\\*{70.79}$ & $26.71$ & $39.02$ & $26.93$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Comparison of different alternative fine-tuning methods that only update a subset of the model's parameters. For adapter layers, $d$ refers to the inner dimensionality of the adapters. } \\label{tab:finetuning} \\end{table}  \\subsubsection{Multi-task Learning} \\label{sec:multitask}  So far, we have been pre-training our model on a single unsupervised learning task before fine-tuning it individually on each downstream task. An alternative approach, called ``multi-task learning'' \\citep{ruder2017overview,caruana1997multitask}, is to train the model on multiple tasks at a time. This approach typically has the goal of training a single model that can simultaneously perform many tasks at once, i.e.\\ the model and most of its parameters are shared across all tasks. We relax this goal somewhat and instead investigate methods for training on multiple tasks at once in order to eventually produce separate parameter settings that perform well on each individual task. For example, we might train a single model on many tasks, but when reporting performance we are allowed to select a different checkpoint for each task. This loosens the multi-task learning framework and puts it on more even footing compared to the pre-train-then-fine-tune approach we have considered so far. We also note that in our unified text-to-text framework, ``multi-task learning'' simply corresponds to mixing data sets together. It follows that we can still train on unlabeled data when using multi-task learning by treating the unsupervised task as one of the tasks being mixed together. In contrast, most applications of multi-task learning to NLP add task-specific classification networks or use different loss functions for each task \\citep{liu2019multi}.  As pointed out by \\cite{arivazhagan2019massively}, an extremely important factor in multi-task learning is how much data from each task the model should be trained on. Our goal is to not under- or over-train the model---that is, we want the model to see enough data from a given task that it can perform the task well, but not to see so much data that it memorizes the training set. How exactly to set the proportion of data coming from each task can depend on various factors including data set sizes, the ``difficulty'' of learning the task (i.e.\\ how much data the model must see before being able to perform the task effectively), regularization, etc. An additional issue is the potential for ``task interference'' or ``negative transfer'', where achieving good performance on one task can hinder performance on another. Given these concerns, we begin by exploring various strategies for setting the proportion of data coming from each task. A similar exploration was performed by \\cite{wang2019can}.  \\begin{description} \\item[Examples-proportional mixing] A major factor in how quickly a model will overfit to a given task is the task's data set size. As such, a natural way to set the mixing proportions is to sample in proportion to the size of each task's data set. This is equivalent to concatenating the data sets for all tasks and randomly sampling examples from the combined data set. Note, however, that we are including our unsupervised denoising task, which uses a data set that is orders of magnitude larger than every other task's. It follows that if we simply sample in proportion to each data set's size, the vast majority of the data the model sees will be unlabeled, and it will undertrain on all of the supervised tasks. Even without the unsupervised task, some tasks (e.g.\\ WMT English to French) are so large that they would similarly crowd out most of the batches. To get around this issue, we set an artificial ``limit'' on the data set sizes before computing the proportions. Specifically, if the number of examples in each of our $N$ task's data sets is $e_n, n \\in \\{1, \\ldots, N\\}$ then we set probability of sampling an example from the $m$th task during training to $r_m = \\min(e_m, K)/\\sum \\min(e_n, K)$ where $K$ is the artificial data set size limit.  \\item[Temperature-scaled mixing] An alternative way of mitigating the huge disparity between data set sizes is to adjust the ``temperature'' of the mixing rates. This approach was used by multilingual BERT to ensure that the model was sufficiently trained on low-resource languages.\\footnote{\\url{https://github.com/google-research/bert/blob/master/multilingual.md}} To implement temperature scaling with temperature $T$, we raise each task's mixing rate $r_m$ to the power of \\sfrac{1}{$T$} and renormalize the rates so that they sum to 1. When $T = 1$, this approach is equivalent to examples-proportional mixing and as $T$ increases the proportions become closer to equal mixing. We retain the data set size limit $K$ (applied to obtain $r_m$ before temperature scaling) but set it to a large value of $K = 2^{21}$. We use a large value of $K$ because increasing the temperature will decrease the mixing rate of the largest data sets.  \\item[Equal mixing] In this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the data sets we train on. This is most likely a suboptimal strategy, as the model will overfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly include it as a point of reference of what might go wrong when the proportions are set suboptimally.  \\end{description}   To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: $2^{19} + 2^{18} = 786{,}432$. The results are shown in \\cref{tab:multitask}.  In general, we find that multi-task training underperforms pre-training followed by fine-tuning on most tasks. The ``equal'' mixing strategy in particular results in dramatically degraded performance, which may be because the low-resource tasks have overfit, the high-resource tasks have not seen enough data, or the model has not seen enough unlabeled data to learn general-purpose language capabilities. For examples-proportional mixing, we find that for most tasks there is a ``sweet spot'' for $K$ where the model obtains the best performance, and larger or smaller values of $K$ tend to result in worse performance. The exception (for the range of $K$ values we considered) was WMT English to French translation, which is such a high-resource task that it always benefits from a higher mixing proportion. Finally, we note that temperature-scaled mixing also provides a means of obtaining reasonable performance from most tasks, with $T = 2$ performing the best in most cases. The finding that a multi-task model is outperformed by separate models trained on each individual task has previously been observed e.g.\\ by \\citet{arivazhagan2019massively} and \\citet{mccann2018natural}, though it has been shown that the multi-task setup can confer benefits across very similar tasks \\cite{liu2019multi,ratner2018snorkel}. In the following section, we explore ways to close the gap between multi-task training and the pre-train-then-fine-tune approach.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Mixing strategy                     & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Baseline (pre-train/fine-tune) & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $\\*{39.82}$ & $\\*{27.65}$ \\\\ Equal                               & $76.13$     & $19.02$     & $76.51$     & $63.37$     & $23.89$     & $34.31$     & $26.78$ \\\\ Examples-proportional, $K = 2^{16}$ & $80.45$     & $19.04$     & $77.25$     & $69.95$     & $24.35$     & $34.99$     & $27.10$ \\\\ Examples-proportional, $K = 2^{17}$ & $81.56$     & $19.12$     & $77.00$     & $67.91$     & $24.36$     & $35.00$     & $27.25$ \\\\ Examples-proportional, $K = 2^{18}$ & $81.67$     & $19.07$     & $78.17$     & $67.94$     & $24.57$     & $35.19$     & $27.39$ \\\\ Examples-proportional, $K = 2^{19}$ & $81.42$     & $\\*{19.24}$ & $79.78$     & $67.30$     & $25.21$     & $36.30$     & $\\*{27.76}$ \\\\ Examples-proportional, $K = 2^{20}$ & $80.80$     & $\\*{19.24}$ & $\\*{80.36}$ & $67.38$     & $25.66$     & $36.93$     & $\\*{27.68}$ \\\\ Examples-proportional, $K = 2^{21}$ & $79.83$     & $18.79$     & $79.50$     & $65.10$     & $25.82$     & $37.22$     & $27.13$ \\\\ Temperature-scaled, $T = 2$         & $81.90$     & $\\*{19.28}$ & $79.42$     & $69.92$     & $25.42$     & $36.72$     & $27.20$ \\\\ Temperature-scaled, $T = 4$         & $80.56$     & $\\*{19.22}$ & $77.99$     & $69.54$     & $25.04$     & $35.82$     & $27.45$ \\\\ Temperature-scaled, $T = 8$         & $77.21$     & $19.10$     & $77.14$     & $66.07$     & $24.55$     & $35.35$     & $27.17$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Comparison of multi-task training using different mixing strategies. Examples-proportional mixing refers to sampling examples from each data set according to the total size of each data set, with an artificial limit ($K$) on the maximum data set size. Temperature-scaled mixing re-scales the sampling rates by a temperature $T$. For temperature-scaled mixing, we use an artificial data set size limit of $K = 2^{21}$. } \\label{tab:multitask} \\end{table}  \\subsubsection{Combining Multi-Task Learning with Fine-Tuning} \\label{sec:mtft}  Recall that we are studying a relaxed version of multi-task learning where we train a single model on a mixture of tasks but are allowed to evaluate performance using different parameter settings (checkpoints) for the model. We can extend this approach by considering the case where the model is pre-trained on all tasks at once but is then fine-tuned on the individual supervised tasks. This is the method used by the ``MT-DNN'' \\citep{liu2015representation,liu2019multi}, which achieved state-of-the-art performance on GLUE and other benchmarks when it was introduced. We consider three variants of this approach: In the first, we simply pre-train the model on an examples-proportional mixture with an artificial data set size limit of $K=2^{19}$ before fine-tuning it on each individual downstream task. This helps us measure whether including the supervised tasks alongside the unsupervised objective during pre-training gives the model some beneficial early exposure to the downstream tasks. We might also hope that mixing in many sources of supervision could help the pre-trained model obtain a more general set of ``skills'' (loosely speaking) before it is adapted to an individual task. To measure this directly, we consider a second variant where we pre-train the model on the same examples-proportional mixture (with $K=2^{19}$) except that we omit one of the downstream tasks from this pre-training mixture. Then, we fine-tune the model on the task that was left out during pre-training. We repeat this for each of the downstream tasks we consider. We call this approach ``leave-one-out'' multi-task training. This simulates the real-world setting where a pre-trained model is fine-tuned on a task it had not seen during pre-training. Note that multi-task pre-training provides a diverse mixture of supervised tasks. Since other fields (e.g.\\ computer vision \\citep{oquab2014learning,jia2014caffe,huh2016makes,yosinski2014transferable}) use a supervised data set for pre-training, we were interested to see whether omitting the unsupervised task from the multi-task pre-training mixture still produced good results. For our third variant we therefore pre-train on an examples-proportional mixture of all of the supervised tasks we consider with $K = 2^{19}$. In all of these variants, we follow our standard procedure of pre-training for $2^{19}$ steps before fine-tuning for $2^{18}$ steps.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Training strategy                             & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Unsupervised pre-training + fine-tuning  & $\\*{83.28}$ & $\\*{19.24}$ & $\\*{80.88}$ & $\\*{71.36}$ & $\\*{26.98}$ & $39.82$     & $27.65$ \\\\ Multi-task training                            & $81.42$     & $\\*{19.24}$ & $79.78$     & $67.30$     & $25.21$     & $36.30$     & $27.76$ \\\\ Multi-task pre-training + fine-tuning          & $\\*{83.11}$ & $\\*{19.12}$ & $\\*{80.26}$ & $\\*{71.03}$ & $\\*{27.08}$ & $39.80$     & $\\*{28.07}$ \\\\ Leave-one-out multi-task training              & $81.98$     & $19.05$     & $79.97$     & $\\*{71.68}$ & $\\*{26.93}$ & $39.79$     & $\\*{27.87}$      \\\\ Supervised multi-task pre-training             & $79.93$     & $18.96$     & $77.38$     & $65.36$     & $26.81$     & $\\*{40.13}$ & $\\*{28.04}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Comparison of unsupervised pre-training, multi-task learning, and various forms of multi-task pre-training. } \\label{tab:multitask_ft} \\end{table}  We compare the results of these approaches in \\cref{tab:multitask_ft}. For comparison, we also include results for our baseline (pre-train then fine-tune) and for standard multi-task learning (without fine-tuning) on an examples-proportional mixture with $K=2^{19}$. We find that fine-tuning after multi-task pre-training results in comparable performance to our baseline. This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates described in \\cref{sec:multitask}. Interestingly, the performance of ``leave-one-out'' training was only slightly worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e.\\ multi-task pre-training might not result in a dramatic task interference). Finally, supervised multi-task pre-training performed significantly worse in every case except for the translation tasks. This could suggest that the translation tasks benefit less from (English) pre-training, whereas unsupervised pre-training is an important factor in the other tasks.  "
            },
            {
                "section_name": "Scaling_10",
                "paragraphs": "\\label{sec:scaling}  The ``bitter lesson'' of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise \\citep{sutton2019bitter,hestness2017deep,shazeer2017outrageously,jozefowicz2016exploring,mahajan2018exploring,shazeer2018mesh,shazeer2017outrageously,huang2018gpipe,keskar2019ctrl}. Recent results suggest that this may hold true for transfer learning in NLP \\citep{liu2019roberta,radford2019language,yang2019xlnet,lan2019albert}, i.e.\\ it has repeatedly been shown that scaling up produces improved performance compared to more carefully-engineered methods. However, there are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling. In this section, we compare these different approaches by addressing the following premise: ``You were just given $4\\times$ more compute. How should you use it?''  We start with our baseline model, which has $220\\mathrm{M}$ parameters and is pre-trained and fine-tuned for $2^{19}$ and $2^{18}$ steps respectively. The encoder and decoder are both sized similarly to ``BERT\\textsubscript{BASE}''. To experiment with increased model size, we follow the guidelines of ``BERT\\textsubscript{LARGE}'' \\cite{devlin2018bert} and use $d_{\\mathrm{ff}} = 4096$, $d_{\\mathrm{model}} = 1024$, $d_{\\mathrm{kv}} = 64$ and $16$-head attention mechanisms. We then generate two variants with $16$ and $32$ layers each in the encoder and decoder, producing models with $2\\times$ and $4\\times$ as many parameters as our original model. These two variants also have a roughly $2\\times$ and $4\\times$ the computational cost. Using our baseline and these two larger models, we consider three ways of using $4\\times$ as much computation: Training for $4\\times$ as many steps, training for $2\\times$ as many steps with the $2\\times$ bigger model, and training the $4\\times$ bigger model for the ``baseline'' number of training steps. When we increase the training steps, we scale both the pre-train and fine-tune steps for simplicity. Note that when increasing the number of pre-training steps, we are effectively including more pre-training data as C4 is so large that we do not complete one pass over the data even when training for $2^{23}$ steps.  An alternative way for the model to see $4\\times$ as much data is to increase the batch size by a factor of $4$. This can potentially result in faster training due to more efficient parallelization. However, training with a $4\\times$ larger batch size can yield a different outcome than training for $4\\times$ as many steps \\citep{shallue2018measuring}. We include an additional experiment where we train our baseline model with a $4\\times$ larger batch size to compare these two cases.  It is common practice on many of the benchmarks we consider to eke out additional performance by training and evaluating using an ensemble of models. This provides an orthogonal way of using additional computation. To compare other scaling methods to ensembling, we also measure the performance of an ensemble of $4$ separately pre-trained and fine-tuned models. We average the logits across the ensemble before feeding them into the output $\\mathrm{softmax}$ nonlinearity to obtain an aggregate prediction. Instead of pre-training $4$ separate models, a cheaper alternative is to take a single pre-trained model and produce $4$ separate fine-tuned versions. While this does not use our entire $4\\times$ computational budget, we also include this method to see if it produces competitive performance to the other scaling methods.   \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Scaling strategy                         & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Baseline                            & $83.28$     & $19.24$     & $80.88$     & $71.36$     & $26.98$     & $39.82$     & $27.65$ \\\\ $1\\times$ size, $4\\times$ training steps & $85.33$     & $19.33$     & $82.45$     & $74.72$     & $27.08$     & $40.66$     & $27.93$ \\\\ $1\\times$ size, $4\\times$ batch size     & $84.60$     & $19.42$     & $82.52$     & $74.64$     & $27.07$     & $40.60$     & $27.84$ \\\\ $2\\times$ size, $2\\times$ training steps & $\\*{86.18}$ & $19.66$     & $\\*{84.18}$ & $77.18$     & $27.52$     & $\\*{41.03}$ & $28.19$ \\\\ $4\\times$ size, $1\\times$ training steps & $\\*{85.91}$ & $19.73$     & $\\*{83.86}$ & $\\*{78.04}$ & $27.47$     & $40.71$     & $28.10$ \\\\ $4\\times$ ensembled                      & $84.77$     & $\\*{20.10}$ & $83.09$     & $71.74$     & $\\*{28.05}$ & $40.53$     & $\\*{28.57}$  \\\\ $4\\times$ ensembled, fine-tune only      & $84.05$     & $19.57$     & $82.36$     & $71.55$     & $27.55$     & $40.22$     & $28.09$  \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Comparison of different methods of scaling up our baseline model. All methods except ensembling fine-tuned models use $4\\times$ the computation as the baseline. ``Size'' refers to the number of parameters in the model and ``training time'' refers to the number of steps used for both pre-training and fine-tuning. } \\label{tab:scaling} \\end{table}  The performance achieved after applying these various scaling methods is shown in \\cref{tab:scaling}. Unsurprisingly, increasing the training time and/or model size consistently improves the baseline. There was no clear winner between training for $4\\times$ as many steps or using a $4\\times$ larger batch size, though both were beneficial. In general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size. We did not observe a large difference between training a $2\\times$ bigger model for $2\\times$ as long and training a $4\\times$ bigger model on any of the tasks we studied. This suggests that increasing the training time and increasing the model size can be complementary means of improving performance. Our results also suggest that ensembling provides an orthogonal and effective means of improving performance through scale. In some tasks (CNN/DM, WMT English to German, and WMT English to Romanian), ensembling $4$ completely separately trained models significantly outperformed every other scaling approach. Ensembling models that were pre-trained together but fine-tuned separately also gave a substantial performance increase over the baseline, which suggests a cheaper means of improving performance. The only exception was SuperGLUE, where neither ensembling approach significantly improved over the baseline.  We note that different scaling methods have different trade-offs that are separate from their performance. For example, using a larger model can make downstream fine-tuning and inference more expensive. In contrast, the cost of pre-training a small model for longer is effectively amortized if it is applied to many downstream tasks. Separately, we note that ensembling $N$ separate models has a similar cost to using a model that has an $N\\times$ higher computational cost. As a result, some consideration for the eventual use of the model is important when choosing between scaling methods.  "
            },
            {
                "section_name": "Putting It All Together_11",
                "paragraphs": "\\label{sec:together}  We now leverage the insights from our systematic study to determine how far we can push performance on popular NLP benchmarks. We are also interested in exploring the current limits of transfer learning for NLP by training larger models on large amounts of data. We start with our baseline training approach and make the following changes:  \\begin{description}  \\item[Objective] We swap out the i.i.d.\\ denoising objective in our baseline for the span-corruption objective described in \\cref{sec:objective_spans}, which was loosely inspired by SpanBERT \\citep{joshi2019spanbert}. Specifically, we use a mean span length of $3$ and corrupt $15\\%$ of the original sequence. We found that this objective produced marginally better performance (\\cref{tab:objectives_span}) while being slightly more computationally efficient due to shorter target sequence lengths.  \\item[Longer training] Our baseline model uses a relatively small amount of pre-training (\\sfrac{1}{4} as much as BERT \\citep{devlin2018bert}, \\sfrac{1}{16} as much as XLNet \\citep{yang2019xlnet}, \\sfrac{1}{64} as much as RoBERTa \\citep{liu2019roberta}, etc.). Fortunately, C4 is big enough that we can train for substantially longer without repeating data (which can be detrimental, as shown in \\cref{sec:datasets_take}). We found in \\cref{sec:scaling} that additional pre-training can indeed be helpful, and that both increasing the batch size and increasing the number of training steps can confer this benefit. We therefore pre-train our models for $1$ million steps on a batch size of $2^{11}$ sequences of length $512$, corresponding to a total of about $1$ trillion pre-training tokens (about $32\\times$ as many as our baseline). In \\cref{sec:datasets_comparison}, we showed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC data sets outperformed pre-training on C4 on a few downstream tasks. However, these data set variants are sufficiently small that they would be repeated hundreds of times over the course of pre-training on $1$ trillion tokens. Since we showed in \\cref{sec:datasets_take} that this repetition could be harmful, we opted instead to continue using the C4 data set.  \\item[Model sizes] In \\cref{sec:scaling} we also showed how scaling up the baseline model size improved performance. However, using smaller models can be helpful in settings where limited computational resources are available for fine-tuning or inference. Based on these factors, we train models with a wide range of sizes: \\begin{itemize} \\item \\textbf{Base.} This is our baseline model, whose hyperparameters are described in \\cref{sec:model_hparams}. It has roughly $220$ million parameters. \\item \\textbf{Small.} We consider a smaller model, which scales the baseline down by using $d_{\\mathrm{model}} = 512$, $d_{\\mathrm{ff}} = 2{,}048$, $8$-headed attention, and only $6$ layers each in the encoder and decoder. This variant has about $60$ million parameters. \\item \\textbf{Large.} Since our baseline uses a BERT\\textsubscript{BASE}-sized encoder and decoder, we also consider a variant where the encoder and decoder are both similar in size and structure to BERT\\textsubscript{LARGE}. Specifically, this variant uses $d_{\\mathrm{model}} = 1{,}024$, $d_{\\mathrm{ff}} = 4{,}096$, $d_{\\mathrm{kv}} = 64$, $16$-headed attention, and $24$ layers each in the encoder and decoder, resulting in around $770$ million parameters. \\item \\textbf{3B and 11B.} To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use $d_{\\mathrm{model}} = 1024$, a $24$ layer encoder and decoder, and $d_{\\mathrm{kv}} = 128$. For the ``3B'' variant, we use $d_{\\mathrm{ff}} = 16{,}384$ with $32$-headed attention, which results in around $2.8$ billion parameters; for ``11B'' we use $d_{\\mathrm{ff}} = 65{,}536$ with $128$-headed attention producing a model with about $11$ billion parameters. We chose to scale up $d_{\\mathrm{ff}}$ specifically because modern accelerators (such as the TPUs we train our models on) are most efficient for large dense matrix multiplications like those in the Transformer's feed-forward networks. \\end{itemize}  \\item[Multi-task pre-training] In \\cref{sec:mtft}, we showed that pre-training on a multi-task mixture of unsupervised and supervised tasks before fine-tuning worked as well as pre-training on the unsupervised task alone. This is the approach advocated by the ``MT-DNN'' \\citep{liu2015representation,liu2019multi}. It also has the practical benefit of being able to monitor ``downstream'' performance for the entire duration of training, rather than just during fine-tuning. We therefore used multi-task pre-training in our final set of experiments. We hypothesize that larger models trained for longer might benefit from a larger proportion of unlabeled data because they are more likely to overfit to smaller training data sets. However, we also note that the results of \\cref{sec:mtft} suggest that fine-tuning after multi-task pre-training can mitigate some of the issues that might arise from choosing a suboptimal proportion of unlabeled data. Based on these ideas, we substitute the following artificial data set sizes for our unlabeled data before using standard example-proportional mixing (described in \\cref{sec:multitask}): $710{,}000$ for Small, $2{,}620{,}000$ for Base, $8{,}660{,}000$ for Large, $33{,}500{,}000$ for 3B, and $133{,}000{,}000$ for 11B. For all model variants, we also capped the effective data set size of the WMT English to French and WMT English to German data sets to $1\\mathrm{M}$ examples during pre-training.  \\item[Fine-tuning on individual GLUE and SuperGLUE tasks] So far, when fine-tuning on GLUE and SuperGLUE, we have concatenated all of the data sets in each benchmark so that we only fine-tune models once for GLUE and once for SuperGLUE. This approach makes our study logistically simpler, but we found that this sacrifices a small amount of performance on some tasks compared to fine-tuning on the task separately. A potential issue with fine-tuning on individual tasks, which would otherwise be mitigated by training on all tasks at once, is that we might overfit quickly to low-resource tasks. For example, our large batch size of $2^{11}$ length-$512$ sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of $8$ length-$512$ sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every $1{,}000$ steps rather than every $5{,}000$ steps to ensure we have access to the model's parameters before it overfits.  \\item[Beam search] All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search \\citep{sutskever2014sequence}. Specifically, we use a beam width of $4$ and a length penalty of $\\alpha = 0.6$ \\citep{wu2016google} for the WMT translation and CNN/DM summarization tasks.  \\item[Test set] Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using \\texttt{newstest2014} for English-German, \\texttt{newstest2015} for English-French, and \\texttt{newstest2016} for English-Romanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.\\footnote{\\url{http://gluebenchmark.com}}\\textsuperscript{,}\\footnote{\\url{http://super.gluebenchmark.com}} For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art.  \\end{description}  Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.). For reference, these details are described in \\cref{sec:setup}.  \\begin{table} \\footnotesize \\begin{widepage} \\centering  \\begin{tabular}{l c c c c c c c} \\toprule & GLUE       & CoLA       & SST-2         & MRPC          & MRPC          & STS-B         & STS-B    \\\\ Model         & Average    & Matthew's  & Accuracy      & F1            & Accuracy      & Pearson       & Spearman \\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-8} Previous best & $89.4$\\xa  & $69.2$\\xb     & $97.1$\\xa    & $\\*{93.6}$\\xb & $\\*{91.5}$\\xb  & $92.7$\\xb  & $92.3$\\xb   \\\\ \\b\\\\ \\\\ T5-Small      & $77.4$     & $41.0$        & $91.8$        & $89.7$        & $86.6$        & $85.6$     & $85.0$      \\\\ T5-Base       & $82.7$     & $51.1$        & $95.2$        & $90.7$        & $87.5$        & $89.4$     & $88.6$      \\\\ T5-Large      & $86.4$     & $61.2$        & $96.3$        & $92.4$        & $89.9$        & $89.9$     & $89.2$      \\\\ T5-3B         & $88.5$     & $67.1$        & $97.4$        & $92.5$        & $90.0$        & $90.6$     & $89.8$      \\\\ T5-11B        & $\\*{90.3}$ & $\\*{71.6}$    & $\\*{97.5}$    & $92.8$        & $90.4$        & $\\*{93.1}$ & $\\*{92.8}$  \\\\ \\bottomrule \\end{tabular}  \\vspace{0.2em}  \\begin{tabular}{l c c c c c c c} & QQP           & QQP           & MNLI-m     & MNLI-mm     & QNLI         & RTE       & WNLI     \\\\ Model         & F1            & Accuracy      & Accuracy   & Accuracy    & Accuracy     & Accuracy  & Accuracy \\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-8} Previous best & $74.8$\\xc     & $\\*{90.7}$\\xb & $91.3$\\xa  & $91.0$\\xa  & $\\*{99.2}$\\xa & $89.2$\\xa  & $91.8$\\xa \\\\ \\b\\\\ \\\\ T5-Small      & $70.0$        & $88.0$        & $82.4$     & $82.3$     & $90.3$       & $69.9$     & $69.2$        \\\\ T5-Base       & $72.6$        & $89.4$        & $87.1$     & $86.2$     & $93.7$       & $80.1$     & $78.8$        \\\\ T5-Large      & $73.9$        & $89.9$        & $89.9$     & $89.6$     & $94.8$       & $87.2$     & $85.6$        \\\\ T5-3B         & $74.4$        & $89.7$        & $91.4$     & $91.2$     & $96.3$       & $91.1$     & $89.7$        \\\\ T5-11B        & $\\*{75.1}$    & $90.6$        & $\\*{92.2}$ & $\\*{91.9}$ & $96.9$       & $\\*{92.8}$ & $\\*{94.5}$    \\\\ \\bottomrule \\end{tabular}  \\vspace{0.2em}  \\begin{tabular}{l c c c c c c c} & SQuAD       & SQuAD       & SuperGLUE & BoolQ       & CB          & CB        & COPA     \\\\ Model         & EM          & F1          & Average   & Accuracy    & F1          & Accuracy  & Accuracy \\\\ \\cmidrule(r){1-1} \\cmidrule(lr){2-3} \\cmidrule(l){4-8} Previous best & $90.1$\\xa   & $95.5$\\xa   & $84.6$\\xd  & $87.1$\\xd  & $90.5$\\xd  & $95.2$\\xd  & $90.6$\\xd \\\\ \\b\\\\ \\\\ T5-Small      & $79.10$     & $87.24$     & $63.3$     & $76.4$     & $56.9$     & $81.6$     & $46.0$     \\\\ T5-Base       & $85.44$     & $92.08$     & $76.2$     & $81.4$     & $86.2$     & $94.0$     & $71.2$     \\\\ T5-Large      & $86.66$     & $93.79$     & $82.3$     & $85.4$     & $91.6$     & $94.8$     & $83.4$     \\\\ T5-3B         & $88.53$     & $94.95$     & $86.4$     & $89.9$     & $90.3$     & $94.4$     & $92.0$     \\\\ T5-11B        & $\\*{91.26}$ & $\\*{96.22}$ & $\\*{88.9}$ & $\\*{91.2}$ & $\\*{93.9}$ & $\\*{96.8}$ & $\\*{94.8}$ \\\\ \\bottomrule \\end{tabular}  \\vspace{0.2em}  \\begin{tabular}{l c c c c c c c} & MultiRC     & MultiRC   & ReCoRD     & ReCoRD      & RTE       & WiC        & WSC \\\\ Model         & F1a         & EM        & F1         & Accuracy    & Accuracy  & Accuracy   & Accuracy\\\\ \\cmidrule(r){1-1} \\cmidrule(l){2-8} Previous best & $84.4$\\xd  & $52.5$\\xd  & $90.6$\\xd  & $90.0$\\xd  & $88.2$\\xd  & $69.9$\\xd  & $89.0$\\xd \\\\ \\b\\\\ \\\\ T5-Small      & $69.3$     & $26.3$     & $56.3$     & $55.4$     & $73.3$     & $66.9$     & $70.5$     \\\\ T5-Base       & $79.7$     & $43.1$     & $75.0$     & $74.2$     & $81.5$     & $68.3$     & $80.8$     \\\\ T5-Large      & $83.3$     & $50.7$     & $86.8$     & $85.9$     & $87.8$     & $69.3$     & $86.3$     \\\\ T5-3B         & $86.8$     & $58.3$     & $91.2$     & $90.4$     & $90.7$     & $72.1$     & $90.4$     \\\\ T5-11B        & $\\*{88.1}$ & $\\*{63.3}$ & $\\*{94.1}$ & $\\*{93.4}$ & $\\*{92.5}$ & $\\*{76.9}$ & $\\*{93.8}$ \\\\ \\bottomrule \\end{tabular}  \\vspace{0.2em}  \\begin{tabular}{l c c c c c c} & WMT EnDe      & WMT EnFr      & WMT EnRo      & CNN/DM      & CNN/DM      & CNN/DM  \\\\ Model         & BLEU          & BLEU          & BLEU          & ROUGE-1     & ROUGE-2     & ROUGE-L \\\\ \\cmidrule(r){1-1} \\cmidrule(lr){2-4} \\cmidrule(l){5-7} Previous best & $\\*{33.8}$\\xe & $\\*{43.8}$\\xe & $\\*{38.5}$\\xf & $43.47$\\xg  & $20.30$\\xg  & $40.63$\\xg \\\\ \\b\\\\ \\\\ T5-Small      & $26.7$        & $36.0$        & $26.8$        & $41.12$     & $19.56$     & $38.35$     \\\\ T5-Base       & $30.9$        & $41.2$        & $28.0$        & $42.05$     & $20.34$     & $39.40$     \\\\ T5-Large      & $32.0$        & $41.5$        & $28.1$        & $42.50$     & $20.68$     & $39.75$     \\\\ T5-3B         & $31.8$        & $42.6$        & $28.2$        & $42.72$     & $21.02$     & $39.94$     \\\\ T5-11B        & $32.1$        & $43.4$        & $28.1$        & $\\*{43.52}$ & $\\*{21.55}$ & $\\*{40.69}$ \\\\ \\bottomrule \\end{tabular}  \\end{widepage} \\caption{ Performance of our T5 variants on every task we study. Small, Base, Large, 3B, and 11B refer to model configurations with $60$ million, $220$ million, $770$ million, $3$ billion, and $11$ billion parameters, respectively. In the first row of each table, we report the state-of-the-art for the task (as of October 24th, 2019), with the superscript denoting its source with references listed at the end of this caption. All results are reported on the test set except for SQuAD where we use the validation set. $^a$\\citep{lan2019albert} $^b$\\citep{wang2019structbert} $^c$\\citep{zhu2019freelb} $^d$\\citep{liu2019roberta} $^e$\\citep{edunov2018understanding} $^f$\\citep{lample2019cross} $^g$\\citep{dong2019unified} } \\label{tab:final} \\end{table}  The results of this final set of experiments are shown in \\cref{tab:final}. Overall, we achieved state-of-the-art performance on $18$ out of the $24$ tasks we consider. As expected, our largest ($11$ billion parameter) model performed best among our model size variants across all tasks. Our T5-3B model variant did beat the previous state of the art in a few tasks, but scaling the model size to $11$ billion parameters was the most important ingredient for achieving our best performance. We now analyze the results for each individual benchmark.  We achieved a state-of-the-art average GLUE score of $90.3$. Notably, our performance was substantially better than the previous state-of-the-art for the natural language inference tasks MNLI, RTE, and WNLI. RTE and WNLI are two of the tasks where machine performance has historically lagged behind human performance, which is $93.6$ and $95.9$ respectively \\citep{wang2018glue}. In terms of parameter count, our 11B model variant is the largest model that has been submitted to the GLUE benchmark. However, most of the best-scoring submissions use a large amount of ensembling and computation to produce predictions. For example, the best-performing variant of ALBERT \\citep{lan2019albert} uses a model similar in size and architecture to our 3B variant (though it has dramatically fewer parameters due to clever parameter sharing). To produce its impressive performance on GLUE, the ALBERT authors ensembled ``from 6 to 17'' models depending on the task. This likely results in it being more computationally expensive to produce predictions with the ALBERT ensemble than it is with T5-11B.  For SQuAD, we outperformed the previous state-of-the-art (ALBERT \\citep{lan2019albert}) by over one point on the Exact Match score. SQuAD is a long-standing benchmark that was created over three years ago, and most recent improvements have only increased the state-of-the-art by a fraction of a percentage point. We note that when results are reported on the test set, they are typically based on an ensemble of models and/or leverage external data sets (e.g.\\ TriviaQA \\citep{joshi2017triviaqa} or NewsQA \\citep{trischler2016newsqa}) to augment the small SQuAD training set. Human performance on SQuAD is estimated at $82.30$ and $91.22$ for the Exact Match and F1 metric respectively \\citep{rajpurkar2016squad}, so it is not clear if further improvements on this benchmark are meaningful.  For SuperGLUE, we improved upon the state-of-the-art by a large margin (from an average score of $84.6$ \\citep{liu2019roberta} to $88.9$). SuperGLUE was designed to include tasks that were ``beyond the scope of current state-of-the-art systems, but solvable by most college-educated English speakers'' \\citep{wang2019superglue}. We nearly match the human performance of $89.8$ \\citep{wang2019superglue}. Interestingly, on the reading comprehension tasks (MultiRC and ReCoRD) we exceed human performance by a large margin, suggesting the evaluation metrics used for these tasks may be biased towards machine-made predictions. On the other hand, humans achieve $100\\%$ accuracy on both COPA and WSC, which is significantly better than our model's performance. This suggests that there remain linguistic tasks that are hard for our model to perfect, particularly in the low-resource setting.  We did not achieve state-of-the-art performance on any of the WMT translation tasks. This may be in part due to our use of an English-only unlabeled data set. We also note that most of the best results on these tasks use backtranslation \\citep{edunov2018understanding,lample2019cross}, which is a sophisticated data augmentation scheme. The state of the art on the low-resource English to Romanian benchmark also uses additional forms of cross-lingual unsupervised training \\citep{lample2019cross}. Our results suggest that scale and English-language pre-training may be insufficient to match the performance of these more sophisticated methods. On a more specific note, the best results on English to German \\texttt{newstest2014} set use the much larger training set from WMT 2018 \\citep{edunov2018understanding}, making direct comparison to our results difficult.  Finally, on CNN/Daily Mail we attain state-of-the-art performance, though only by a significant amount on the ROUGE-2-F score. It has been shown that improvements to the ROUGE score do not necessarily correspond to more coherent summaries \\citep{paulus2017deep}. Furthermore, while CNN/Daily Mail is posed as an abstractive summarization benchmark, purely extractive approaches have been shown to work well \\citep{liu2019fine}. It has also been argued that generative models trained with maximum likelihood are prone to producing repetitive summaries \\citep{see2017get}. Despite these potential issues, we find that our models do generate coherent and largely correct summaries. We provide some non-cherry-picked validation set examples in \\cref{sec:cnndm_decodes}.  To achieve its strong results, T5 combines insights from our experimental study with unprecedented scale. Note that in \\cref{sec:scaling} we found that scaling up the pre-training amount or size of our baseline model produced substantial gains. Given this, we were interested to measure how much the ``non-scaling'' changes we introduced into T5 contributed to its strong performance. We therefore carried out a final experiment where we compared the following three configurations: First, the standard baseline model, which was pre-trained on $2^{35} \\approx 34\\mathrm{B}$ tokens; second, the baseline trained instead for about 1 trillion tokens (i.e.\\ the same amount of pre-training used for T5), which we refer to as ``baseline-1T''; and third, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the ``non-scaling'' changes we made when designing T5. As such, comparing the performance of these two models gives us a concrete measurement of the impact of the insights from our systematic study.  The performance of these three model configurations is shown in \\cref{tab:insight_impact}. Consistent with the findings in \\cref{sec:scaling}, we find that additional pre-training improves performance over the baseline. Nevertheless, T5-Base substantially outperforms baseline-1T on all downstream tasks. This suggests that scale is not the only factor that contributes to T5's success. We hypothesize that the larger models benefit not only from their increased size but also from these non-scaling factors.  \\begin{table} \\footnotesize \\begin{widepage} \\centering \\begin{tabular}{l c c c c c c c c c c} \\toprule Model          & GLUE        & CNNDM       & SQuAD       & SGLUE       & EnDe        & EnFr        & EnRo    \\\\ \\midrule \\bsl Baseline  & $83.28$     & $19.24$     & $80.88$     & $71.36$     & $26.98$     & $39.82$     & $27.65$ \\\\ Baseline-1T    & $84.80$     & $19.62$     & $83.01$     & $73.90$     & $27.46$     & $40.30$     & $28.34$ \\\\ T5-Base        & $\\*{85.97}$ & $\\*{20.90}$ & $\\*{85.44}$ & $\\*{75.64}$ & $\\*{28.37}$ & $\\*{41.37}$ & $\\*{28.98}$ \\\\ \\bottomrule \\end{tabular} \\end{widepage} \\caption{ Performance comparison of T5-Base to our baseline experimental setup used in the rest of the paper. Results are reported on the validation set. ``Baseline-1T'' refers to the performance achieved by pre-training the baseline model on 1 trillion tokens (the same number used for the T5 model variants) instead of $2^{35} \\approx 34\\mathrm{B}$ tokens (as was used for the baseline). } \\label{tab:insight_impact} \\end{table}  "
            },
            {
                "section_name": "Reflection_4",
                "paragraphs": "\\label{sec:conclusion}  Having completed our systematic study, we wrap up by first recapping some of our most significant findings. Our results provide some high-level perspective on which avenues of research might be more or less promising. To conclude, we outline some topics we think might provide effective approaches for further progressing the field.  "
            },
            {
                "section_name": "Takeaways_12",
                "paragraphs": " \\begin{description}  \\item[Text-to-text] Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS-B. In spite of its simplicity, we found the text-to-text framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale.  \\item[Architectures] While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoder-decoder form worked best in our text-to-text framework. Though an encoder-decoder model uses twice as many parameters as ``encoder-only'' (e.g.\\ BERT) or ``decoder-only'' (language model) architectures, it has a similar computational cost. We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count.  \\item[Unsupervised objectives] Overall, we found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient.  \\item[Data sets] We introduced the ``Colossal Clean Crawled Corpus'' (C4), which comprises heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to data sets that use additional filtering, we found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set. We separately showed that performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks.  \\item[Training strategies] We found that the basic approach of updating all of a pre-trained model's parameters during fine-tuning outperformed methods that are designed to update fewer parameters, although updating all parameters is most expensive. We also experimented with various approaches for training the model on multiple tasks at once, which in our text-to-text setting simply corresponds to mixing examples from different data sets when constructing batches. The primary concern in multi-task learning is setting the proportion of each task to train on. We ultimately did not find a strategy for setting mixing proportions that matched the performance of the basic approach of unsupervised pre-training followed by supervised fine-tuning. However, we found that fine-tuning after pre-training on a mixture of tasks produced comparable performance to unsupervised pre-training.  \\item[Scaling] We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble of models. We found each approach conferred a significant boost in performance, though training a smaller model on more data was often outperformed by training a larger model for fewer steps. We also showed an ensemble of models can provide substantially better results than a single model, which provides an orthogonal means of leveraging additional computation. Ensembling models that were fine-tuned from the same base pre-trained model performed worse than pre-training and fine-tuning all models completely separately, though fine-tune-only ensembling still substantially outperformed a single model.  \\item[Pushing the limits] We combined our above insights and trained substantially larger models (up to $11$ billion parameters) to achieve state-of-the-art results across many of the benchmarks we considered. For unsupervised training, we extracted text from our C4 data set and applied a denoising objective that corrupts contiguous spans of tokens. We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall, our models were trained on over $1$ trillion tokens. In the interest of facilitating the replication, extension, and application of our results, we release our code, the C4 data set, and pre-trained model weights for each T5 variant.\\footnoteref{fn:oss}  \\end{description}  "
            },
            {
                "section_name": "Outlook_13",
                "paragraphs": " \\begin{description}  \\item[The inconvenience of large models] An unsurprising but important result from our study is that larger models tend to perform better. The fact that the hardware used for running these models is continually getting cheaper and more powerful suggests that scaling up may continue to be a promising way to achieve better performance \\citep{sutton2019bitter}. However, it will always be the case that there are applications and scenarios where using a smaller or less expensive model is helpful, for example when performing client-side inference or federated learning \\citep{konevcny2015federated,konevcny2016federated}. Relatedly, one beneficial use of transfer learning is the possibility of attaining good performance on low-resource tasks. Low-resource tasks often occur (by definition) in settings where one lacks the assets to label more data. It follows that low-resource applications often also have limited access to computational resources which can incur additional costs. As a result, we advocate for research on methods that achieve stronger performance with cheaper models so that transfer learning can be applied where it will have the most impact. Some current work along these lines include distillation \\citep{hinton2015distilling,sanh2019distilbert,jiao2019tinybert}, parameter sharing \\citep{lan2019albert}, and conditional computation \\citep{shazeer2017outrageously}.  \\item[More efficient knowledge extraction] Recall that one of the goals of pre-training is (loosely speaking) to provide the model with general-purpose ``knowledge'' that improves its performance on downstream tasks. The method we use in this work, which is currently common practice, is to train the model to denoise corrupted spans of text. We suspect that this simplistic technique may not be a very efficient way to teach the model general-purpose knowledge. More concretely, it would be useful to be able to attain good fine-tuning performance without needing to train our models on $1$ trillion tokens of text first. Some concurrent work along these lines improves efficiency by pre-training a model to distinguish between real and machine-generated text \\citep{clark2020electra}.  \\item[Formalizing the similarity between tasks] We observed that pre-training on unlabeled in-domain data can improve performance on downstream tasks (\\cref{sec:datasets}). This finding mostly relies on basic observations like the fact that SQuAD was created using data from Wikipedia. It would be useful to formulate a more rigorous notion of the ``similarity'' between the pre-training and downstream tasks, so that we could make more principled choices about what source of unlabeled data to use. There is some early empirical work along these lines in the field of computer vision \\citep{huh2016makes,kornblith2018better,he2018rethinking}. A better notion of the relatedness of tasks could also help choose \\textit{supervised} pre-training tasks, which has been shown to be helpful for the GLUE benchmark \\citep{phang2018sentence}.  \\item[Language-agnostic models] We were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks we studied. We also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time. To address these issues, we are interested in further investigating language-agnostic models, i.e.\\ models that can perform a given NLP task with good performance regardless of the text's language. This is an especially pertinent issue given that English is not the native language for the majority of the world's population.  The motivation for this paper was the flurry of recent work on transfer learning for NLP. Before we began this work, these advances had already enabled breakthroughs in settings where learning-based methods had not yet been shown to be effective. We are happy to be able to continue this trend, for example by nearly matching human-level performance on the SuperGLUE benchmark, a task specifically designed to be difficult for modern transfer-learning pipelines. Our results stem from the combination of a straightforward and unified text-to-text framework, our new C4 data set, and insights from our systematic study. Additionally, we provided an empirical overview of the field and a perspective on where it stands. We are excited to see continued work using transfer learning towards the goal of general language understanding.  \\end{description}  \\acks{We thank Grady Simon, Noah Fiedel, Samuel R.\\ Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight.}  \\clearpage  \\appendix  "
            },
            {
                "section_name": "Contributions_5",
                "paragraphs": "\\label{sec:contributions}  Colin designed the scope of this project and wrote this paper, ran all the experiments in \\cref{sec:baseline,sec:architectures,sec:objectives,sec:datasets,sec:transfer,sec:scaling}, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in \\cref{sec:together}. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase. Michael owned all aspects of the Winograd data sets, ingested many of the data sets we used, contributed various improvements and fixes to our infrastructure, and ran some preliminary experiments. Yanqi ran experiments and implemented methods to help settle on a reasonable baseline and helped with the final fine-tuning of the models in \\cref{sec:together}. Wei also helped with final fine-tuning and improved some of our preprocessors. Peter prototyped an early version of the pre-training data set and resolved issues pertaining to the SQuAD and CNN/DM tasks. All authors helped set the scope and research direction we followed in this work.  "
            },
            {
                "section_name": "Converting WNLI to Our Text-to-Text Format_6",
                "paragraphs": "\\label{sec:wnli_preprocessing}  Note that as discussed in \\cref{sec:format}, we do not train on any of the data from WNLI. Instead, when evaluating on the WNLI test set (for the results in \\cref{sec:together}), we convert the WNLI test set to the ``referent noun prediction'' text-to-text format so that we can evaluate using a model trained on WSC and DPR. Our WNLI preprocessor is inspired by the one proposed by \\cite{he2019hybrid}. Recall that examples from WNLI consist of a premise, a hypothesis, and a label that indicates whether the hypothesis is True or False. Using the example from \\cref{sec:format}, the hypothesis would be ``The city councilmen refused the demonstrators a permit because they feared violence.'' with the premise ``The demonstrators feared violence.'' and the label False. We first find the location of all pronouns in the premise (``they'' in our example). Then, we find the maximum number of words that precede or follow each pronoun that are a substring in the hypothesis (``feared violence'' in our example), ignoring case and punctuation. When the premise contains multiple candidate pronouns, we choose the pronoun that is preceded or followed by the largest substring of the hypothesis. We then highlight the pronoun in the premise by surrounding it with asterisks. For the candidate noun (which is compared to our model's prediction to obtain a True or False label), we remove the matching substring from the hypothesis and optionally make it non-possessive (resulting in ``the demonstrators'').  "
            },
            {
                "section_name": "Example Predictions on CNN/Daily Mail_7",
                "paragraphs": "\\label{sec:cnndm_decodes}  To show that our model is generating fluent summaries, we include a few example decodes from our best model (T5-11B) on the validation set along with the ground-truth summaries. These examples selected at random and were not cherry-picked.  \\begin{enumerate} \\item \\textbf{Ground-truth:} \\texttt{leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside - watched by ms moolman .}  \\textbf{Prediction:} \\texttt{leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera .}  \\item \\textbf{Ground-truth:} \\texttt{researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us .}  \\textbf{Prediction:} \\texttt{researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us .}  \\item \\textbf{Ground-truth:} \\texttt{executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn't work . it is 'essential that executions are carried out in a constitutional manner,' said georgia attorney general sam olens .}  \\textbf{Prediction:} \\texttt{the execution of kelly gissendaner, the only woman on georgia's death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country .}  \\item \\textbf{Ground-truth:} \\texttt{dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain .}  \\textbf{Prediction:} \\texttt{dani alves was not selected for brazil's friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter of a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news .}  \\end{enumerate}  "
            },
            {
                "section_name": "Preprocessed Examples_8",
                "paragraphs": "\\label{sec:preprocessing}    In this section, we provide examples of our preprocessing for each of the data sets we consider.  "
            },
            {
                "section_name": "CoLA_14",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Sentence:] \\texttt{John made Bill master of himself.} \\end{description} \\item[Processed input:] \\texttt{cola sentence:\\ John made Bill master of himself.} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{acceptable} \\end{description}  "
            },
            {
                "section_name": "RTE_15",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Sentence 1:] \\texttt{A smaller proportion of Yugoslavia's Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians).} \\item[Sentence 2:] \\texttt{Slovenia has 3,000 inhabitants.} \\end{description} \\item[Processed input:] \\texttt{rte sentence1:\\ A smaller proportion of Yugoslavia's Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). sentence2:\\ Slovenia has 3,000 inhabitants.} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{not\\_entailment} \\end{description}  "
            },
            {
                "section_name": "MNLI_16",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Hypothesis:] \\texttt{The St.\\ Louis Cardinals have always won.} \\item[Premise:] \\texttt{yeah well losing is i mean i'm i'm originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but} \\end{description} \\item[Processed input:] \\texttt{mnli hypothesis:\\ The St. Louis Cardinals have always won. premise:\\ yeah well losing is i mean i'm i'm originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but} \\item[Original target:] \\texttt{2} \\item[Processed target:] \\texttt{contradiction} \\end{description}  "
            },
            {
                "section_name": "MRPC_17",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Sentence 1:] \\texttt{We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .} \\item[Sentence 2:] \\texttt{Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" .} \\end{description} \\item[Processed input:] \\texttt{mrpc sentence1:\\ We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . sentence2:\\ Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" .} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{equivalent} \\end{description}  "
            },
            {
                "section_name": "QNLI_18",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Question:] \\texttt{Where did Jebe die?} \\item[Sentence:] \\texttt{Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand.} \\end{description} \\item[Processed input:] \\texttt{qnli question:\\ Where did Jebe die? sentence:\\ Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand.} \\item[Original target:] \\texttt{0} \\item[Processed target:] \\texttt{entailment} \\end{description}  "
            },
            {
                "section_name": "QQP_19",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Question 1:] \\texttt{What attributes would have made you highly desirable in ancient Rome?} \\item[Question 2:] \\texttt{How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?} \\end{description} \\item[Processed input:] \\texttt{qqp question1:\\ What attributes would have made you highly desirable in ancient Rome? question2:\\ How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?} \\item[Original target:] \\texttt{0} \\item[Processed target:] \\texttt{not\\_duplicate} \\end{description}  "
            },
            {
                "section_name": "SST2_20",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Sentence:] \\texttt{it confirms fincher 's status as a film maker who artfully bends technical know-how to the service of psychological insight . } \\end{description} \\item[Processed input:] \\texttt{sst2 sentence:\\ it confirms fincher 's status as a film maker who artfully bends technical know-how to the service of psychological insight . } \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{positive} \\end{description}  "
            },
            {
                "section_name": "STSB_21",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Sentence 1:] \\texttt{Representatives for Puretunes could not immediately be reached for comment Wednesday.} \\item[Sentence 2:] \\texttt{Puretunes representatives could not be located Thursday to comment on the suit.} \\end{description} \\item[Processed input:] \\texttt{stsb sentence1:\\ Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2:\\ Puretunes representatives could not be located Thursday to comment on the suit.} \\item[Original target:] \\texttt{3.25} \\item[Processed target:] \\texttt{3.2} \\end{description}  "
            },
            {
                "section_name": "CB_22",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Hypothesis:] \\texttt{Valence was helping} \\item[Premise:] \\texttt{Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping?} \\end{description} \\item[Processed input:] \\texttt{cb hypothesis:\\ Valence was helping premise:\\ Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping?} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{contradiction} \\end{description}  "
            },
            {
                "section_name": "COPA_23",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Question:] \\texttt{effect} \\item[Premise:] \\texttt{Political violence broke out in the nation.} \\item[Choice 1:] \\texttt{Many citizens relocated to the capitol.} \\item[Choice 2:] \\texttt{Many citizens took refuge in other territories.} \\end{description} \\item[Processed input:] \\texttt{copa choice1:\\ Many citizens relocated to the capitol. choice2:\\ Many citizens took refuge in other territories. premise:\\ Political violence broke out in the nation. question:\\ effect} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{True} \\end{description}  "
            },
            {
                "section_name": "MultiRC_24",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Answer:] \\texttt{There was only pie to eat, rather than traditional breakfast foods} \\item[Paragraph:] \\texttt{<b>Sent 1:\\ </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2:\\ </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3:\\ </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4:\\ </b>One day, Joey and Jimmy went swimming together at their Aunt Julie's pond.<br><b>Sent 5:\\ </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6:\\ </b>He couldn't find anything to eat except for pie!<br><b>Sent 7:\\ </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8:\\ </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9:\\ </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10:\\ </b>They dove into the water and swam for several hours.<br><b>Sent 11:\\ </b>The sun was out, but the breeze was cold.<br><b>Sent 12:\\ </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13:\\ </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14:\\ </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15:\\ </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\\ </b>The two squirrels ate some food that Joey's mom, Jasmine, made and went off to bed.<br>} \\item[Question:] \\texttt{Why was Joey surprised the morning he woke up for breakfast?} \\end{description} \\item[Processed input:] \\texttt{multirc question:\\ Why was Joey surprised the morning he woke up for breakfast? answer:\\ There was only pie to eat, rather than traditional breakfast foods paragraph:\\ <b>Sent 1:\\ </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2:\\ </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3:\\ </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4:\\ </b>One day, Joey and Jimmy went swimming together at their Aunt Julie's pond.<br><b>Sent 5:\\ </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6:\\ </b>He couldn't find anything to eat except for pie!<br><b>Sent 7:\\ </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8:\\ </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9:\\ </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10:\\ </b>They dove into the water and swam for several hours.<br><b>Sent 11:\\ </b>The sun was out, but the breeze was cold.<br><b>Sent 12:\\ </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13:\\ </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14:\\ </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15:\\ </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\\ </b>The two squirrels ate some food that Joey's mom, Jasmine, made and went off to bed.<br>} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{True} \\end{description}  "
            },
            {
                "section_name": "WiC_25",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[POS:] \\texttt{N} \\item[Sentence 1:] \\texttt{It was the deliberation of his act that was insulting .} \\item[Sentence 2:] \\texttt{The deliberations of the jury .} \\item[Word:] \\texttt{deliberation} \\end{description} \\item[Processed input:] \\texttt{wic pos:\\ N sentence1:\\ It was the deliberation of his act that was insulting . sentence2:\\ The deliberations of the jury . word:\\ deliberation} \\item[Original target:] \\texttt{0} \\item[Processed target:] \\texttt{False} \\end{description}  "
            },
            {
                "section_name": "WSC and DPR_26",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Span 2 text:] \\texttt{it} \\item[Span 1 text:] \\texttt{stable} \\item[Span 2 index:] \\texttt{20} \\item[Span 1 index:] \\texttt{1} \\item[Text:] \\texttt{The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made it pleasant and airy.} \\end{description} \\item[Processed input:] \\texttt{wsc: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.} \\item[Original target:] \\texttt{1} \\item[Processed target:] \\texttt{stable} \\end{description}  "
            },
            {
                "section_name": "CNN/Daily Mail_27",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] \\texttt{marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn\u2019t seem to mind as they later pose on the dance floor with other friends. united haven\u2019t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united's dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .} \\item[Processed input:] \\texttt{summarize:\\ marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn\u2019t seem to mind as they later pose on the dance floor with other friends. united haven\u2019t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united's dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game .} \\item[Original target:] \\texttt{the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday  . red devils will be looking for just their second league away win in seven  . louis van gaal's side currently sit two points clear of liverpool in fourth   .} \\item[Processed target:] \\texttt{the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday  . red devils will be looking for just their second league away win in seven  . louis van gaal's side currently sit two points clear of liverpool in fourth   .} \\end{description}  "
            },
            {
                "section_name": "SQuAD_28",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] ~ \\begin{description}[leftmargin=0.5cm] \\item[Question:] \\texttt{What does increased oxygen concentrations in the patient's lungs displace?} \\item[Context:] \\texttt{Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.} \\end{description} \\item[Processed input:] \\texttt{question:\\ What does increased oxygen concentrations in the patient's lungs displace? context:\\ Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.} \\item[Original target:] \\texttt{carbon monoxide} \\item[Processed target:] \\texttt{carbon monoxide} \\end{description}  "
            },
            {
                "section_name": "WMT English to German_29",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] \\texttt{\"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote.} \\item[Processed input:] \\texttt{translate English to German:\\ \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote.} \\item[Original target:] \\texttt{\"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder vor Gericht landen\", schrieb sie.} \\item[Processed target:] \\texttt{\"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder vor Gericht landen\", schrieb sie.} \\end{description}  "
            },
            {
                "section_name": "WMT English to French_30",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] \\texttt{This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars:\\ the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room.} \\item[Processed input:] \\texttt{translate English to French:\\ This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars:\\ the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room.} \\item[Original target:] \\texttt{Ce d\u00e9tail d'une photographie infrarouge prise par le t\u00e9lescope Spitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d'\u00e9toiles:\\ les plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles \u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d'accouchement de l'univers.} \\item[Processed target:] \\texttt{Ce d\u00e9tail d'une photographie infrarouge prise par le t\u00e9lescope Spitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d'\u00e9toiles:\\ les plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles \u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d'accouchement de l'univers.} \\end{description}  "
            },
            {
                "section_name": "WMT English to Romanian_31",
                "paragraphs": "\\begin{description}[leftmargin=0.5cm] \\item[Original input:] \\texttt{Taco Bell said it plans to add 2,000 locations in the US by 2022.} \\item[Processed input:] \\texttt{translate English to Romanian:\\ Taco Bell said it plans to add 2,000 locations in the US by 2022.} \\item[Original target:] \\texttt{Taco Bell a afirmat c\u0103, p\u00e2n\u0103 \u00een 2022, inten\u021bioneaz\u0103 s\u0103 deschid\u0103 2000 de restaurante \u00een SUA.} \\item[Processed target:] \\texttt{Taco Bell a afirmat c\u0103, p\u00e2n\u0103 \u00een 2022, inten\u021bioneaz\u0103 s\u0103 deschid\u0103 2000 de restaurante \u00een SUA.} \\end{description}  \\b \\h \\\\ \\h \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\h \\e   \\clearpage \\K \\r  "
            },
            {
                "section_name": "Scores on Every Task for All Experiments_9",
                "paragraphs": "\\label{sec:giant}  The following table lists the scores achieved on every task in the experiments described in \\cref{sec:architectures,sec:objectives,sec:datasets,sec:transfer,sec:scaling}.  \\clearpage \\eject \\pdfpagewidth=58cm \\pdfpageheight=39cm \\thispagestyle{empty}  \\begin{table}[!ht] \\centering \\begin{minipage}{0.85\\pdfpagewidth} \\footnotesize \\begin{tabular}{llccccccccccccccccccccccccccccccccc} \\toprule & & \\multicolumn{13}{c}{\\textbf{GLUE}} & & & & & & \\multicolumn{12}{c}{\\textbf{SuperGLUE}} & \\multicolumn{3}{c}{\\textbf{WMT}} \\\\ & & Score & CoLA & SST-2 & MRPC & MRPC & STSB & STSB & QQP & QQP & MNLI\\textsubscript{m} & MNLI\\textsubscript{mm} & QNLI & RTE & \\multicolumn{3}{c}{\\textbf{CNN/DM}} & \\multicolumn{2}{c}{\\textbf{SQuAD}} & Score & BoolQ & CB & CB & COPA & MultiRC & MultiRC & ReCoRD & ReCoRD & RTE & WiC & WSC & EnDe & EnFr & EnRo \\\\ Table & Experiment & Average & MCC & Acc & F1 & Acc & PCC & SCC & F1 & Acc & Acc & Acc & Acc & Acc & R-1-F & R-2-F & R-L-F & EM & F1 & Average & Acc & F1 & Acc & Acc & F1 & EM & F1 & EM & Acc & Acc & Acc & BLEU & BLEU & BLEU \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:baseline} & \\bsl Baseline average & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:baseline} & Baseline standard deviation & $0.235$ & $1.111$ & $0.569$ & $0.729$ & $1.019$ & $0.374$ & $0.418$ & $0.108$ & $0.070$ & $0.291$ & $0.231$ & $0.361$ & $1.393$ & $0.065$ & $0.065$ & $0.058$ & $0.343$ & $0.226$ & $0.416$ & $0.365$ & $3.237$ & $2.560$ & $2.741$ & $0.716$ & $1.011$ & $0.370$ & $0.379$ & $1.228$ & $0.850$ & $2.029$ & $0.112$ & $0.090$ & $0.108$ \\\\ \\ref{tab:baseline} & No pre-training & $66.22$ & $12.29$ & $80.62$ & $81.42$ & $73.04$ & $72.58$ & $72.97$ & $81.94$ & $86.62$ & $68.02$ & $67.98$ & $75.69$ & $58.84$ & $39.19$ & $17.60$ & $36.69$ & $50.31$ & $61.97$ & $53.04$ & $65.38$ & $71.61$ & $76.79$ & $62.00$ & $59.10$ & $0.84$ & $20.33$ & $17.95$ & $54.15$ & $54.08$ & $65.38$ & $25.86$ & $39.77$ & $24.04$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:architectures_results} & \\bsl Enc/dec, denoising & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:architectures_results} & Enc/dec, shared, denoising & $82.81$ & $55.24$ & $91.86$ & $91.58$ & $88.24$ & $87.43$ & $87.58$ & $88.69$ & $91.60$ & $83.88$ & $84.01$ & $90.23$ & $73.65$ & $41.11$ & $18.78$ & $38.48$ & $80.63$ & $88.49$ & $70.73$ & $77.13$ & $95.04$ & $96.43$ & $65.00$ & $66.16$ & $22.98$ & $68.95$ & $68.09$ & $70.76$ & $68.18$ & $75.96$ & $26.72$ & $39.03$ & $27.46$ \\\\ \\ref{tab:architectures_results} & Enc/dec, 6 layers, denoising & $80.88$ & $46.26$ & $92.09$ & $91.51$ & $87.99$ & $87.01$ & $86.76$ & $87.93$ & $90.97$ & $82.20$ & $82.41$ & $88.83$ & $71.48$ & $40.83$ & $18.97$ & $38.31$ & $77.59$ & $86.07$ & $68.42$ & $73.79$ & $91.70$ & $92.86$ & $67.00$ & $61.02$ & $19.62$ & $61.26$ & $60.33$ & $72.20$ & $65.99$ & $75.00$ & $26.38$ & $38.40$ & $26.95$ \\\\ \\ref{tab:architectures_results} & Language model, denoising & $74.70$ & $24.50$ & $90.60$ & $86.08$ & $78.92$ & $85.22$ & $85.42$ & $85.40$ & $88.99$ & $76.72$ & $77.05$ & $86.02$ & $64.62$ & $39.49$ & $17.93$ & $36.91$ & $61.14$ & $71.37$ & $55.02$ & $65.47$ & $60.08$ & $71.43$ & $58.00$ & $43.03$ & $2.94$ & $53.35$ & $52.31$ & $53.07$ & $58.62$ & $63.46$ & $25.09$ & $35.28$ & $25.86$ \\\\ \\ref{tab:architectures_results} & Prefix LM, denoising & $81.82$ & $49.99$ & $92.43$ & $91.43$ & $88.24$ & $87.20$ & $86.98$ & $88.41$ & $91.39$ & $82.32$ & $82.93$ & $88.71$ & $74.01$ & $40.46$ & $18.61$ & $37.90$ & $78.94$ & $87.31$ & $68.11$ & $75.50$ & $93.37$ & $91.07$ & $60.00$ & $63.43$ & $21.20$ & $65.03$ & $64.11$ & $71.48$ & $65.67$ & $73.08$ & $26.43$ & $37.98$ & $27.39$ \\\\ \\ref{tab:architectures_results} & Enc/dec, LM & $79.56$ & $42.03$ & $91.86$ & $91.64$ & $88.24$ & $87.13$ & $87.00$ & $88.21$ & $91.15$ & $81.68$ & $81.66$ & $88.54$ & $65.70$ & $40.67$ & $18.59$ & $38.13$ & $76.02$ & $84.85$ & $64.29$ & $72.23$ & $85.74$ & $89.29$ & $57.00$ & $60.53$ & $16.26$ & $59.28$ & $58.30$ & $65.34$ & $64.89$ & $70.19$ & $26.27$ & $39.17$ & $26.86$ \\\\ \\ref{tab:architectures_results} & Enc/dec, shared, LM & $79.60$ & $44.83$ & $92.09$ & $90.20$ & $85.78$ & $86.03$ & $85.87$ & $87.77$ & $91.02$ & $81.74$ & $82.29$ & $89.16$ & $65.34$ & $40.16$ & $18.13$ & $37.59$ & $76.35$ & $84.86$ & $63.50$ & $70.49$ & $91.41$ & $87.50$ & $55.00$ & $60.21$ & $16.89$ & $57.83$ & $56.73$ & $63.54$ & $63.48$ & $70.19$ & $26.62$ & $39.17$ & $27.05$ \\\\ \\ref{tab:architectures_results} & Enc/dec, 6 layers, LM & $78.67$ & $38.72$ & $91.40$ & $90.40$ & $86.52$ & $86.82$ & $86.49$ & $87.87$ & $91.03$ & $80.99$ & $80.92$ & $88.05$ & $65.70$ & $40.29$ & $18.26$ & $37.70$ & $75.32$ & $84.06$ & $64.06$ & $71.38$ & $85.25$ & $89.29$ & $60.00$ & $57.56$ & $16.79$ & $55.22$ & $54.30$ & $66.79$ & $63.95$ & $71.15$ & $26.13$ & $38.42$ & $26.89$ \\\\ \\ref{tab:architectures_results} & Language model, LM & $73.78$ & $28.53$ & $89.79$ & $85.23$ & $78.68$ & $84.22$ & $84.00$ & $84.88$ & $88.70$ & $74.94$ & $75.77$ & $84.84$ & $58.84$ & $38.97$ & $17.54$ & $36.37$ & $53.81$ & $64.55$ & $56.51$ & $64.22$ & $59.92$ & $71.43$ & $64.00$ & $53.04$ & $1.05$ & $46.81$ & $45.78$ & $58.84$ & $56.74$ & $69.23$ & $25.23$ & $34.31$ & $25.38$ \\\\ \\ref{tab:architectures_results} & Prefix LM, LM & $79.68$ & $41.26$ & $92.09$ & $90.11$ & $86.27$ & $86.82$ & $86.32$ & $88.35$ & $91.35$ & $81.71$ & $82.02$ & $89.04$ & $68.59$ & $39.66$ & $17.84$ & $37.13$ & $76.87$ & $85.39$ & $64.86$ & $71.47$ & $93.37$ & $91.07$ & $57.00$ & $58.67$ & $16.89$ & $59.25$ & $58.16$ & $64.26$ & $66.30$ & $71.15$ & $26.28$ & $37.51$ & $26.76$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:objectives_highlevel} & Language modeling with prefix & $80.69$ & $44.22$ & $93.00$ & $91.68$ & $88.48$ & $87.20$ & $87.18$ & $88.39$ & $91.41$ & $82.66$ & $83.09$ & $89.29$ & $68.95$ & $40.71$ & $18.94$ & $38.15$ & $77.99$ & $86.43$ & $65.27$ & $73.55$ & $83.95$ & $87.50$ & $55.00$ & $59.65$ & $18.89$ & $61.76$ & $60.76$ & $68.59$ & $65.67$ & $73.08$ & $26.86$ & $39.73$ & $27.49$ \\\\ \\ref{tab:objectives_highlevel} & BERT-style \\citep{devlin2018bert} & $82.96$ & $52.49$ & $92.55$ & $92.79$ & $89.95$ & $87.68$ & $87.66$ & $88.47$ & $91.44$ & $83.60$ & $84.05$ & $90.33$ & $75.45$ & $41.27$ & $19.17$ & $38.72$ & $80.65$ & $88.24$ & $69.85$ & $76.48$ & $94.37$ & $94.64$ & $61.00$ & $63.29$ & $25.08$ & $66.76$ & $65.85$ & $72.20$ & $69.12$ & $75.00$ & $26.78$ & $40.03$ & $27.41$ \\\\ \\ref{tab:objectives_highlevel} & Deshuffling & $73.17$ & $22.82$ & $87.16$ & $86.88$ & $81.13$ & $84.03$ & $83.82$ & $86.38$ & $89.90$ & $76.30$ & $76.34$ & $84.18$ & $58.84$ & $40.75$ & $18.59$ & $38.10$ & $67.61$ & $76.76$ & $58.47$ & $69.17$ & $63.70$ & $78.57$ & $56.00$ & $59.85$ & $12.70$ & $45.52$ & $44.36$ & $57.04$ & $64.89$ & $68.27$ & $26.11$ & $39.30$ & $25.62$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:objectives_bert} & BERT-style \\citep{devlin2018bert} & $82.96$ & $52.49$ & $92.55$ & $92.79$ & $89.95$ & $87.68$ & $87.66$ & $88.47$ & $91.44$ & $83.60$ & $84.05$ & $90.33$ & $75.45$ & $41.27$ & $19.17$ & $38.72$ & $80.65$ & $88.24$ & $69.85$ & $76.48$ & $94.37$ & $94.64$ & $61.00$ & $63.29$ & $25.08$ & $66.76$ & $65.85$ & $72.20$ & $69.12$ & $75.00$ & $26.78$ & $40.03$ & $27.41$ \\\\ \\ref{tab:objectives_bert} & MASS-style \\citep{song2019mass} & $82.32$ & $47.01$ & $91.63$ & $92.53$ & $89.71$ & $88.21$ & $88.18$ & $88.58$ & $91.44$ & $82.96$ & $83.67$ & $90.02$ & $77.26$ & $41.16$ & $19.16$ & $38.55$ & $80.10$ & $88.07$ & $69.28$ & $75.08$ & $84.98$ & $89.29$ & $63.00$ & $64.46$ & $23.50$ & $66.71$ & $65.91$ & $72.20$ & $67.71$ & $78.85$ & $26.79$ & $39.89$ & $27.55$ \\\\ \\ref{tab:objectives_bert} & \\bsl Replace corrupted spans & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:objectives_bert} & Drop corrupted tokens & $84.44$ & $60.04$ & $92.89$ & $92.79$ & $89.95$ & $87.28$ & $86.85$ & $88.56$ & $91.54$ & $83.94$ & $83.92$ & $90.74$ & $79.42$ & $41.27$ & $19.31$ & $38.70$ & $80.52$ & $88.28$ & $68.67$ & $75.90$ & $96.02$ & $94.64$ & $56.00$ & $65.06$ & $23.92$ & $65.54$ & $64.60$ & $71.12$ & $67.40$ & $74.04$ & $27.07$ & $39.76$ & $27.82$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:objectives_rate} & Corruption rate = $10\\%$ & $82.82$ & $52.71$ & $92.09$ & $91.55$ & $88.24$ & $88.19$ & $88.15$ & $88.47$ & $91.40$ & $83.50$ & $84.51$ & $90.33$ & $75.45$ & $41.05$ & $19.00$ & $38.53$ & $80.38$ & $88.36$ & $69.55$ & $74.98$ & $92.37$ & $92.86$ & $62.00$ & $66.04$ & $24.66$ & $67.93$ & $67.09$ & $70.76$ & $67.24$ & $75.96$ & $26.87$ & $39.28$ & $27.44$ \\\\ \\ref{tab:objectives_rate} & \\bsl Corruption rate = $15\\%$ & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:objectives_rate} & Corruption rate = $25\\%$ & $83.00$ & $53.47$ & $93.00$ & $92.44$ & $89.46$ & $87.36$ & $87.36$ & $88.68$ & $91.53$ & $84.44$ & $84.15$ & $90.77$ & $74.01$ & $41.69$ & $19.54$ & $39.14$ & $80.96$ & $88.61$ & $70.48$ & $76.39$ & $93.02$ & $92.86$ & $68.00$ & $65.46$ & $24.66$ & $68.20$ & $67.39$ & $73.65$ & $67.87$ & $72.12$ & $27.04$ & $39.83$ & $27.47$ \\\\ \\ref{tab:objectives_rate} & Corruption rate = $50\\%$ & $81.27$ & $46.26$ & $91.63$ & $91.11$ & $87.99$ & $87.87$ & $87.64$ & $88.70$ & $91.57$ & $83.64$ & $84.10$ & $90.24$ & $70.76$ & $41.51$ & $19.32$ & $38.89$ & $79.80$ & $87.76$ & $70.33$ & $75.02$ & $93.05$ & $92.86$ & $68.00$ & $62.97$ & $24.13$ & $64.94$ & $64.13$ & $72.20$ & $68.50$ & $77.88$ & $27.01$ & $39.90$ & $27.49$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:objectives_span} & \\bsl Baseline (i.i.d.) & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:objectives_span}& Average span length = $2$ & $83.54$ & $53.82$ & $92.20$ & $93.05$ & $90.44$ & $87.85$ & $87.71$ & $88.42$ & $91.40$ & $84.28$ & $84.46$ & $90.88$ & $77.62$ & $41.23$ & $19.39$ & $38.69$ & $82.09$ & $89.69$ & $72.20$ & $77.06$ & $90.43$ & $91.07$ & $70.00$ & $66.28$ & $26.13$ & $71.34$ & $70.61$ & $75.45$ & $68.34$ & $78.85$ & $26.76$ & $39.99$ & $27.63$ \\\\ \\ref{tab:objectives_span}& Average span length = $3$ & $83.49$ & $53.90$ & $92.43$ & $92.25$ & $89.46$ & $87.49$ & $87.53$ & $88.72$ & $91.51$ & $84.85$ & $84.84$ & $90.99$ & $77.26$ & $41.50$ & $19.62$ & $38.94$ & $81.84$ & $89.66$ & $72.53$ & $76.85$ & $94.37$ & $94.64$ & $70.00$ & $67.64$ & $28.75$ & $70.84$ & $69.90$ & $74.73$ & $67.71$ & $77.88$ & $26.86$ & $39.65$ & $27.62$ \\\\ \\ref{tab:objectives_span}& Average span length = $5$ & $83.40$ & $52.12$ & $93.12$ & $92.63$ & $89.71$ & $88.70$ & $88.47$ & $88.84$ & $91.64$ & $84.32$ & $84.29$ & $90.79$ & $76.90$ & $41.39$ & $19.24$ & $38.82$ & $82.05$ & $89.79$ & $72.23$ & $77.06$ & $83.06$ & $89.29$ & $69.00$ & $68.16$ & $30.12$ & $71.36$ & $70.53$ & $75.81$ & $69.91$ & $79.81$ & $26.88$ & $39.40$ & $27.53$ \\\\ \\ref{tab:objectives_span}& Average span length = $10$ & $82.85$ & $50.11$ & $92.09$ & $91.95$ & $88.97$ & $88.45$ & $88.22$ & $88.86$ & $91.63$ & $84.34$ & $84.28$ & $91.07$ & $76.17$ & $41.38$ & $19.33$ & $38.80$ & $81.84$ & $89.39$ & $70.44$ & $76.45$ & $87.40$ & $89.29$ & $65.00$ & $66.87$ & $29.59$ & $69.82$ & $68.94$ & $72.56$ & $67.55$ & $75.96$ & $26.79$ & $39.49$ & $27.69$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:datasets} & \\bsl C4 & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:datasets} & C4, unfiltered & $81.46$ & $48.01$ & $91.63$ & $92.72$ & $89.95$ & $87.79$ & $87.60$ & $88.31$ & $91.27$ & $82.30$ & $82.34$ & $88.71$ & $72.20$ & $41.09$ & $19.14$ & $38.54$ & $78.78$ & $87.04$ & $68.04$ & $75.75$ & $89.17$ & $91.07$ & $62.00$ & $65.52$ & $25.60$ & $62.42$ & $61.58$ & $69.68$ & $67.08$ & $72.12$ & $26.55$ & $39.34$ & $27.21$ \\\\ \\ref{tab:datasets} & RealNews-like & $83.83$ & $56.55$ & $92.66$ & $92.06$ & $88.97$ & $87.71$ & $87.37$ & $88.51$ & $91.49$ & $84.35$ & $84.46$ & $90.61$ & $78.34$ & $41.38$ & $19.23$ & $38.84$ & $80.39$ & $88.50$ & $72.38$ & $77.00$ & $93.09$ & $94.64$ & $66.00$ & $65.92$ & $23.82$ & $74.56$ & $73.72$ & $75.81$ & $66.61$ & $80.77$ & $26.75$ & $39.90$ & $27.48$ \\\\ \\ref{tab:datasets} & WebText-like & $84.03$ & $56.38$ & $93.12$ & $92.31$ & $89.22$ & $88.69$ & $88.68$ & $88.65$ & $91.56$ & $84.70$ & $84.84$ & $90.83$ & $77.62$ & $41.23$ & $19.31$ & $38.70$ & $81.42$ & $89.15$ & $71.40$ & $76.88$ & $83.08$ & $89.29$ & $66.00$ & $64.10$ & $24.24$ & $72.24$ & $71.36$ & $75.45$ & $68.03$ & $82.69$ & $26.80$ & $39.74$ & $27.59$ \\\\ \\ref{tab:datasets} & Wikipedia & $81.85$ & $45.53$ & $92.32$ & $91.67$ & $88.24$ & $85.62$ & $86.40$ & $88.37$ & $91.34$ & $82.61$ & $83.25$ & $90.96$ & $77.26$ & $41.39$ & $19.31$ & $38.81$ & $81.29$ & $89.18$ & $68.01$ & $76.12$ & $56.03$ & $80.36$ & $67.00$ & $65.01$ & $25.92$ & $69.03$ & $68.06$ & $74.73$ & $67.08$ & $76.92$ & $26.94$ & $39.69$ & $27.67$ \\\\ \\ref{tab:datasets} & Wikipedia + TBC & $83.65$ & $55.53$ & $92.78$ & $92.41$ & $89.22$ & $86.67$ & $86.27$ & $89.47$ & $92.29$ & $84.38$ & $83.45$ & $91.94$ & $76.90$ & $41.22$ & $19.28$ & $38.67$ & $82.08$ & $89.70$ & $73.24$ & $76.22$ & $95.40$ & $92.86$ & $69.00$ & $51.59$ & $50.93$ & $69.53$ & $68.51$ & $77.62$ & $66.93$ & $81.73$ & $26.77$ & $39.63$ & $27.57$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:datasets_take} & \\bsl Full data set & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:datasets_take} & $2^{29}$ ($64$ repeats) & $82.87$ & $53.82$ & $92.78$ & $91.79$ & $88.73$ & $87.56$ & $87.58$ & $88.73$ & $91.54$ & $84.07$ & $84.21$ & $90.59$ & $73.65$ & $41.18$ & $19.19$ & $38.67$ & $80.97$ & $88.90$ & $72.03$ & $76.76$ & $92.96$ & $92.86$ & $66.00$ & $65.11$ & $26.76$ & $69.35$ & $68.49$ & $75.81$ & $67.24$ & $82.69$ & $26.83$ & $39.74$ & $27.63$ \\\\ \\ref{tab:datasets_take} & $2^{27}$ ($256$ repeats) & $82.62$ & $50.60$ & $92.32$ & $92.07$ & $88.73$ & $87.83$ & $87.60$ & $88.65$ & $91.54$ & $83.43$ & $84.37$ & $90.12$ & $75.81$ & $41.24$ & $19.20$ & $38.70$ & $79.78$ & $87.63$ & $69.97$ & $75.29$ & $93.42$ & $91.07$ & $63.00$ & $61.82$ & $23.61$ & $66.27$ & $65.39$ & $73.65$ & $66.30$ & $80.77$ & $27.02$ & $39.71$ & $27.33$ \\\\ \\ref{tab:datasets_take} & $2^{25}$ ($1{,}024$ repeats) & $79.55$ & $43.84$ & $91.28$ & $89.32$ & $85.05$ & $85.92$ & $85.74$ & $88.05$ & $91.09$ & $81.29$ & $81.72$ & $87.90$ & $69.31$ & $40.66$ & $18.57$ & $38.13$ & $76.27$ & $84.58$ & $64.76$ & $72.63$ & $83.97$ & $82.14$ & $64.00$ & $59.39$ & $17.94$ & $56.94$ & $56.04$ & $64.98$ & $65.20$ & $73.08$ & $26.38$ & $39.56$ & $26.80$ \\\\ \\ref{tab:datasets_take} & $2^{23}$ ($4{,}096$ repeats) & $76.34$ & $32.68$ & $89.45$ & $89.84$ & $86.03$ & $83.49$ & $83.42$ & $87.18$ & $90.61$ & $77.80$ & $78.69$ & $85.47$ & $64.62$ & $40.16$ & $18.33$ & $37.66$ & $70.92$ & $80.20$ & $59.29$ & $69.85$ & $73.48$ & $73.21$ & $56.00$ & $57.66$ & $14.38$ & $46.69$ & $45.79$ & $59.57$ & $65.05$ & $68.27$ & $26.37$ & $38.84$ & $25.81$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:finetuning} & \\bsl All parameters & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:finetuning} & Adapter layers, $d=32$ & $80.52$ & $45.33$ & $91.63$ & $90.59$ & $86.76$ & $88.38$ & $88.06$ & $86.99$ & $90.26$ & $83.63$ & $83.94$ & $90.72$ & $67.15$ & $34.50$ & $15.08$ & $32.15$ & $79.32$ & $87.70$ & $60.40$ & $65.32$ & $50.87$ & $73.21$ & $52.00$ & $58.61$ & $19.41$ & $65.50$ & $64.58$ & $62.09$ & $64.58$ & $73.08$ & $13.84$ & $17.88$ & $15.54$ \\\\ \\ref{tab:finetuning} &  Adapter layers, $d=128$ & $81.51$ & $45.35$ & $92.89$ & $91.49$ & $88.24$ & $87.73$ & $87.65$ & $87.73$ & $90.93$ & $83.64$ & $84.09$ & $90.52$ & $72.56$ & $36.71$ & $16.62$ & $34.37$ & $79.47$ & $87.61$ & $63.03$ & $69.20$ & $52.21$ & $75.00$ & $56.00$ & $61.08$ & $18.05$ & $67.94$ & $66.97$ & $68.59$ & $66.77$ & $73.08$ & $19.83$ & $27.50$ & $22.63$ \\\\ \\ref{tab:finetuning} & Adapter layers, $d=512$ & $81.54$ & $44.25$ & $93.35$ & $91.00$ & $87.25$ & $88.74$ & $88.44$ & $88.02$ & $91.15$ & $83.08$ & $83.80$ & $89.62$ & $74.37$ & $38.63$ & $17.78$ & $36.25$ & $79.18$ & $87.32$ & $64.30$ & $73.18$ & $59.86$ & $71.43$ & $56.00$ & $62.94$ & $18.57$ & $66.56$ & $65.74$ & $70.76$ & $67.87$ & $74.04$ & $23.45$ & $33.98$ & $25.81$ \\\\ \\ref{tab:finetuning} & Adapter layers, $d=2048$ & $82.62$ & $49.86$ & $92.55$ & $91.30$ & $87.99$ & $88.46$ & $88.35$ & $88.36$ & $91.40$ & $83.63$ & $83.18$ & $90.66$ & $76.53$ & $39.44$ & $18.30$ & $37.06$ & $79.40$ & $87.36$ & $68.61$ & $74.53$ & $88.00$ & $91.07$ & $58.00$ & $61.10$ & $18.89$ & $66.73$ & $66.06$ & $73.29$ & $71.16$ & $75.96$ & $25.64$ & $36.92$ & $26.93$ \\\\ \\ref{tab:finetuning} & Gradual Unfreezing & $82.50$ & $51.74$ & $91.97$ & $92.61$ & $89.71$ & $87.27$ & $86.90$ & $88.26$ & $91.35$ & $83.42$ & $83.49$ & $89.71$ & $75.09$ & $40.88$ & $18.95$ & $38.40$ & $79.17$ & $87.30$ & $70.79$ & $75.51$ & $93.09$ & $94.64$ & $70.00$ & $62.03$ & $21.51$ & $65.69$ & $64.79$ & $72.92$ & $69.12$ & $77.89$ & $26.71$ & $39.02$ & $26.93$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:multitask} & \\bsl Baseline (pre-train/fine-tune) & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:multitask} & Equal & $76.13$ & $39.47$ & $90.94$ & $82.90$ & $75.74$ & $78.83$ & $78.44$ & $86.45$ & $89.71$ & $82.08$ & $82.92$ & $90.13$ & $59.93$ & $40.95$ & $19.02$ & $38.39$ & $76.51$ & $85.61$ & $63.37$ & $73.06$ & $82.37$ & $83.93$ & $65.00$ & $60.89$ & $17.52$ & $60.51$ & $59.70$ & $61.01$ & $60.03$ & $65.38$ & $23.89$ & $34.31$ & $26.78$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{16}$ & $80.45$ & $42.07$ & $91.97$ & $90.97$ & $87.50$ & $85.41$ & $85.04$ & $86.89$ & $90.10$ & $83.01$ & $83.66$ & $90.74$ & $72.56$ & $41.16$ & $19.04$ & $38.59$ & $77.25$ & $85.72$ & $69.95$ & $76.67$ & $86.38$ & $89.29$ & $70.00$ & $65.93$ & $27.91$ & $62.78$ & $61.95$ & $76.90$ & $65.83$ & $73.08$ & $24.35$ & $34.99$ & $27.10$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{17}$ & $81.56$ & $47.35$ & $91.40$ & $91.55$ & $88.24$ & $86.15$ & $85.93$ & $86.94$ & $90.06$ & $82.76$ & $84.12$ & $90.79$ & $75.09$ & $41.06$ & $19.12$ & $38.47$ & $77.00$ & $85.87$ & $67.91$ & $77.89$ & $77.54$ & $85.71$ & $57.00$ & $67.78$ & $27.07$ & $61.51$ & $60.54$ & $79.06$ & $65.20$ & $74.04$ & $24.36$ & $35.00$ & $27.25$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{18}$ & $81.67$ & $46.85$ & $91.63$ & $91.99$ & $88.73$ & $87.68$ & $87.20$ & $86.93$ & $90.35$ & $83.30$ & $84.01$ & $91.47$ & $73.29$ & $40.96$ & $19.07$ & $38.43$ & $78.17$ & $86.74$ & $67.94$ & $76.57$ & $78.88$ & $87.50$ & $62.00$ & $67.70$ & $30.85$ & $63.43$ & $62.54$ & $76.53$ & $65.67$ & $67.31$ & $24.57$ & $35.19$ & $27.39$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{19}$ & $81.42$ & $45.94$ & $91.63$ & $92.20$ & $89.22$ & $88.44$ & $88.32$ & $86.84$ & $90.10$ & $83.73$ & $84.29$ & $91.84$ & $70.40$ & $41.26$ & $19.24$ & $38.71$ & $79.78$ & $88.15$ & $67.30$ & $75.66$ & $75.59$ & $87.50$ & $59.00$ & $68.22$ & $30.64$ & $65.32$ & $64.29$ & $73.65$ & $65.05$ & $69.23$ & $25.21$ & $36.30$ & $27.76$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{20}$ & $80.80$ & $42.55$ & $92.78$ & $91.27$ & $87.99$ & $88.36$ & $88.10$ & $86.10$ & $89.62$ & $84.15$ & $84.26$ & $92.20$ & $68.95$ & $41.05$ & $19.24$ & $38.46$ & $80.36$ & $88.27$ & $67.38$ & $73.21$ & $76.18$ & $83.93$ & $62.00$ & $67.57$ & $26.86$ & $66.12$ & $65.22$ & $76.90$ & $64.73$ & $69.23$ & $25.66$ & $36.93$ & $27.68$ \\\\ \\ref{tab:multitask} & Examples-proportional, $K=2^{21}$ & $79.83$ & $44.45$ & $91.28$ & $89.00$ & $84.31$ & $87.54$ & $87.40$ & $84.93$ & $88.53$ & $82.54$ & $84.16$ & $90.85$ & $67.87$ & $40.51$ & $18.79$ & $37.92$ & $79.50$ & $87.48$ & $65.10$ & $71.16$ & $68.88$ & $85.71$ & $57.00$ & $62.75$ & $23.40$ & $64.50$ & $63.65$ & $72.92$ & $64.11$ & $71.15$ & $25.82$ & $37.22$ & $27.13$ \\\\ \\ref{tab:multitask} & Temperature-scaled, $T = 2$ & $81.90$ & $54.00$ & $91.74$ & $90.56$ & $86.76$ & $85.11$ & $84.60$ & $86.40$ & $89.74$ & $83.47$ & $84.15$ & $91.51$ & $72.56$ & $41.09$ & $19.28$ & $38.54$ & $79.42$ & $87.77$ & $69.92$ & $76.73$ & $92.37$ & $92.86$ & $57.00$ & $69.80$ & $31.90$ & $66.65$ & $65.74$ & $72.92$ & $67.08$ & $75.96$ & $25.42$ & $36.72$ & $27.20$ \\\\ \\ref{tab:multitask} & Temperature-scaled, $T = 4$ & $80.56$ & $45.38$ & $91.97$ & $89.68$ & $85.78$ & $83.13$ & $82.76$ & $86.39$ & $90.00$ & $82.78$ & $84.19$ & $91.16$ & $73.65$ & $41.09$ & $19.22$ & $38.51$ & $77.99$ & $86.81$ & $69.54$ & $76.76$ & $97.36$ & $96.43$ & $59.00$ & $68.10$ & $31.48$ & $64.26$ & $63.27$ & $74.73$ & $64.26$ & $71.15$ & $25.04$ & $35.82$ & $27.45$ \\\\ \\ref{tab:multitask} & Temperature-scaled, $T = 8$ & $77.21$ & $40.07$ & $91.06$ & $88.11$ & $83.33$ & $79.20$ & $79.06$ & $86.60$ & $89.90$ & $83.05$ & $83.56$ & $90.21$ & $59.93$ & $41.01$ & $19.10$ & $38.40$ & $77.14$ & $85.99$ & $66.07$ & $73.94$ & $93.70$ & $94.64$ & $60.00$ & $66.36$ & $26.86$ & $63.46$ & $62.60$ & $62.09$ & $63.32$ & $65.38$ & $24.55$ & $35.35$ & $27.17$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:multitask_ft} & \\bsl Unsupervised pre-training + fine-tuning & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:multitask_ft} & Multi-task training & $81.42$ & $45.94$ & $91.63$ & $92.20$ & $89.22$ & $88.44$ & $88.32$ & $86.84$ & $90.10$ & $83.73$ & $84.29$ & $91.84$ & $70.40$ & $41.26$ & $19.24$ & $38.71$ & $79.78$ & $88.15$ & $67.30$ & $75.66$ & $75.59$ & $87.50$ & $59.00$ & $68.22$ & $30.64$ & $65.32$ & $64.29$ & $73.65$ & $65.05$ & $69.23$ & $25.21$ & $36.30$ & $27.76$ \\\\ \\ref{tab:multitask_ft} & Multi-task pre-training + fine-tuning & $83.11$ & $51.42$ & $92.66$ & $91.73$ & $88.73$ & $88.06$ & $87.70$ & $88.61$ & $91.61$ & $84.09$ & $84.31$ & $91.85$ & $76.53$ & $41.15$ & $19.12$ & $38.59$ & $80.26$ & $88.50$ & $71.03$ & $79.54$ & $81.69$ & $87.50$ & $65.00$ & $70.72$ & $31.48$ & $65.94$ & $65.03$ & $81.23$ & $68.18$ & $73.08$ & $27.08$ & $39.80$ & $28.07$ \\\\ \\ref{tab:multitask_ft} & Leave-one-out multi-task training & $81.98$ & $48.00$ & $93.23$ & $91.72$ & $88.24$ & $87.76$ & $87.32$ & $88.61$ & $91.44$ & $84.00$ & $84.11$ & $90.79$ & $72.20$ & $41.34$ & $19.05$ & $38.77$ & $79.97$ & $88.10$ & $71.68$ & $78.35$ & $86.76$ & $89.29$ & $66.00$ & $68.09$ & $29.49$ & $66.23$ & $65.27$ & $79.06$ & $68.65$ & $78.85$ & $26.93$ & $39.79$ & $27.87$ \\\\ \\ref{tab:multitask_ft} & Supervised multi-task pre-training & $79.93$ & $36.60$ & $92.43$ & $91.58$ & $88.24$ & $87.03$ & $86.78$ & $88.15$ & $91.20$ & $82.87$ & $83.16$ & $90.13$ & $70.76$ & $41.12$ & $18.96$ & $38.49$ & $77.38$ & $85.65$ & $65.36$ & $75.66$ & $68.87$ & $83.93$ & $58.00$ & $64.81$ & $21.93$ & $55.37$ & $54.61$ & $71.12$ & $67.40$ & $75.96$ & $26.81$ & $40.13$ & $28.04$ \\\\ \\cmidrule(r){1-2} \\cmidrule(lr){3-15} \\cmidrule(lr){16-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-32} \\cmidrule(l){33-35} \\ref{tab:scaling} & \\bsl Baseline & $83.28$ & $53.84$ & $92.68$ & $92.07$ & $88.92$ & $88.02$ & $87.94$ & $88.67$ & $91.56$ & $84.24$ & $84.57$ & $90.48$ & $76.28$ & $41.33$ & $19.24$ & $38.77$ & $80.88$ & $88.81$ & $71.36$ & $76.62$ & $91.22$ & $91.96$ & $66.20$ & $66.13$ & $25.78$ & $69.05$ & $68.16$ & $75.34$ & $68.04$ & $78.56$ & $26.98$ & $39.82$ & $27.65$ \\\\ \\ref{tab:scaling} & $1\\times$ size, $4\\times$ training steps & $85.33$ & $60.29$ & $93.81$ & $94.06$ & $91.67$ & $89.42$ & $89.25$ & $89.15$ & $91.87$ & $86.01$ & $85.70$ & $91.63$ & $78.34$ & $41.52$ & $19.33$ & $38.96$ & $82.45$ & $90.19$ & $74.72$ & $79.17$ & $94.75$ & $92.86$ & $71.00$ & $67.34$ & $29.70$ & $72.63$ & $71.59$ & $78.34$ & $72.10$ & $82.69$ & $27.08$ & $40.66$ & $27.93$ \\\\ \\ref{tab:scaling} & $1\\times$ size, $4\\times$ batch size & $84.60$ & $56.08$ & $93.12$ & $92.31$ & $89.22$ & $88.85$ & $88.84$ & $89.35$ & $92.07$ & $85.98$ & $86.13$ & $91.07$ & $80.14$ & $41.70$ & $19.42$ & $39.08$ & $82.52$ & $90.21$ & $74.64$ & $78.78$ & $93.69$ & $94.64$ & $72.00$ & $68.09$ & $30.95$ & $74.73$ & $73.90$ & $76.53$ & $70.06$ & $81.73$ & $27.07$ & $40.60$ & $27.84$ \\\\ \\ref{tab:scaling} & $2\\times$ size, $2\\times$ training steps & $86.18$ & $62.04$ & $93.69$ & $93.36$ & $90.69$ & $89.18$ & $89.23$ & $89.35$ & $92.05$ & $87.23$ & $87.05$ & $92.68$ & $81.95$ & $41.74$ & $19.66$ & $39.14$ & $84.18$ & $91.29$ & $77.18$ & $80.98$ & $97.36$ & $96.43$ & $74.00$ & $71.34$ & $35.68$ & $77.11$ & $76.34$ & $80.51$ & $69.28$ & $85.58$ & $27.52$ & $41.03$ & $28.19$ \\\\ \\ref{tab:scaling} & $4\\times$ size, $1\\times$ training steps & $85.91$ & $57.58$ & $94.38$ & $92.67$ & $89.95$ & $89.60$ & $89.60$ & $89.44$ & $92.14$ & $87.05$ & $87.12$ & $93.12$ & $83.39$ & $41.60$ & $19.73$ & $39.08$ & $83.86$ & $91.32$ & $78.04$ & $81.38$ & $89.09$ & $94.64$ & $73.00$ & $73.74$ & $40.40$ & $78.25$ & $77.40$ & $81.59$ & $70.22$ & $91.35$ & $27.47$ & $40.71$ & $28.10$ \\\\ \\ref{tab:scaling} & $4\\times$ ensembled & $84.77$ & $56.14$ & $93.46$ & $93.31$ & $90.67$ & $89.71$ & $89.60$ & $89.62$ & $92.24$ & $86.22$ & $86.53$ & $91.60$ & $77.98$ & $42.10$ & $20.10$ & $39.56$ & $83.09$ & $90.40$ & $71.74$ & $77.58$ & $89.85$ & $91.07$ & $66.00$ & $69.32$ & $29.49$ & $72.67$ & $71.94$ & $76.90$ & $69.12$ & $72.12$ & $28.05$ & $40.53$ & $28.09$ \\\\ \\ref{tab:scaling} & $4\\times$ ensembled, fine-tune only & $84.05$ & $54.78$ & $92.78$ & $93.15$ & $90.44$ & $88.34$ & $88.12$ & $89.27$ & $91.97$ & $85.33$ & $85.88$ & $90.98$ & $77.62$ & $41.66$ & $19.57$ & $39.12$ & $82.36$ & $89.86$ & $71.56$ & $77.43$ & $90.07$ & $92.86$ & $69.00$ & $67.31$ & $26.34$ & $70.47$ & $69.64$ & $75.45$ & $68.18$ & $74.04$ & $27.55$ & $40.22$ & $28.09$ \\\\ \\bottomrule \\end{tabular} \\caption{ Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with $\\bigstar$ denotes our baseline model (described in \\cref{sec:baseline}). }\\label{tab:giant} \\end{minipage} \\end{table}  \\clearpage  \\eject \\pdfpagewidth=8.5in \\pdfpageheight=11in  \\bibliography{biblio}  \\end{document}  "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/1910.10683/text_to_text.png",
                "caption": " A diagram of our text-to-text framework. Every task we consider---including translation, question answering, and classification---is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc.\\ across our diverse set of tasks. It also provides a standard testbed for the methods included in our empirical survey. ``T5'' refers to our model, which we dub the ``\\textbf{T}ext-\\textbf{t}o-\\textbf{T}ext \\textbf{T}ransfer \\textbf{T}ransformer''. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/objective.png",
                "caption": "Schematic of the objective we use in our baseline model. In this example, we process the sentence ``Thank you for inviting me to your party last week.'' The words ``for'', ``inviting'' and ``last'' (marked with an $\\times$) are randomly chosen for corruption. Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as \\texttt{<X>} and \\texttt{<Y>}) that is unique over the example. Since ``for'' and ``inviting'' occur consecutively, they are replaced by a single sentinel \\texttt{<X>}. The output sequence then consists of the dropped-out spans, delimited by the sentinel tokens used to replace them in the input plus a final sentinel token \\texttt{<Z>}.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/attention_masks.png",
                "caption": " Matrices representing different attention mask patterns. The input and output of the self-attention mechanism are denoted $x$ and $y$ respectively. A dark cell at row $i$ and column $j$ indicates that the self-attention mechanism is allowed to attend to input element $j$ at output timestep $i$. A light cell indicates that the self-attention mechanism is \\textit{not} allowed to attend to the corresponding $i$ and $j$ combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the $i$th output element from depending on any input elements from ``the future''. Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/architectures.png",
                "caption": " Schematics of the Transformer architecture variants we consider. In this diagram, blocks represent elements of a sequence and lines represent attention visibility. Different colored groups of blocks indicate different Transformer layer stacks. Dark grey lines correspond to fully-visible masking and light grey lines correspond to causal masking. We use ``.''\\ to denote a special end-of-sequence token that represents the end of a prediction. The input and output sequences are represented as $x$ and $y$ respectively. Left: A standard encoder-decoder architecture uses fully-visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder. Middle: A language model consists of a single Transformer layer stack and is fed the concatenation of the input and target, using a causal mask throughout. Right: Adding a prefix to a language model corresponds to allowing fully-visible masking over the input. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/objectives_flow.png",
                "caption": " A flow chart of our exploration of unsupervised objectives. We first consider a few disparate approaches in \\cref{sec:objectives_highlevel} and find that a BERT-style denoising objective performs best. Then, we consider various methods for simplifying the BERT objective so that it produces shorter target sequences in \\cref{sec:simplifying_bert}. Given that replacing dropped-out spans with sentinel tokens performs well and results in short target sequences, in \\cref{sec:corruption_rate} we experiment with different corruption rates. Finally, we evaluate an objective that intentionally corrupts contiguous spans of tokens in \\cref{sec:objective_spans}. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/datasets_take_loss.png",
                "caption": " Pre-training loss for our original C4 data set as well as $4$ artificially truncated versions. The sizes listed refer to the number of tokens in each data set. The four sizes considered correspond to repeating the data set between $64$ and $4{,}096$ times over the course of pre-training. Using a smaller data set size results in smaller training loss values, which may suggest some memorization of the unlabeled data set. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table1.tex",
                "caption": " Average and standard deviation of scores achieved by our baseline model and training procedure. For comparison, we also report performance when training on each task from scratch (i.e.\\ without any pre-training) for the same number of steps used to fine-tune the baseline model. All scores in this table (and every table in our paper except \\cref{tab:final}) are reported on the validation sets of each data set. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table2.tex",
                "caption": " Performance of the different architectural variants described in \\cref{sec:architecture_variants}. We use $P$ to refer to the number of parameters in a 12-layer base Transformer layer stack and $M$ to refer to the FLOPs required to process a sequence using the encoder-decoder model. We evaluate each architectural variant using a denoising objective (described in \\cref{sec:baseline_objective}) and an autoregressive objective (as is commonly used to train language models). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table3.tex",
                "caption": " Examples of inputs and targets produced by some of the unsupervised objectives we consider applied to the input text ``Thank you for inviting me to your party last week .'' Note that all of our objectives process \\textit{tokenized} text. For this particular sentence, all words were mapped to a single token by our vocabulary. We write \\textit{(original text)} as a target to denote that the model is tasked with reconstructing the entire input text. \\texttt{<M>} denotes a shared mask token and \\texttt{<X>}, \\texttt{<Y>}, and \\texttt{<Z>} denote sentinel tokens that are assigned unique token IDs. The BERT-style objective (second row) includes a corruption where some tokens are replaced by a random token ID; we show this via the greyed-out word \\textcolor{gray}{apple}. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table4.tex",
                "caption": " Performance of the three disparate pre-training objectives described in \\cref{sec:objectives_highlevel}. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table5.tex",
                "caption": " Comparison of variants of the BERT-style pre-training objective. In the first two variants, the model is trained to reconstruct the original uncorrupted text segment. In the latter two, the model only predicts the sequence of corrupted tokens. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table6.tex",
                "caption": " Performance of the i.i.d.\\ corruption objective with different corruption rates. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table7.tex",
                "caption": " Performance of the span-corruption objective (inspired by \\cite{joshi2019spanbert}) for different average span lengths. In all cases, we corrupt 15\\% of the original text sequence. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table8.tex",
                "caption": " Performance resulting from pre-training on different data sets. The first four variants are based on our new C4 data set. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table9.tex",
                "caption": " Measuring the effect of repeating data during pre-training. In these experiments, we only use the first $N$ tokens from C4 (with varying values of $N$ shown in the first column) but still pre-train over $2^{35}$ tokens. This results in the data set being repeated over the course of pre-training (with the number of repeats for each experiment shown in the second column), which may result in memorization (see \\cref{fig:datasets_take_loss}). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table10.tex",
                "caption": " Comparison of different alternative fine-tuning methods that only update a subset of the model's parameters. For adapter layers, $d$ refers to the inner dimensionality of the adapters. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table11.tex",
                "caption": " Comparison of multi-task training using different mixing strategies. Examples-proportional mixing refers to sampling examples from each data set according to the total size of each data set, with an artificial limit ($K$) on the maximum data set size. Temperature-scaled mixing re-scales the sampling rates by a temperature $T$. For temperature-scaled mixing, we use an artificial data set size limit of $K = 2^{21}$. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table12.tex",
                "caption": " Comparison of unsupervised pre-training, multi-task learning, and various forms of multi-task pre-training. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table13.tex",
                "caption": " Comparison of different methods of scaling up our baseline model. All methods except ensembling fine-tuned models use $4\\times$ the computation as the baseline. ``Size'' refers to the number of parameters in the model and ``training time'' refers to the number of steps used for both pre-training and fine-tuning. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table14.tex",
                "caption": " Performance of our T5 variants on every task we study. Small, Base, Large, 3B, and 11B refer to model configurations with $60$ million, $220$ million, $770$ million, $3$ billion, and $11$ billion parameters, respectively. In the first row of each table, we report the state-of-the-art for the task (as of October 24th, 2019), with the superscript denoting its source with references listed at the end of this caption. All results are reported on the test set except for SQuAD where we use the validation set. $^a$\\citep{lan2019albert} $^b$\\citep{wang2019structbert} $^c$\\citep{zhu2019freelb} $^d$\\citep{liu2019roberta} $^e$\\citep{edunov2018understanding} $^f$\\citep{lample2019cross} $^g$\\citep{dong2019unified} ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table15.tex",
                "caption": " Performance comparison of T5-Base to our baseline experimental setup used in the rest of the paper. Results are reported on the validation set. ``Baseline-1T'' refers to the performance achieved by pre-training the baseline model on 1 trillion tokens (the same number used for the T5 model variants) instead of $2^{35} \\approx 34\\mathrm{B}$ tokens (as was used for the baseline). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/1910.10683/table16.tex",
                "caption": " Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with $\\bigstar$ denotes our baseline model (described in \\cref{sec:baseline}). ",
                "description": ""
            }
        ]
    },
    "2104.09864": {
        "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
        "abstract": null,
        "tldr": "A novel method named Rotary Position Embedding(RoPE) is proposed to effectively leverage the positional information in transformer-based language models and enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "The sequential order of words is of great value to natural language understanding. Recurrent neural networks (RRNs) based models encode tokens' order by recursively computing a hidden state along the time dimension.  Convolution neural networks (CNNs) based models (CNNs) \\cite{Gehring:2017} were typically considered position-agnostic, but recent work \\cite{Islam2020HowMP} has shown that the commonly used padding operation can implicitly learn position information. Recently,  the pre-trained language models (PLMs), which were built upon the transformer \\cite{Vaswani:2017}, have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including context representation learning \\cite{Devlin2019BERTPO}, machine translation \\cite{Vaswani:2017}, and language modeling \\cite{Radford2019LanguageMA}, to name a few.  Unlike, RRNs and CNNs-based models, PLMs utilize the self-attention mechanism to semantically capture the contextual representation of a given corpus. As a consequence, PLMs achieve a significant improvement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations compared to CNNs\\footnote{A stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting.}.  It is noteworthy that the self-attention architecture of the current PLMs has shown to be position-agnostic \\cite{Yun2020Are}. Following this claim, various approaches have been proposed to encode the position information into the learning process. On one side, generated absolute position encoding through a pre-defined function \\cite{Vaswani:2017} was added to the contextual representations, while a trainable absolute position encoding \\cite{Gehring:2017,Devlin2019BERTPO,Lan2020ALBERT:2020,clark2020electra,Radford2019LanguageMA,Radford2018ImprovingLU:2018}. On the other side, the previous work \\cite{Parikh2016ADA,Shaw2018SelfAttentionWR,Huang2018MusicT,Dai2019TransformerXLAL,Yang2019XLNetGA,Raffel2020ExploringTL,Ke2020RethinkingPE,He2020DeBERTaDB,huang-etal-2020-improve} focuses on relative position encoding, which typically encodes the relative position information into the attention mechanism. In addition to these approaches, the authors of \\cite{LiuYDH:2020} have proposed to model the dependency of position encoding from the perspective of Neural ODE \\cite{ChenRBD:2018}, and the authors of \\cite{Wang2020Encoding} have proposed to model the position information in complex space. Despite the effectiveness of these approaches, they commonly add the position information to the context representation and thus render them unsuitable for the linear self-attention architecture.  \\c \\c\\c\\c\\c\\c\\c\\\\   In this paper, we introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the proposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length flexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Experimental results on various long text classification benchmark datasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better performance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.  In brief, our contributions are three-folds as follows: \\begin{itemize} \\item We investigated the existing approaches to the relative position encoding and found that they are mostly built based on the idea of the decomposition of adding position encoding to the context representations. We introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information into the learning process of PLMS. The key idea is  to encode relative position by multiplying the context representations with a rotation matrix with a clear theoretical interpretation. \\i \\item We study the properties of RoPE and show that it decays with the relative distance increased, which is desired for natural language encoding.  We kindly argue that previous relative position encoding-based approaches are not compatible with linear self-attention. \\i\\f  \\item We evaluate the proposed RoFormer on various long text benchmark datasets. Our experiments show that it consistently achieves better performance compared to its alternatives.  Some experiments with pre-trained language models are available on GitHub: \\url{https://github.com/ZhuiyiTechnology/roformer}. \\end{itemize}  The remaining of the paper is organized as follows. We establish a formal description of the position encoding problem in self-attention architecture and revisit previous works in \\Cref{sec:background}. We then describe the rotary position encoding (RoPE) and study its properties in \\Cref{sec:approach}. We report experiments in \\Cref{sec:experiment}. Finally, we conclude this paper in \\Cref{sec:conclu}.  "
            },
            {
                "section_name": "Background and Related Work_2",
                "paragraphs": ""
            },
            {
                "section_name": "Preliminary_1",
                "paragraphs": "Let $\\mathbb{S}_N=\\{w_i\\}_{i=1}^{N}$ be a sequence of $N$ input tokens with $w_i$ being the $i^{th}$ element. The corresponding word embedding of $\\mathbb{S}_N$ is denoted as $\\mathbb{E}_N = \\{\\x_i\\}_{i=1}^{N}$, where $\\x_i\\in \\mathbb{R}^{d}$ is the d-dimensional word embedding vector of token $w_i$ without position information. The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys, and value representations. \\begin{equation} \\begin{aligned} \\q_m &=f_q(\\x_m, m)\\\\ \\k_n &=f_k(\\x_n, n)\\\\ \\v_n &=f_v(\\x_n, n),\\\\ \\end{aligned} \\label{fn:qkv} \\end{equation} where $\\q_m,\\k_n$ and $ \\v_n$ incorporate the $m^{th}$ and $n^{th}$ positions through $f_q,f_k$ and $f_v$, respectively. The query and key values are then used to compute the attention weights, while  the output is computed as the weighted sum over the value representation. \\begin{equation} \\begin{aligned} a_{m,n}&=\\frac{\\exp(\\frac{\\q_m^{\\intercal}\\k_n}{\\sqrt{d}})}{\\sum_{j=1}^{N}\\exp(\\frac{\\q_m^{\\intercal}\\k_j}{\\sqrt{d}})}\\\\ \\mathbf{o}_m&=\\sum_{n=1}^{N}a_{m,n}\\v_{n} \\end{aligned} \\label{fn:attn} \\end{equation} The existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form \\Cref{fn:qkv}.  "
            },
            {
                "section_name": "Absolute position embedding_2",
                "paragraphs": "A typical choice of \\Cref{fn:qkv} is \\begin{equation} f_{t:t\\in\\{q,k,v\\}}(\\x_i,i):=\\W_{t:t\\in\\{q,k,v\\}}(\\x_i+\\p_i), \\label{fn:adtv-posi} \\end{equation} where $\\p_i\\in\\mathbb{R}^{d}$ is a d-dimensional vector depending of the position of token $\\x_i$. Previous work \\cite{Devlin2019BERTPO,Lan2020ALBERT:2020,clark2020electra,Radford2019LanguageMA,Radford2018ImprovingLU:2018} introduced the use of a set of trainable vectors $\\p_i\\in\\{\\p_t\\}_{t=1}^{L}$, where $L$ is the maximum sequence length. The authors of \\cite{Vaswani:2017} have proposed to generate $\\p_i$ using the sinusoidal function. \\begin{equation} \\begin{cases} \\p_{i,2t}&=\\sin(k/10000^{2t/d})\\\\ \\p_{i,2t+1}&=\\cos(k/10000^{2t/d}) \\end{cases} \\label{fn:sins} \\end{equation} in which $\\p_{i,2t}$ is the $2t^{th}$ element of the d-dimensional vector $\\p_i$. In the next section, we show that our proposed RoPE is related to this intuition from the sinusoidal function perspective. However, instead of directly adding the position to the context representation, RoPE proposes to incorporate the relative position information by multiplying with the sinusoidal functions.  "
            },
            {
                "section_name": "Relative position embedding_3",
                "paragraphs": "The authors of \\cite{Shaw2018SelfAttentionWR} applied  different settings of \\Cref{fn:qkv} as following: \\begin{equation} \\begin{aligned} f_q(\\x_m):=\\W_{q}\\x_m\\\\ f_k(\\x_n, n):=\\W_{k}(\\x_n+\\tilde{\\p}^k_r)\\\\ f_v(\\x_n, n):=\\W_{v}(\\x_n+\\tilde{\\p}^v_r)\\\\ \\end{aligned} \\label{fn:shaw} \\end{equation} where $\\tilde{\\p}^k_r,\\tilde{\\p}^v_r\\in\\mathbb{R}^{d}$ are trainable relative position embeddings. Note that $r=\\operatorname{clip}(m-n,r_{\\text{min}},r_{\\text{max}})$ represents the relative distance between position $m$ and $n$. They clipped the relative distance with the hypothesis that precise relative position information is not useful beyond a certain distance. Keeping the form of \\Cref{fn:adtv-posi}, the authors \\cite{Dai2019TransformerXLAL} have proposed to decompose $\\q_m^{\\intercal}\\k_n$ of \\Cref{fn:attn} as \\begin{equation} \\q_m^{\\intercal}\\k_n=\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n+\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\p_n+\\p_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n+\\p_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\p_n, \\label{fn:rela-posi1} \\end{equation} the key idea is to replace the absolute position embedding $\\p_n$ with its sinusoid-encoded relative counterpart $\\tilde{\\p}_{m-n}$, while the absolute position $\\p_m$ in the third and fourth term with two trainable vectors $\\mathbf{u}$ and $\\mathbf{v}$ independent of the query positions. Further, $\\W_k$ is distinguished for the content-based and location-based key vectors $\\x_n$ and $\\p_n$, denoted as $\\W_k$ and $\\widetilde{\\W}_k$, resulting in: \\begin{equation} \\q_m^{\\intercal}\\k_n=\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n+\\x_m^{\\intercal}\\W_q^{\\intercal}\\widetilde{\\W}_k\\tilde{\\p}_{m-n}+\\mathbf{u}^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n+\\mathbf{v}^{\\intercal}\\W_q^{\\intercal}\\widetilde{\\W}_k\\tilde{\\p}_{m-n} \\label{fn:rela-posi2} \\end{equation}  It is noteworthy that the position information in the value term is removed by setting $f_v(\\x_j):=\\W_{v}\\x_j$. Later work \\cite{Raffel2020ExploringTL,He2020DeBERTaDB,Ke2020RethinkingPE,huang-etal-2020-improve} followed these settings by only encoding the relative position information into the attention weights. However, the authors of \\cite{Raffel2020ExploringTL} reformed \\Cref{fn:rela-posi1} as: \\begin{equation} \\q_m^{\\intercal}\\k_n=\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n + b_{i,j} \\label{fn:rela-posi3} \\end{equation} where $b_{i,j}$ is a trainable bias. The authors of \\cite{Ke2020RethinkingPE} investigated the middle two terms of \\Cref{fn:rela-posi1} and found little correlations between absolute positions and words. The authors of \\cite{Raffel2020ExploringTL} proposed to model a pair of words or positions using different projection matrices. \\begin{equation} \\q_m^{\\intercal}\\k_n=\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n +\\p_m^{\\intercal}\\mathbf{U}_q^{\\intercal}\\mathbf{U}_k\\p_n+b_{i,j} \\label{fn:rela-posi4} \\end{equation} The authors of \\cite{He2020DeBERTaDB} argued that the relative positions of two tokens could only be fully modeled using the middle two terms of \\Cref{fn:rela-posi1}. As a consequence, the absolute position embeddings $\\p_m$ and $\\p_n$ were simply replaced with the relative position embeddings $\\tilde{\\p}_{m-n}$: \\begin{equation} \\q_m^{\\intercal}\\k_n=\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n+\\x_m^{\\intercal}\\W_q^{\\intercal}\\W_k\\tilde{\\p}_{m-n}+\\tilde{\\p}_{m-n}^{\\intercal}\\W_q^{\\intercal}\\W_k\\x_n \\label{fn:debeta} \\end{equation} A comparison of the four variants of the relative position embeddings \\cite{Radford2018ImprovingLU:2018} has shown that the variant similar to \\Cref{fn:debeta} is the most efficient among the other three. Generally speaking, all these approaches attempt to  modify \\Cref{fn:rela-posi1} based on the decomposition of \\Cref{fn:adtv-posi} under the self-attention settings in \\Cref{fn:attn}, which was originally proposed in \\cite{Vaswani:2017}. They commonly introduced to directly add the position information to the context representations. Unlikely, our approach aims to derive the relative position encoding from \\Cref{fn:qkv} under some constraints. Next, we show that the derived approach is more interpretable by incorporating relative position information with the rotation of context representations. "
            },
            {
                "section_name": "Proposed approach_3",
                "paragraphs": "\\label{sec:approach} In this section, we discuss the proposed rotary position embedding (RoPE). We first formulate the relative position encoding problem in  \\Cref{sec:formulation}, we then derive the RoPE in \\Cref{sec:RoPE} and investigate its properties in  \\Cref{sec: prop of RoPE}.  "
            },
            {
                "section_name": "Formulation_4",
                "paragraphs": "\\label{sec:formulation} Transformer-based language modeling usually leverages the position information of individual tokens through a self-attention mechanism. As can be observed in \\Cref{fn:attn}, $\\q_m^{\\intercal}\\k_n$ typically enables knowledge conveyance between tokens at different positions. In order to incorporate relative position information, we require the inner product of query $\\q_m$ and key $\\k_n$ to be formulated by a function $g$, which takes only the word embeddings $\\x_m$, $\\x_n$, and their relative position $m -n$ as input variables. In other words, we hope that the inner product encodes position information only in the relative form:  \\if We start with \\cref{fn:qkv}. To incorporate position information with self-attention in \\cref{fn:attn}, we first consider incorporating the position information in the query and key, respectively.  Furthermore, we set the inner product of these two terms to a function explicitly depending on their relative distance. In other words, we hope the inner product only encodes relative position. \\fi  \\b \\b \\l\\q\\k\\r\\l\\x\\x\\r\\\\ \\x\\x \\e \\l \\e  \\begin{equation} \\langle f_q(\\x_m, m),f_k(\\x_n, n)\\rangle=g(\\x_m,\\x_n,m-n). \\label{fn:formulation} \\end{equation} The ultimate goal is to find an equivalent encoding mechanism to solve the functions $f_q(\\x_m, m)$ and $f_k(\\x_n, n)$ to conform the aforementioned relation.  "
            },
            {
                "section_name": "Rotary position embedding_5",
                "paragraphs": "\\label{sec:RoPE}  \\subsubsection{A 2D case} We begin with a simple case with a dimension $d=2$. Under these settings, we make use of the geometric property of vectors on a 2D plane and its complex form to prove (refer \\Cref{appendix:rope-deriv} for more details) that a solution to our formulation \\Cref{fn:formulation} is:  \\begin{equation} \\begin{aligned} f_q(\\x_m, m) &= (\\W_q\\x_m)e^{im\\theta}\\\\ f_k(\\x_n, n) &= (\\W_k\\x_n)e^{in\\theta}\\\\ \\x\\x\\W\\x\\i\\W\\x\\t g(\\x_m, \\x_n, m - n) &= \\operatorname{Re}[(\\W_q\\x_m)(\\W_k\\x_n)^{*}e^{i(m - n)\\theta}] \\end{aligned} \\label{fn:res-2d} \\end{equation} where $\\operatorname{Re}[\\cdot]$ is the real part of a complex number and $(\\W_k\\x_n)^{*}$ represents the conjugate complex number of $(\\W_k\\x_n)$. $\\theta \\in \\mathbb{R}$ is a preset non-zero constant. We can further write $f_{\\{q, k\\}}$ in a multiplication matrix: \\begin{equation} f_{\\{q, k\\}}(\\x_m, m) = \\left( \\begin{array}{cc} \\cos{m\\theta}& -\\sin{m\\theta}  \\\\ \\sin{m\\theta}&\\cos{m\\theta} \\end{array} \\right) \\left( \\begin{array}{cc} W^{(11)}_{\\{q, k\\}} & W^{(12)}_{\\{q, k\\}} \\\\ W^{(21)}_{\\{q, k\\}} & W^{(22)}_{\\{q, k\\}} \\end{array} \\right) \\left( \\begin{array}{cc} x^{(1)}_m\\\\ x^{(2)}_m \\end{array} \\right) \\end{equation} where $(x^{(1)}_m, x^{(2)}_m)$ is $\\x_m$ expressed in the 2D coordinates. Similarly, $g$ can be viewed as a matrix  and thus enables the solution of formulation in \\Cref{sec:formulation} under the 2D case. Specifically, incorporating the relative position embedding is straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its position index and thus interprets the intuition behind \\textit{Rotary Position Embedding}.  \\subsubsection{General form}\\label{subsec:general-form} In order to generalize our results in 2D to any $\\x_i \\in \\mathbb{R}^d$ where $d$ is even,  we divide the d-dimension space into $d/2$ sub-spaces and combine them in the merit of the linearity of the inner product, turning $f_{\\{q, k\\}}$ into:  \\begin{equation} f_{\\{q, k\\}}(\\x_m, m) = \\R^d_{\\Theta, m}\\W_{\\{q, k\\}}\\x_m \\label{fn:rope-fqk} \\end{equation} where \\begin{equation} \\R^d_{\\Theta,m} = \\begin{pmatrix} \\cos{m\\theta_1}& -\\sin{m\\theta_1}&0&0&\\cdots&0&0\\\\ \\sin{m\\theta_1}&\\cos{m\\theta_1}&0&0&\\cdots&0&0 \\\\ 0&0&\\cos{m\\theta_2}& -\\sin{m\\theta_2}&\\cdots&0&0\\\\ 0&0&\\sin{m\\theta_2}&\\cos{m\\theta_2}&\\cdots&0&0 \\\\ \\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\ 0&0&0&0&\\cdots&\\cos{m\\theta_{d/2}}& -\\sin{m\\theta_{d/2}}\\\\ 0&0&0&0&\\cdots&\\sin{m\\theta_{d/2}}&\\cos{m\\theta_{d/2}} \\end{pmatrix} \\label{fn:rope-RMat} \\end{equation} is the rotary matrix with pre-defined parameters $\\Theta = \\{\\theta_i=10000^{-2(i-1)/d}, i \\in [1, 2, ..., d/2]\\}$. A graphic illustration of RoPE is shown in \\Cref{fig:rope}. Applying our RoPE to self-attention in \\Cref{fn:attn}, we obtain: \\begin{equation} \\q_m^{\\intercal}\\k_n \\l\\x\\x\\r\\\\ =(\\R^d_{\\Theta, m}\\W_q\\x_m)^\\intercal(\\R^d_{\\Theta, n}\\W_k\\x_n) =\\x^\\intercal\\W_qR^d_{\\Theta, n-m}\\W_k\\x_n \\label{fn:rope-qk} \\end{equation} where $\\R^d_{\\Theta, n-m}=(\\R^{d}_{\\Theta, m})^\\intercal\\R^d_{\\Theta, n}$. Note that $\\R^d_\\Theta$ is an orthogonal matrix, which ensures stability during the process of encoding position information. In addition, due to the sparsity of $R^d_\\Theta$, applying matrix multiplication directly as in \\Cref{fn:rope-qk} is not computationally efficient; we provide another realization in theoretical explanation.   In contrast to the additive nature of position embedding method adopted in the previous works, i.e.,  \\Cref{fn:adtv-posi,fn:sins,fn:shaw,fn:rela-posi1,fn:rela-posi2,fn:rela-posi3,fn:rela-posi4,fn:debeta}, our approach is multiplicative. Moreover,  RoPE naturally incorporates relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied with self-attention.  \\begin{figure}[hb] \\centering \\includegraphics[width=0.7\\textwidth]{images/roformer_RoPE_v2.pdf} \\caption{Implementation of Rotary Position Embedding(RoPE). } \\label{fig:rope} \\end{figure}   "
            },
            {
                "section_name": "Properties of RoPE_6",
                "paragraphs": "\\label{sec: prop of RoPE} \\paragraph{Long-term decay:} Following \\cite{Vaswani:2017}, we set $\\theta_i=10000^{-2i/d}$. One can prove that this setting provides a long-term decay property (refer to \\Cref{appendix:long-term-decay} for more details), which means the inner-product will decay when the relative position increase. This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection.  \\paragraph{RoPE with linear attention:} The self-attention can be rewritten in a more general form.  \\begin{equation} \\operatorname{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})_m=\\frac{\\sum_{n=1}^{N}\\operatorname{sim}(\\q_m,\\k_n)\\v_n}{\\sum_{n=1}^{N}\\operatorname{sim}(\\q_m, \\k_n)}. \\label{fn:atten-full} \\end{equation} The original self-attention chooses $\\operatorname{sim}(\\q_m,\\k_n)=\\exp(\\q_m^{\\intercal}\\k_n/\\sqrt{d})$. Note that the original self-attention should compute the inner product of query and key for every pair of tokens, which has a quadratic complexity $\\mathbb{O}(N^2)$.  Follow \\cite{katharopoulos2020transformers}, the linear attentions reformulate \\Cref{fn:atten-full} as  \\begin{equation} \\operatorname{Attention}(\\Q,\\K,\\V)_m=\\frac{\\sum_{n=1}^{N}\\phi(\\q_m)^{\\intercal}\\varphi(\\k_n)\\v_n}{\\sum_{n=1}^{N}\\phi(\\q_m)^{\\intercal}\\varphi(\\k_n)}, \\label{fn:linear-attention} \\end{equation} where $\\phi(\\cdot), \\varphi(\\cdot)$ are usually non-negative functions. The authors of \\cite{katharopoulos2020transformers} have proposed $\\phi(x)=\\varphi(x)=\\operatorname{elu}(x)+1$ and first computed the multiplication between keys and values using the associative property of matrix multiplication.  A softmax function is used in \\cite{shen2021efficient} to normalize queries and keys separately before the inner product, which is equivalent to   $\\phi(\\q_i)=\\operatorname{softmax}(\\q_i)$ and $\\phi(\\k_j)=\\exp(\\k_j)$. For more details about linear attention, we encourage readers to refer to original papers. In this section, we focus on discussing incorporating RoPE with \\Cref{fn:linear-attention}. Since RoPE injects position information by rotation, which keeps the norm of hidden representations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the outputs of the non-negative functions.  \\begin{equation} \\operatorname{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})_m=\\frac{\\sum_{n=1}^{N}\\big(\\R^d_{\\Theta, m}\\phi(\\q_m)\\big)^{\\intercal}\\big(\\R^d_{\\Theta, n}\\varphi(\\k_n)\\big)\\v_n}{\\sum_{n=1}^{N}\\phi(\\q_m)^{\\intercal}\\varphi(\\k_n)}. \\label{fn:linear-rope} \\end{equation}  It is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in the numerator could contain negative terms. Although the weights for each value $\\v_i$ in  \\Cref{fn:linear-rope} are not strictly probabilistic normalized, we kindly argue that the computation can still model the importance of values. "
            },
            {
                "section_name": "Theoretical Explanation_7",
                "paragraphs": "\\subsubsection{Derivation of RoPE under 2D}\\label{appendix:rope-deriv} Under the case of $d=2$, we consider two-word embedding vectors $\\x_q$, $\\x_k$ corresponds to query and key and their position $m$ and $n$, respectively. According to \\cref{fn:qkv}, their position-encoded counterparts are: \\begin{equation} \\begin{aligned} \\q_m &= f_q(\\x_q, m),  \\\\ \\k_n &= f_k(\\x_k, n), \\end{aligned} \\label{fn:deri-qk} \\end{equation} where the subscripts of $\\q_m$ and $\\k_n$ indicate the encoded positions information. Assume that there exists a function $g$ that defines the inner product between vectors produced by $f_{\\{q, k\\}}$: \\begin{equation} \\q^\\intercal_m\\k_n = \\langle f_q(\\x_m, m),f_k(\\x_n, n)\\rangle= g(\\x_m, \\x_n, n - m), \\label{fn:deri-g} \\end{equation} we further require below initial condition to be satisfied: \\begin{equation} \\begin{aligned} \\q &= f_q(\\x_q, 0), \\\\ \\k &= f_k(\\x_k, 0), \\end{aligned} \\label{fn:deri-initCond1} \\end{equation} which can be read as the vectors with empty position information encoded. Given these settings, we attempt to find a solution of $f_q$, $f_k$.  First, we take advantage of the geometric meaning of vector in 2D and its complex counter part, decompose functions in \\Cref{fn:deri-qk,fn:deri-g} into: \\begin{equation} \\begin{aligned} f_q(\\x_q, m) &= R_q(\\x_q, m)e^{i\\Theta_q(\\x_q, m)}, \\\\ f_k(\\x_k, n) &= R_k(\\x_k, n)e^{i\\Theta_k(\\x_k, n)}, \\\\ g(\\x_q, \\x_k, n - m) &= R_g(\\x_q, \\x_k, n - m)e^{i\\Theta_g(\\x_q, \\x_k, n - m)}, \\end{aligned} \\end{equation} where $R_f$, $R_g$ and $\\Theta_f$, $\\Theta_g$ are the radical and angular components for $f_{\\{q, k\\}}$ and $g$, respectively. Plug them into \\Cref{fn:deri-g}, we get the relation:  \\begin{equation} \\begin{aligned} R_q(\\x_q, m)R_k(\\x_k, n) &= R_g(\\x_q, \\x_k, n - m), \\\\ \\Theta_k(\\x_k, n) - \\Theta_q(\\x_q, m) &= \\Theta_g(\\x_q, \\x_k, n - m), \\end{aligned} \\label{fn:deri-rtheta} \\end{equation} with the corresponding initial condition as: \\begin{equation} \\begin{aligned} \\q &= \\|\\q\\|e^{i\\theta_q} = R_q(\\x_q, 0)e^{i\\Theta_q(\\x_q, 0)},\\\\ \\k & = \\|\\k\\|e^{i\\theta_k} = R_k(\\x_k, 0)e^{i\\Theta_k(\\x_k, 0)}, \\end{aligned} \\label{fn:deri-initCond2} \\end{equation} where $\\|\\q\\|$, $\\|\\k\\|$ and $\\theta_q$, $\\theta_k$ are the radial and angular part of $\\q$ and $\\k$ on the 2D plane.  Next, we set $m=n$ in \\Cref{fn:deri-rtheta} and take into account initial conditions in \\Cref{fn:deri-initCond2}: \\begin{subequations} \\begin{align} R_q(\\x_q, m)R_k(\\x_k, m) &= R_g(\\x_q, \\x_k, 0) = R_q(\\x_q, 0)R_k(\\x_k, 0)  = \\|\\q\\|\\|\\k\\| \\label{fn:deri-solveR},\\\\ \\Theta_k(\\x_k, m) - \\Theta_q(\\x_q, m) &= \\Theta_g(\\x_q, \\x_k, 0) =\\Theta_k(\\x_k, 0) - \\Theta_q(\\x_q, 0) =  \\theta_k - \\theta_q \\label{fn:deri-solveTheta}. \\end{align} \\end{subequations} On one hand, from, a straightforward solution of $R_f$ could be formed from \\Cref{fn:deri-solveR} : \\begin{equation} \\begin{aligned} R_q(\\x_q, m) &= R_q(\\x_q, 0) =  \\|\\q\\| \\\\ R_k(\\x_k, n) &= R_k(\\x_k, 0) = \\|\\k\\|\\\\ R_g(\\x_q, \\x_k, n - m) &= R_g(\\x_q, \\x_k, 0) =  \\|\\q\\|\\|\\k\\| \\end{aligned} \\label{fn:deri-solR1} \\end{equation} which interprets the radial functions $R_q$, $R_k$ and $R_g$ are independent from the position information. On the other hand, as can be noticed in \\Cref{fn:deri-solveTheta},  $\\Theta_q(\\x_q, m) - \\theta_q = \\Theta_k(\\x_k, m) - \\theta_k$ indicates that the angular functions does not dependent on query and key, we set them to  $\\Theta_f := \\Theta_q = \\Theta_k$ and term $\\Theta_f(\\x_{\\{q, k\\}}, m) - \\theta_{\\{q, k\\}}$ is a function of position $m$ and is independent of word embedding $\\x_{\\{q, k\\}}$, we denote it as $\\phi(m)$, yielding: \\begin{equation} \\Theta_f(\\x_{\\{q, k\\}}, m) = \\phi(m) + \\theta_{\\{q,k\\}}, \\label{fn:deri-solTheta1} \\end{equation} Further, by plugging  $n = m + 1$ to \\Cref{fn:deri-rtheta} and consider the above equation, we can get: \\begin{equation} \\phi(m + 1) - \\phi(m) = \\Theta_g(\\x_q, \\x_k, 1) + \\theta_q - \\theta_k, \\label{fn:deri-solTheta2} \\end{equation} Since RHS is a constant irrelevant to $m$, $\\phi(m)$ with continuous integer inputs produce an arithmetic progression: \\begin{equation} \\phi(m) = m\\theta + \\gamma, \\label{fn:deri-solTheta3} \\end{equation} where $\\theta,\\gamma \\in \\mathbb{R}$ are constants and $\\theta$ is non-zero. To summarize our solutions from \\Cref{fn:deri-solR1,fn:deri-solTheta1,fn:deri-solTheta2,fn:deri-solTheta3}:  \\begin{equation} \\begin{aligned} f_q(\\x_q, m) &= \\|\\q\\|e^{i\\theta_q + m\\theta + \\gamma} = \\q e^{i(m\\theta + \\gamma)}, \\\\ f_k(\\x_k, n) &= \\|\\k\\|e^{i\\theta_k + n\\theta + \\gamma}= \\k e^{i(n\\theta + \\gamma)}. \\end{aligned} \\label{fn:deri-solution1} \\end{equation}  Note that we do not apply any constrains to $f_q$ and $f_k$ of \\Cref{fn:deri-initCond1}, thus $f_q(\\x_m, 0)$ and $f_k(\\x_n, 0)$ are left to choose freely. To make our results comparable to \\Cref{fn:adtv-posi}, we define: \\begin{equation} \\begin{aligned} \\q = f_q(\\x_m, 0) &= \\W_q\\x_n,\\\\ \\k = f_k(\\x_n, 0) &= \\W_k\\x_n. \\end{aligned} \\end{equation} Then, we simply set $\\gamma = 0$ in \\Cref{fn:deri-solution1} of the final solution: \\begin{equation} \\begin{aligned} f_q(\\x_m, m) &= (\\W_q\\x_m)e^{im\\theta},\\\\ f_k(\\x_n, n) &= (\\W_k\\x_n)e^{in\\theta}. \\end{aligned} \\end{equation}   \\subsubsection{Computational efficient realization of rotary matrix multiplication}\\label{appendix:rope-efficient} Taking the advantage of the sparsity of $\\R^d_{\\Theta, m}$ in \\Cref{fn:rope-RMat}, a more computational efficient realization of a multiplication of $R^d_\\Theta$ and $\\x \\in \\mathbb{R}^d$ is: \\begin{equation} \\R^d_{\\Theta, m}\\x = \\begin{pmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ \\vdots\\\\ x_{d-1}\\\\ x_d \\end{pmatrix} \\otimes \\begin{pmatrix} \\cos{m\\theta_1} \\\\ \\cos{m\\theta_1} \\\\ \\cos{m\\theta_2} \\\\ \\cos{m\\theta_2} \\\\ \\vdots \\\\ \\cos{m\\theta_{d/2}} \\\\ \\cos{m\\theta_{d/2}} \\end{pmatrix} + \\begin{pmatrix} -x_2\\\\ x_1\\\\ -x_4\\\\ x_3\\\\ \\vdots\\\\ -x_d\\\\ x_{d-1} \\end{pmatrix} \\otimes \\begin{pmatrix} \\sin{m\\theta_1}\\\\ \\sin{m\\theta_1}\\\\ \\sin{m\\theta_2}\\\\ \\sin{m\\theta_2}\\\\ \\vdots\\\\ \\sin{m\\theta_{d/2}}\\\\ \\sin{m\\theta_{d/2}} \\end{pmatrix} \\end{equation}     \\subsubsection{Long-term decay of RoPE}\\label{appendix:long-term-decay} We can group entries of vectors $\\q=\\W_q\\x_m$ and $\\k=\\W_k\\x_n$ in pairs, and the inner product of RoPE in \\Cref{fn:rope-qk} can be written as a complex number multiplication.  \\begin{equation} (\\R^d_{\\Theta, m}\\W_q\\x_m)^\\intercal(\\R^d_{\\Theta, n}\\W_k\\x_n) = \\operatorname{Re}\\bigg[\\sum_{i=0}^{d/2-1}\\q_{[2i:2i+1]}\\k_{[2i:2i+1]}^{*}e^{i(m-n)\\theta_{i}}\\bigg] \\end{equation} where $\\q_{[2i:2i+1]}$ represents the $2i^{th}$ to $(2i+1)^{th}$ entries of $\\q$. Denote $h_i=\\q_{[2i:2i+1]}\\k_{[2i:2i+1]}^{*}$ and $S_j=\\sum_{i=0}^{j-1}e^{i(m-n)\\theta_i}$, and let $h_{d/2}=0$ and $S_0=0$, we can rewrite the summation using Abel transformation  \\begin{equation} \\sum_{i=0}^{d/2-1}\\q_{[2i:2i+1]}\\k_{[2i:2i+1]}^{*}e^{i(m-n)\\theta_{i}}=\\sum_{i=0}^{d/2-1}h_i(S_{i+1}-S_{i})=-\\sum_{i=0}^{d/2-1}S_{i+1}(h_{i+1}-h_i). \\end{equation}  Thus,  \\begin{equation} \\begin{aligned} \\bigg\\vert\\sum_{i=0}^{d/2-1}\\q_{[2i:2i+1]}\\k_{[2i:2i+1]}^{*}e^{i(m-n)\\theta_{i}}\\bigg\\vert &= \\bigg\\vert\\sum_{i=0}^{d/2-1}S_{i+1}(h_{i+1}-h_i)\\bigg\\vert\\\\ &\\leq \\sum_{i=0}^{d/2-1}\\vert S_{i+1}\\vert\\vert(h_{i+1}-{h_i})\\vert\\\\ &\\leq \\big(\\max_i\\vert h_{i+1}-h_i\\vert\\big)\\sum_{i=0}^{d/2-1}\\vert S_{i+1}\\vert \\end{aligned} \\end{equation} Note that the value of $\\frac{1}{d/2}\\sum_{i=1}^{d/2}\\vert S_i\\vert$ decay with the relative distance $m-n$ increases by setting $\\theta_i=10000^{-2i/d}$, as shown in \\Cref{fig:decay}.  \\begin{figure} \\centering \\includegraphics[width=0.7\\textwidth]{images/long-term-decay.pdf} \\caption{Long-term decay of RoPE.} \\label{fig:decay} \\end{figure}   "
            },
            {
                "section_name": "Experiments and Evaluation_4",
                "paragraphs": "\\label{sec:experiment} \\C  We evaluate the proposed RoFormer on various NLP tasks as follows. We validate the performance of the proposed solution on machine translation task \\Cref{subsec:mt}. Then, we compare our RoPE implementation with BERT\\cite{Devlin2019BERTPO} during the pre-training stage in \\Cref{subsec:pretrain}. Based on the pre-trained model, in \\Cref{subsec:glue}, we further carry out evaluations across different downstream tasks from GLUE benchmarks\\cite{glue}. In Addition, we conduct experiments using the proposed RoPE with the linear attention of PerFormer \\cite{performer} in \\Cref{subsec:peformer}. By the end, additional tests on Chinese data are included in \\Cref{subsec:chinese}. All the experiments were run on two cloud severs with 4 x V100 GPUs.  "
            },
            {
                "section_name": "Machine Translation_8",
                "paragraphs": "We first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks. \\subsubsection{Experimental Settings} We choose the standard WMT 2014 English-German dataset\\cite{mtdataset}, which consists of approximately 4.5 million sentence pairs. We compare to the transformer-based baseline alternative \\cite{Vaswani:2017}. \\subsubsection{Implementation details} We carry out some modifications on  self-attention layer of the baseline model \\cite{Vaswani:2017} to enable RoPE to its learning process. We replicate the setup for English-to-German translation with a vocabulary of 37k based on a joint source and target byte pair encoding(BPE)\\cite{bpe}. During the evaluation, a single model is obtained by averaging the last 5 checkpoints. The result uses beam search with a beam size of 4 and length penalty 0.6.  We implement the experiment in PyTorch in the fairseq toolkit (MIT License)\\cite{fairseq}.  Our model is optimized with the Adam optimizer using $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, learning rate is increased linearly from $1e-7$ to $5e-4$ and then decayed proportionally to the inverse square root of the step number. Label smoothing with 0.1 is also adopted. We report the BLEU\\cite{bleu} score on the test set as the final metric.  \\subsubsection{Results} We train the baseline model and our RoFormer under the same settings and report the results in \\Cref{tb:mt}. As can be seen, our model gives better BLEU scores compared to the baseline Transformer.  \\begin{table} \\caption{The proposed RoFormer gives better BLEU scores compared to its baseline alternative \\cite{Vaswani:2017} on the WMT 2014 English-to-German translation task\\cite{mtdataset}.} \\label{tb:mt} \\centering \\begin{tabular}{ll} \\toprule Model     & BLEU \\\\ \\midrule Transformer-base\\cite{Vaswani:2017} & 27.3\\\\ RoFormer & \\textbf{27.5} \\\\ \\bottomrule \\end{tabular} \\end{table}     "
            },
            {
                "section_name": "Pre-training Language Modeling_9",
                "paragraphs": "The second experiment is to validate the performance of our proposal in terms of learning contextual representations. To achieve this, we replace  the original sinusoidal position encoding of BERT with our RoPE during the pre-training step.  \\subsubsection{Experimental Settings} We use the BookCorpus \\cite{moviebook} and the Wikipedia Corpus \\cite{wikidump} from Huggingface Datasets library (Apache License 2.0) for pre-training. The corpus is further split into train and validation sets at 8:2 ratio. We use the masked language-modeling (MLM) loss values of the training process as an evaluation  metric. The well-known BERT \\cite{Devlin2019BERTPO} is adopted as our baseline model. Note that we use bert-base-uncased in our experiments.  \\subsubsection{Implementation details} For RoFormer, we replace the sinusoidal position encoding in the self-attention block of the baseline model with our proposed RoPE and realizes self-attention according to \\Cref{fn:rope-qk}. We train both BERT and RoFormer with batch size 64 and maximum sequence length  of 512 for 100k steps. AdamW \\cite{adamw} is used as the optimizer with learning rate 1e-5.  \\subsubsection{Results} The MLM loss during pre-training is shown on the left plot of \\Cref{fig:pretrain-loss}. Compare to the vanilla BERT, RoFormer experiences faster convergence.  \\begin{figure}[hbt] \\centering \\includegraphics[width=0.48\\textwidth]{images/loss.pdf} \\includegraphics[width=0.48\\textwidth]{images/performer_loss.pdf} \\caption{Evaluation of RoPE in language modeling pre-training. \\textbf{Left}: training loss for BERT and RoFormer. \\textbf{Right}: training loss for PerFormer with and without RoPE.} \\label{fig:pretrain-loss} \\end{figure}  "
            },
            {
                "section_name": "Fine-tuning on GLUE tasks_10",
                "paragraphs": "Consistent with the previous experiments, we fine-tune the weights of our pre-trained RoFormer across various GLUE tasks in order to evaluate its generalization ability on the downstream NLP tasks.  \\subsubsection{Experimental Settings} We look at several datasets from GLUE, i.e. MRPC \\cite{mrpc}, SST-2 \\cite{sst2}, QNLI \\cite{qnli}, STS-B \\cite{stsb}, QQP \\cite{qqp} and MNLI \\cite{mnli}. We use F1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation metrics.  \\subsubsection{Implementation details} We use Huggingface Transformers library (Apache License 2.0)\\cite{wolf-etal-2020-transformers} to fine-tune each of the aforementioned downstream tasks for 3 epochs, with a maximum sequence length of 512, batch size of 32 and learning rates {2,3,4,5}e-5. Following \\cite{Devlin2019BERTPO},  we report the best-averaged results on the validation set. \\begin{table}[hbt] \\caption{Comparing RoFormer and BERT by fine tuning on downstream GLEU tasks.} \\label{tb:glue-eval} \\centering \\begin{tabular}{lcccccc} \\toprule Model & MRPC & SST-2 & QNLI & STS-B & QQP & MNLI(m/mm)\\\\ \\midrule BERT\\cite{Devlin2019BERTPO} & 88.9 & 93.5 & 90.5 & 85.8 & 71.2 & 84.6/83.4\\\\ RoFormer & \\textbf{89.5}& 90.7 & 88.0 & \\textbf{87.0} & \\textbf{86.4} & 80.2/79.8 \\\\ \\bottomrule \\end{tabular} \\end{table} \\subsubsection{Results} The evaluation results of the fine-tuning tasks are reported in \\Cref{tb:glue-eval}. As can be seen, RoFormer can significantly outperform  BERT in three out of six datasets, and the improvements are considerable.  "
            },
            {
                "section_name": "Performer with RoPE_11",
                "paragraphs": "Performer \\cite{performer} introduces an alternative attention mechanism, linear attention, which is designed to avoid quadratic computation cost that scales with input sequence length. As discussed in \\Cref{sec: prop of RoPE}, the proposed RoPE can be easily implemented in the PerFormer model to realize the relative position encoding while keeping its linearly scaled complexity in self-attention. We  demonstrate its performance with  the pre-training task of language modeling.  \\subsubsection{Implementation details} We carry out tests on the Enwik8 dataset \\cite{enwik8}, which is from English Wikipedia that includes markup, special characters and text in other languages in addition to English text. We incorporate RoPE into the 12 layer char-based PerFormer with 768 dimensions and 12 heads\\footnote{For this experiment, we adopt code (MIT License) from \\url{https://github.com/lucidrains/performer-pytorch}}. To better illustrate the efficacy of RoPE,  we report the loss curves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size 128 and a fixed maximum sequence length of 1024, etc.  \\subsubsection{Results} As shown on the right plot of \\Cref{fig:pretrain-loss}, substituting RoPE into Performer leads to rapid convergence and lower loss under the same amount of training steps. These improvements, in addition to the linear complexity,  make Performer more attractive.  "
            },
            {
                "section_name": "Evaluation on Chinese Data_12",
                "paragraphs": "In addition to experiments on English data, we show additional results on Chinese data.  To validate the performance of RoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters.  \\subsubsection{Implementation} In these experiments, we carried out some modifications on WoBERT \\cite{zhuiyipretrainedmodels} by replacing the absolute position embedding with our proposed RoPE. As a cross-comparison with other pre-trained Transformer-based models in Chinese, i.e. BERT \\cite{Devlin2019BERTPO}, WoBERT \\cite{zhuiyipretrainedmodels}, and NEZHA \\cite{nezha},  we tabulate their tokenization level and position embedding information in \\Cref{tb:roformer-tokpos-imp}.  \\begin{table}[h] \\caption{Cross-comparison between our RoFormer and other pre-trained models on Chinese data. 'abs' and 'rel' annotates absolute position embedding and relative position embedding, respectively. } \\label{tb:roformer-tokpos-imp} \\centering \\begin{tabular}{lcccc} \\toprule \\makecell[c]{Model} & BERT\\cite{Devlin2019BERTPO} & WoBERT\\cite{zhuiyipretrainedmodels} & NEZHA\\cite{nezha} & RoFormer \\\\ \\midrule Tokenization level & char & word & char & word \\\\ Position embedding & abs. & abs. & rel. & RoPE \\\\ \\bottomrule \\end{tabular} \\end{table}  \\subsubsection{Pre-training} We pre-train RoFormer on approximately 34GB of data collected from Chinese Wikipedia, news and forums. The pre-training is carried out in multiple stages with changing batch size and maximum input sequence length in order to adapt the model to various scenarios. As shown in \\Cref{tb:roformer-pretrain}, the accuracy of RoFormer elevates with an increasing upper bound of sequence length, which demonstrates the ability of RoFormer in dealing with long texts. We claim that this is the attribute to the excellent generalizability of the proposed RoPE.  \\begin{table}[hbt] \\caption{Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various consecutive stages. In each stage, we train the model with a specific combination of maximum sequence length and batch size. } \\label{tb:roformer-pretrain} \\centering \\begin{tabular}{lccccc} \\toprule Stage & Max seq length & Batch size & Training steps & Loss & Accuracy \\\\ \\midrule 1 & 512 &  256 & 200k & 1.73 & 65.0\\% \\\\ 2 & 1536 & 256 & 12.5k & 1.61 & 66.8\\% \\\\ 3 & 256 & 256 & 120k & 1.75 & 64.6\\% \\\\ 4 & 128 & 512 & 80k & 1.83 & 63.4\\% \\\\ 5 & 1536 & 256 & 10k & 1.58 & 67.4\\% \\\\ 6 & 512 & 512 & 30k & 1.66 & 66.2\\% \\\\ \\bottomrule \\end{tabular} \\end{table}    \\subsubsection{Downstream Tasks \\& Dataset} We choose Chinese AI and Law 2019 Similar Case Matching (CAIL2019-SCM)\\cite{cail2019scm} dataset to illustrate the ability of RoFormer in dealing with long texts, i.e., semantic text matching. CAIL2019-SCM contains 8964 triplets of cases published by the Supreme People's Court of China. The input triplet, denoted as (A, B and C), are fact descriptions of three cases. The task is to predict whether the pair (A, B) is closer than (A, C) under a predefined similarity measure. Note that existing methods mostly cannot perform significantly on CAIL2019-SCM dataset due to the length of documents (i.e., mostly more than 512 characters). We split train, validation and test sets based on the well-known  ratio  6:2:2.  \\subsubsection{Results} We apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths. The model is compared with the pre-trained BERT and WoBERT model on the same pre-training data,  as shown in \\Cref{tb:roformer-cail2019}. With short text cut-offs, i.e., 512, the result from RoFormer is comparable to WoBERT and is slightly better than the BERT implementation. However, when increasing the maximum input text length to 1024, RoFormer outperforms WoBERT by an absolute improvement of 1.5\\%.  \\begin{table}[hbt] \\caption{Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off sequence length. The results are presented in terms of percent accuracy.} \\label{tb:roformer-cail2019} \\centering \\begin{tabular}{lll} \\toprule \\makecell[c]{Model} & Validation & Test \\\\ \\midrule BERT-512 & 64.13\\% & 67.77\\% \\\\ WoBERT-512 & 64.07\\% & 68.10\\% \\\\ \\textbf{RoFormer-512} & 64.13\\% & 68.29\\% \\\\ \\textbf{RoFormer-1024} & \\textbf{66.07}\\% & \\textbf{69.79}\\% \\\\ \\bottomrule \\end{tabular} \\end{table}  \\subsubsection{Limitations of the work} Although we provide theoretical groundings as well as promising experimental justifications, our method is limited by following facts: \\begin{itemize} \\item Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces, there lacks of thorough explanations on why it converges faster than baseline models that incorporates other position encoding strategies. \\item Although we have proved that our model has favourable property of long-term decay for intern-token products, \\Cref{sec: prop of RoPE}, which is similar to the existing position encoding mechanisms, our model shows superior performance on long texts than peer models, we have not come up with a faithful explanation. \\end{itemize} Our proposed RoFormer is built upon the  Transformer-based infrastructure, which requires  hardware resources for pre-training purpose.   "
            },
            {
                "section_name": "Conclusions_5",
                "paragraphs": " In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolution position information being encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark datasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show that our proposed RoFormer can achieve better performance on long texts task.   \\bibliographystyle{unsrtnat} \\bibliography{references}  \\end{document}   "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2104.09864/roformer_RoPE_v2.png",
                "caption": "Implementation of Rotary Position Embedding(RoPE). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/long-term-decay.png",
                "caption": "Long-term decay of RoPE.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/loss.png",
                "caption": "Evaluation of RoPE in language modeling pre-training. \\textbf{Left}: training loss for BERT and RoFormer. \\textbf{Right}: training loss for PerFormer with and without RoPE.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/performer_loss.png",
                "caption": "Evaluation of RoPE in language modeling pre-training. \\textbf{Left}: training loss for BERT and RoFormer. \\textbf{Right}: training loss for PerFormer with and without RoPE.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/table1.tex",
                "caption": "The proposed RoFormer gives better BLEU scores compared to its baseline alternative \\cite{Vaswani:2017} on the WMT 2014 English-to-German translation task\\cite{mtdataset}.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/table2.tex",
                "caption": "Comparing RoFormer and BERT by fine tuning on downstream GLEU tasks.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/table3.tex",
                "caption": "Cross-comparison between our RoFormer and other pre-trained models on Chinese data. 'abs' and 'rel' annotates absolute position embedding and relative position embedding, respectively. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/table4.tex",
                "caption": "Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various consecutive stages. In each stage, we train the model with a specific combination of maximum sequence length and batch size. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2104.09864/table5.tex",
                "caption": "Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off sequence length. The results are presented in terms of percent accuracy.",
                "description": ""
            }
        ]
    },
    "2105.08050": {
        "title": "Pay Attention to MLPs",
        "abstract": "Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.",
        "tldr": "This work proposes a simple network architecture, gMLP, based on MLPs with gating, and shows that it can perform as well as Transformers in key language and vision applications and can scale as much as Transformers over increased data and compute.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "\\label{sec:intro}  Transformers~\\cite{vaswani2017attention} have enabled many  breakthroughs in natural language processing (e.g.,~\\cite{devlin2018bert, yang2019xlnet,liu2019roberta,raffel2019exploring,brown2020language}) and have been shown to work well for computer vision (e.g., \\cite{dosovitskiy2020image, touvron2020training,carion2020end, liu2021swin}). Thanks to this success, Transformers have largely replaced LSTM-RNN~\\cite{lstm} as the default architecture in NLP, and have become an appealing alternative to ConvNets~\\cite{lecun,krizhevsky2012imagenet,simonyan2014very,szegedy2015going,he2016deep,tan2019efficientnet} in computer vision.  The Transformer architecture combines two important concepts: (1) a recurrent-free architecture which computes the representations for each individual token in parallel, and (2) multi-head self-attention blocks which aggregate spatial information across tokens. On one hand, the attention mechanism~\\cite{bahdanau2014neural} introduces the inductive bias that the spatial interactions should be dynamically parameterized based on the input representations. On the other hand, it is known that MLPs with static parameterization can represent arbitrary functions~\\cite{hornik1989multilayer}. It therefore remains an open question whether the inductive bias in self-attention is essential to the remarkable effectiveness of Transformers.  Here we study the necessity of self-attention modules in key language and vision applications of Transformers. Specifically, we propose an MLP-based alternative to Transformers without self-attention, which simply consists of channel projections and spatial projections with static parameterization. We experiment with several design choices for this architecture and find spatial projections work well when they are linear and paired with multiplicative gating (Figure~\\ref{fig:architecture}). We name the model \\textbf{\\gffn} because it is built out of basic MLP layers with gating.   We apply \\gffn to image classification and obtain strong results on ImageNet. \\gffn achieves comparable performance with DeiT~\\cite{touvron2020training}, namely Vision Transformer (ViT)~\\cite{dosovitskiy2020image} with improved regularization, in a similar training setup. With 66\\% less parameters, a \\gffn model is 3\\% more accurate than MLP-Mixer~\\cite{tolstikhin2021mlpmixer}. Together with Tolstikhin et al.~\\cite{tolstikhin2021mlpmixer}, Melas-Kyriazi~\\cite{melaskyriazi2021doyou}, Touvron et al.~\\cite{touvron2021resmlp} and Ding et. al.~\\cite{ding2021repmlp}, our results question the necessity of self-attention layers in Vision Transformers.  We apply \\gffn to masked language modeling (MLM) in the BERT~\\cite{devlin2018bert} setup, one of the most well-established applications of Transformers, and find that it is as good as Transformers at minimizing perplexity during pretraining. Our experiments indicate that perplexity is only correlated with model capacity and is insensitive to the presence of self-attention. As capacity increases, we observe that both pretraining and finetuning metrics for \\gffn{s} improve as quickly as for Transformers. This is remarkable because it indicates \\gffn{s} scale just as well as Transformers despite the absence of self-attention, and any performance gap can always be offset by training a larger model with increased data and compute. With a standard 256-batch size $\\times$ 1M-step training setup as in original BERT, a large gMLP model achieves 87.7\\% accuracy on MNLI and 82.1\\% F1 on SQuAD v2.0. Note, these are better than the BERT\\textsubscript{large} results reported in Devlin et al.~\\cite{devlin2018bert} obtained using Transformers.  For BERT's finetuning, Transformers can be more practically advantageous over \\gffn{s} on tasks that require cross-sentence alignment (e.g., by 0.8\\% on MNLI-m in the 300M-param regime), even with similar pretraining perplexity. This problem can be addressed by making \\gffn{s} substantially larger---3$\\times$ as large as Transformers. A more practical solution is to blend in only a tiny bit of self-attention---a single-head self-attention with size up to 128 is sufficient to make \\gffn{s} outperform Transformers on all NLP tasks we evaluated with even better parameter efficiency. The improvement is sometimes very significant (e.g., +4.4\\% on SQuAD v2.0 over BERT\\textsubscript{large}).  Overall, the surprising effectiveness of \\gffn{s} in both vision and NLP domains suggest that self-attention is not a necessary ingredient for scaling up machine learning models, although it can be a useful addition depending on the task. With increased data and compute, models with simpler spatial interaction mechanisms such as gMLP can be as powerful as Transformers and the capacity allocated to self-attention can be either removed or substantially reduced.  \\begin{figure}[t] \\centering \\begin{minipage}{0.5\\linewidth} \\includegraphics[width=0.95\\linewidth]{figures/gmlp-overview.pdf} \\end{minipage} \\begin{minipage}{0.475\\linewidth} \\begin{lstlisting}[ language=python, title={Pseudo-code for the \\gffn block}, captionpos=t] def gmlp_block(x, d_model, d_ffn): shortcut = x x = norm(x, axis=\"channel\") x = proj(x, d_ffn, axis=\"channel\") x = gelu(x) x = spatial_gating_unit(x) x = proj(x, d_model, axis=\"channel\") return x + shortcut  def spatial_gating_unit(x): u, v = split(x, axis=\"channel\") v = norm(v, axis=\"channel\") n = get_dim(v, axis=\"spatial\") v = proj(v, n, axis=\"spatial\", init_bias=1) return u * v \\end{lstlisting} \\end{minipage} \\caption{Overview of the \\gffn architecture with Spatial Gating Unit (SGU). The model consists of a stack of $L$ blocks with identical structure and size. All projection operations are linear and ``$\\odot$'' refers to element-wise multiplication (linear gating). The input and output protocols follow BERT for NLP and ViT for vision. Unlike Transformers, \\gffn{s} do not require positional encodings, nor is it necessary to mask out the paddings during NLP finetuning.} \\label{fig:architecture} \\end{figure}  "
            },
            {
                "section_name": "Model_2",
                "paragraphs": " Our model, \\gffn, consists of a stack of $L$ blocks with identical size and structure. Let $X \\in \\mathbb{R}^{n \\times d}$ be the token representations with sequence length $n$ and dimension $d$. Each block is defined as: \\begin{align} Z = \\sigma(XU), \\qquad \\tilde{Z} &= s(Z), \\qquad Y = \\tilde{Z}V \\end{align} where $\\sigma$ is an activation function such as GeLU~\\cite{hendrycks2016gaussian}. $U$ and $V$ define linear projections along the channel dimension---the same as those in the FFNs of Transformers (e.g., their shapes are 768$\\times$ 3072 and 3072$\\times$ 768 for BERT\\textsubscript{base}). Shortcuts, normalizations and biases are omitted for brevity.  A key ingredient in the aforementioned formulation is $s(\\cdot)$, a layer which captures spatial interactions (see below). When $s$ is an identity mapping, the above transformation degenerates to a regular FFN, where individual tokens are processed independently without any cross-token communication. One of our major focuses is therefore to design a good $s$ capable of capturing complex spatial interactions across tokens. The overall block layout is inspired by inverted bottlenecks~\\cite{sandler2018mobilenetv2} which define $s(\\cdot)$ as a spatial depthwise convolution. Note, unlike Transformers, our model \\emph{does not require position embeddings} because such information will be captured in $s(\\cdot)$.  Our model uses exactly the same input and output protocols as BERT (for NLP) and ViT (for vision). For example, when finetuning on language tasks, we concatenate together multiple text segments followed by paddings, and the predictions are deduced from the last-layer representation of a reserved \\texttt{<cls>} symbol. Although many of these protocols were introduced for Transformers and hence can be suboptimal for \\gffn{s}, strictly following them helps avoid confounding factors in our experiments and makes our layers more compatible with existing Transformer implementations.   "
            },
            {
                "section_name": "Spatial Gating Unit_1",
                "paragraphs": "\\label{subsec:spatial-interaction} To enable cross-token interactions, it is necessary for the layer $s(\\cdot)$ to contain a contraction operation over the spatial dimension. The simplistic option would be a linear projection: \\begin{equation} f_{W, b}(Z) = WZ + b \\label{eq:spatial-proj} \\end{equation} where $W \\in \\mathbb{R}^{n \\times n}$ is a matrix for which the size is the same as the sequence length, $n$, and $b$ refers token-specific biases. For example, if the padded input sequence has 128 tokens, the shape for $W$ will be 128$\\times$128. Unlike self-attention where $W(Z)$ is dynamically generated from $Z$, the spatial projection matrix $W$ here in Equation~\\eqref{eq:spatial-proj} is independent from the input representations.  In this work, we formulate layer $s(\\cdot)$ as the output of linear gating: \\begin{align} s(Z) = Z \\odot f_{W, b}(Z) \\label{eq:spatial-gating} \\end{align} where $\\odot$ denotes element-wise multiplication. For training stability, we find it critical to initialize $W$ as near-zero values and $b$ as ones, meaning that $f_{W, b}(Z) \\approx \\mathbf{1}$ and therefore $s(Z) \\approx Z$ at the beginning of training. This initialization ensures each \\gffn block behaves like a regular FFN at the early stage of training, where each token is processed independently, and only gradually injects spatial information across tokens during the course of learning.  We further find it effective to split $Z$ into two independent parts ($Z_1$, $Z_2$) along the channel dimension for the gating function and for the multiplicative bypass: \\begin{align} s(Z) = Z_1 \\odot f_{W, b}(Z_2)  \\label{eq:spatial-gating-independent} \\end{align} We also normalize the input to $f_{W, b}$ which empirically improves stability of large NLP models. This gives us the unit illustrated in Figure~\\ref{fig:architecture}, which we refer to as the \\emph{Spatial Gating Unit} (SGU) in the rest of the paper. In Table~\\ref{tab:baselines}, we provide ablation studies to compare SGU with several other variants of $s(\\cdot)$, showing that it works better and narrows the performance gap with self-attention.  \\paragraph{Connections to Existing Layers.} The overall formulation of SGU resembles Gated Linear Units (GLUs)~\\cite{dauphin2017language, shazeer2020glu, wu2019pay} as well as earlier works including Highway Networks~\\cite{srivastava2015highway} and LSTM-RNNs~\\cite{lstm}. A key distinction is that our gating is computed based on a projection over the spatial (cross-token) dimension rather than the channel (hidden) dimension. SGU is also related to Squeeze-and-Excite (SE) blocks~\\cite{hu2018squeeze} in terms of element-wise multiplication. However, different from SE blocks, SGU does not contain cross-channel projections at all, nor does it enforce permutation invariance (a key feature for content-based attentive modules) due to its static parameterization for the spatial transformation. The spatial projection in SGU could in theory learn to express superficial depthwise convolutions---unlike typical depthwise convolutions with channel-specific filters, SGU learns only a single transformation shared across channels. Finally, we note SGUs offer an alternative mechanism to capture high-order relationships other than self-attention. Specifically, the output for Equation~\\eqref{eq:spatial-gating} contains up to 2nd-order interactions (e.g., $z_iz_j$) whereas output for self-attention (assuming no nonlinearity) contains up to 3rd-order interactions (e.g., $q_ik_jv_k$). In terms of computation cost, SGU has $n^2 e / 2$ multiply-adds which is comparable to the $2n^2 d$ of dot-product self-attention.\\footnote{The input channel size $e$ for SGU is typically larger than the input channel size $d$ for self-attention, because the former is applied in the middle of the block after a channel expansion.} Both are linear over the input channel size and quadratic over the sequence length $n$.  \\FloatBarrier "
            },
            {
                "section_name": "Image Classification_3",
                "paragraphs": "\\label{sec:vision} Here we examine \\gffn in the vision domain by applying it to the image classification task on ImageNet~\\cite{deng2009imagenet} without using extra data. We compare our MLP-like models with recent attentive models based on vanilla Transformers, including Vision Transformer (ViT)~\\cite{dosovitskiy2020image}, DeiT~\\cite{touvron2020training} (ViT with improved regularization), and several other representative convolutional networks.  Table~\\ref{tab:vision-configs} summarizes the configurations of our \\gffn image classification models. The input and output protocols follow ViT/B16 where the raw image is converted into 16$\\times$16 patches at the stem. The depth and width are chosen so that the models are comparable with ViT/DeiT in capacity. Like Transformers, we find \\gffn{s} tend to drastically overfit the training data. We therefore apply a similar regularization recipe as the one used in DeiT.\\footnote{Unlike DeiT, we do not use repeated augmentation or random erasing.} To avoid extensive tuning, we adjust only the strengths of stochastic depth~\\cite{huang2016deep} as we move from smaller to larger models in Table~\\ref{tab:vision-configs}. All the other hyperparameters remain shared across our three models. See Appendix~\\ref{sec:vision-hparams} for details. \\begin{table}[h] \\centering \\small \\caption{Architecture specifications of \\gffn models for vision. The survival probability of stochastic depth is the only hyperparameter change as we move from smaller to larger models.} \\begin{tabular}{@{}l|ccc|cc|c@{}} \\toprule \\multicolumn{1}{c}{} & \\#L & $d_\\mathrm{model}$ & $d_\\mathrm{ffn}$ & Params (M) & FLOPs (B) & Survival Prob \\\\ \\midrule \\gffn-Ti & 30 & 128 & 768 & 5.9 & 2.7 & 1.00 \\\\ \\gffn-S & 30 & 256 & 1536 & 19.5 & 8.9 & 0.95 \\\\ \\gffn-B & 30 & 512 & 3072 & 73.4 & 31.6 & 0.80 \\\\ \\bottomrule \\end{tabular} \\label{tab:vision-configs} \\end{table} \\vspace{-0.3cm}   Our ImageNet results are summarized in Table~\\ref{tab:vision-configs} and Figure~\\ref{fig:vision-main}. It is interesting to see that \\gffn{s} are comparable with DeiT~\\cite{touvron2020training}, namely ViT~\\cite{dosovitskiy2020image} trained using improved regularization. The results suggest that models without self-attention can be as data-efficient as Transformers for image classification. In fact, when the models are properly regularized, their accuracies seem better correlated with capacity instead of the presence of self-attention. Moreover, the accuracy-parameter/FLOPs tradeoff of \\gffn{s} surpasses all concurrently proposed MLP-like architectures~\\cite{tolstikhin2021mlpmixer, melaskyriazi2021doyou, touvron2021resmlp}, which we attribute to the effectiveness of our Spatial Gating Unit (see Table~\\ref{tab:baselines} in the next section for an ablation). We also note while \\gffn{s} are competitive with vanilla Transformers, their performance is behind the best existing ConvNet models (e.g., \\cite{brock2021high, tan2021efficientnetv2}) or hybrid models (e.g., \\cite{bello2021lambdanetworks, vaswani2021scaling, srinivas2021bottleneck, wu2021cvt, liu2021swin}).  \\begin{table}[h] \\centering \\caption{ImageNet-1K results without extra data.} \\resizebox{.805\\textwidth}{!}{ \\begin{threeparttable} \\begin{tabular}{@{}l|cccc@{}} \\toprule \\multirow{2}{*}{Model} & ImageNet Top-1 (\\%)$^*$ & Input Resolution & Params (M) & MAdds (B) \\\\ \\cmidrule(l){2-5} & \\multicolumn{4}{c}{ConvNets} \\\\ \\midrule ResNet-152~\\cite{he2016deep} & 78.3 & 224 & 60 & 11.3 \\\\ RegNetY-8GF~\\cite{radosavovic2020designing} & 81.7 & 224 & 39 & 8.0 \\\\ EfficientNet-B0~\\cite{tan2019efficientnet} & 77.1 & 224 & 5 & 0.39 \\\\ EfficientNet-B3~\\cite{tan2019efficientnet} & 81.6 & 300 & 12 & 1.8 \\\\ EfficientNet-B7~\\cite{tan2019efficientnet} & 84.3 & 600 & 66 & 37.0 \\\\ NFNet-F0~\\cite{brock2021high} & 83.6 & 192 & 72 & 12.4 \\\\ \\midrule & \\multicolumn{4}{c}{Transformers} \\\\ \\midrule ViT-B/16~\\cite{dosovitskiy2020image} & 77.9 & 384 & 86 & 55.4 \\\\ ViT-L/16~\\cite{dosovitskiy2020image} & 76.5 & 384 & 307 & 190.7 \\\\ DeiT-Ti~\\cite{touvron2020training} (ViT+reg) & 72.2 & 224 & 5 & 1.3 \\\\ DeiT-S~\\cite{touvron2020training} (ViT+reg) & 79.8 & 224 & 22 & 4.6 \\\\ DeiT-B~\\cite{touvron2020training} (ViT+reg) & 81.8 & 224 & 86 & 17.5 \\\\ \\midrule & \\multicolumn{4}{c}{MLP-like$^\\dagger$} \\\\ \\midrule Mixer-B/16~\\cite{tolstikhin2021mlpmixer} & 76.4 & 224 & 59 & 12.7 \\\\ Mixer-B/16 (our setup) & 77.3 & 224 & 59 & 12.7 \\\\ Mixer-L/16~\\cite{tolstikhin2021mlpmixer} & 71.8 & 224 & 207 & 44.8 \\\\ ResMLP-12~\\cite{touvron2021resmlp} & 76.6 & 224 & 15 & 3.0 \\\\ ResMLP-24~\\cite{touvron2021resmlp} & 79.4 & 224 & 30 & 6.0 \\\\ ResMLP-36~\\cite{touvron2021resmlp} & 79.7 & 224 & 45 & 8.9 \\\\ \\gffn-Ti (ours) & 72.3 & 224 & 6 & 1.4 \\\\ \\gffn-S (ours) & 79.6 & 224 & 20 & 4.5 \\\\ \\gffn-B (ours) & 81.6 & 224 & 73 & 15.8 \\\\ \\bottomrule \\end{tabular} \\begin{tablenotes} \\small \\item[*] Standard deviation across multiple independent runs is around 0.1. \\item[$\\dagger$] Tokenization \\& embedding process at the stem can be viewed as a convolution. \\end{tablenotes} \\end{threeparttable} } \\label{tab:vision-main} \\end{table}   \\begin{figure}[h] \\begin{minipage}{0.51\\linewidth} \\centering \\includegraphics[width=0.94\\linewidth]{figures/gffn-imagenet.pdf} \\caption{ImageNet accuracy vs model capacity.} \\label{fig:vision-main} \\end{minipage} \\hfill \\begin{minipage}{0.475\\linewidth} \\centering \\includegraphics[width=1.0\\linewidth]{figures/vision-learned-filters.png} \\caption{ Spatial projection weights in gMLP-B. Each row shows the filters (reshaped into 2D) for a selected set of tokens in the same layer.} \\label{fig:vision-filters} \\end{minipage} \\end{figure}   Figure~\\ref{fig:vision-filters} visualizes the spatial projection matrices in \\gffn-B. Remarkably, the spatial weights after learning exhibit both locality and spatial invariance. In other words, each spatial projection matrix effectively learns to perform convolution with a data-driven, irregular (non-square) kernel shape.  "
            },
            {
                "section_name": "Masked Language Modeling with BERT_4",
                "paragraphs": "\\label{sec:mlm} Here we conduct empirical studies over the masked language modeling (MLM) task. The input/output protocol for both pretraining and finetuning follows BERT~\\cite{devlin2018bert}. Different from Transformer-based models, we do not use positional encodings. We also find it unnecessary to mask out \\texttt{<pad>} tokens in \\gffn blocks during finetuning as the model can quickly learn to ignore them. For ablations and case studies, all models are trained with batch size 2048, max length 128 for 125K steps over the RealNews-like subset of C4~\\cite{raffel2019exploring}. For main results, models are trained with batch size 256, max length 512 for 1M steps over the full English C4 dataset. See Appendix~\\ref{sec:mlm-hparams} for details.  Our preliminary MLM experiments show that gMLPs always learn Toeplitz-like matrices as the spatial weights (Appendix~\\ref{sec:shift-invariance}). This means gMLPs are able to learn the notion of shift invariance from data, a property naturally implied by the MLM task where any offset of the input sequence does not affect the slot filling outcome. In this case, the learned $f_{W,b}(\\cdot)$ acts like a 1-d convolution whose kernel size equals the entire sequence length (unlike depthwise convolution with channel-specific filters, here the same $W$ is shared across channels). In the following MLM experiments, we restrict $W$ to be a Toeplitz matrix to avoid redundant model parameterization (since $W$ will be Toeplitz-like regardless after learning). Note this constraint is empirically quality-neutral.  "
            },
            {
                "section_name": "Ablation: The Importance of Gating in \\gffn for BERT's Pretraining_2",
                "paragraphs": "\\label{sec:baselines}  In Table~\\ref{tab:baselines} below, we establish baselines for our ablation studies. These include: \\begin{enumerate} \\item BERT with a Transformer architecture and learnable absolute position embeddings. \\item BERT with a Transformer architecture and T5-style learnable relative position biases~\\cite{raffel2019exploring}. The biases are both layer- and head-specific as we find this yields the best results. \\item Same as above, but we remove all content-dependent terms inside the softmax and only retain the relative positional biases. This baseline is a straightforward variant of Transformers without self-attention, which can also be viewed as a Random Synthesizer~\\cite{tay2020synthesizer}. \\item MLP-Mixer~\\cite{tolstikhin2021mlpmixer} which replaces the multi-head self-attention module in Transformers with a two-layer spatial MLP. This model was developed for image classification and here we investigate it on MLM tasks using the same training setup with BERT and gMLP. \\end{enumerate}  \\begin{table}[h] \\centering\\small \\caption{MLM validation perplexities of Transformer baselines and four versions of \\gffn{s}. $f$ refers to the spatial linear projection in Equation~\\eqref{eq:spatial-proj} with input normalization. The MLP-Mixer baseline model has L=24 layers with $d_\\mathrm{model}$=768, $d_\\mathrm{spatial}$=384 and $d_\\mathrm{ffn}$=3072. Each \\gffn model has L=36 layers with $d_\\mathrm{model}$=512 and $d_\\mathrm{ffn}$ = 3072. No positional encodings are used for Mixer or \\gffn{s}.} \\begin{threeparttable} \\begin{tabular}{@{}l|c|c@{}} \\toprule Model & Perplexity$^*$ & Params (M) \\\\ \\midrule BERT\\textsubscript{base} &  4.37 &  110 \\\\ BERT\\textsubscript{base} + rel pos & 4.26 & 110 \\\\ BERT\\textsubscript{base} + rel pos - attn & 5.64 & 96 \\\\ \\midrule MLP-Mixer & 5.34 & 112 \\\\ \\midrule Linear \\gffn, $s(Z) = f(Z)$ & 5.14 & 92 \\\\ Additive \\gffn, $s(Z) = Z + f(Z)$ & 4.97 & 92 \\\\ Multiplicative \\gffn, $s(Z) = Z \\odot f(Z)$  & 4.53 & 92 \\\\ Multiplicative, Split \\gffn, $s(Z) = Z_1 \\odot f(Z_2)$, $Z=Z_1\\|Z_2$  & 4.35 & 102 \\\\  \\bottomrule \\end{tabular} \\begin{tablenotes} \\small \\item[*] Standard deviation across multiple independent runs is around 0.01. \\end{tablenotes} \\end{threeparttable} \\vspace{-0.525cm} \\label{tab:baselines} \\end{table}  We compare these baselines against several versions of \\gffn{s} with similar sizes in Table~\\ref{tab:baselines}. Note that Multiplicative, Split (last row) is the Spatial Gating Unit we describe in the method section and use in the rest of the paper. First, SGU outperforms other variants in perplexity. Secondly and remarkably, \\gffn with SGU also achieves perplexity comparable to Transformer. Note the difference between the strongest baseline (perplexity=4.26) and ours (perplexity=4.35) is insignificant relative to the perplexity change when the models are scaled (see Table~\\ref{tab:depth-scaling} in the next section). Spatial projection weights learned by \\gffn{s} are visualized in Figure~\\ref{fig:mlm-filters}.  \\begin{figure}[h] \\centering \\includegraphics[width=0.7\\linewidth]{figures/mlm-filters.png} \\caption{Visualization of the spatial filters in \\gffn learned on the MLM task. For each layer in the model we plot the row in $W$ associated with the token in the middle of the sequence. The x-axis of each subplot has a length of 128 which equal the number of tokens in the sequence. The learned filters appear to be smooth and have several types: forward-looking (e.g., 1st in 2nd row), backward-looking (e.g., 5th in 2nd row) and bi-directional (e.g., 2nd last in the last row).} \\label{fig:mlm-filters} \\vspace{-0.2cm} \\end{figure}  "
            },
            {
                "section_name": "Case Study: The Behavior of \\gffn as Model Size Increases_3",
                "paragraphs": "\\label{sec:scaling}  In Table~\\ref{tab:depth-scaling}, we investigate the scaling properties of Transformers and \\gffn{s} in BERT as their model capacity grows. Specifically, we scale the depth of these models by a factor of $\\{0.5, 1, 2, 4\\}\\times$ and report the their pretraining MLM perplexities on the validation set as well as finetuning results on the dev sets of two tasks in GLUE~\\cite{wang2018glue}. Note each individual Transformer layer is effectively two consecutive blocks: one for self-attention and one for FFN. In the table below we use the notation of 12 + 12 to refer to 12 of self-attention blocks plus 12 of FFN blocks in the Transformer baselines.  \\begin{table}[h] \\centering \\small \\caption{Pretraining and dev-set finetuning results over increased model capacity. We use the relative positional encoding scheme for Transformers which performs the best in Table~\\ref{tab:baselines}.} \\begin{tabular}{@{}l|c|c|c|cc@{}} \\toprule Model & \\#L & Params (M) & Perplexity & SST-2 & MNLI-m \\\\ \\midrule Transformer & 6+6 & 67 & \\textbf{4.91} & 90.4 & 81.5 \\\\ \\gffn  & 18  & 59 & 5.25 & 91.2 & 77.7 \\\\ \\midrule Transformer  & 12+12  & 110 & \\textbf{4.26} & 91.3 & 83.3 \\\\ \\gffn & 36 & 102 & 4.35 & 92.3 & 80.9 \\\\ \\midrule Transformer  & 24+24 & 195 & 3.83 & 92.1 & 85.2 \\\\ \\gffn & 72 & 187 & \\textbf{3.79} & 93.5 & 82.8 \\\\ \\midrule Transformer & 48+48 & 365  & 3.47 & 92.8 & 86.3 \\\\ \\gffn  & 144 & 357 & \\textbf{3.43} & 95.1 & 84.6 \\\\ \\bottomrule \\end{tabular} \\label{tab:depth-scaling} \\vspace{-0.2cm} \\end{table}  The results above show that a deep enough \\gffn is able to match and even outperform the perplexity of Transformers with comparable capacity.\\footnote{We also experimented with deeper-and-thinner Transformers (with capacity fixed) but found increasing depth further does not improve perplexity. See Appendix~\\ref{sec:deeper-thinner-tfm} for more details.} In addition, the perplexity-parameter relationships for both architecture families approximately follow a power law (left of Figure~\\ref{fig:scaling}). This implies the empirical scaling laws originally observed for Transformer-based language models~\\cite{kaplan2020scaling} might be broadly applicable across different model families.  \\begin{figure}[h] \\centering \\includegraphics[width=0.9\\linewidth]{figures/scaling.pdf} \\caption{Scaling properties with respect to perplexity and finetuning accuracies. The figures show that for pretraining, \\gffn{s} are equally good at optimizing perplexity as Transformers. For finetuning, the two model families exhibit comparable scalability despite task-specific offsets.} \\label{fig:scaling} \\end{figure}  Table~\\ref{tab:depth-scaling} also leads to an interesting observation that the pretraining perplexities across different model families are not equal in terms of finetuning. While \\gffn{s} outperform Transformers on SST-2, they are worse on MNLI. The results imply that the finetuning performance for NLP tasks is a function of not only the perplexity but also the inductive bias in the architecture. Figure~\\ref{fig:scaling} shows that despite the architecture-specific discrepancies between pretraining and finetuning, \\gffn{s} and Transformers exhibit comparable scalability (slope) on both finetuning tasks. This means one can always offset the gap by enlarging the model capacity. In other words, the results indicate that model scalability with respect to downstream metrics can be independent from the presence of self-attention.  "
            },
            {
                "section_name": "Ablation: The Usefulness of Tiny Attention in BERT's Finetuning_4",
                "paragraphs": "So far we have found that self-attention is not a required component to achieve strong MLM perplexity or scalability. At the meantime, we also identified NLP finetuning tasks where \\gffn{s} transfer less well than Transformers (Table~\\ref{tab:depth-scaling}). The fact that our MLP-like model is advantageous on SST-2 but worse on MNLI is particularly informative---the former is a single-sentence task whereas the latter involves sentence pairs (premise and hypothesis)~\\cite{williams2017broad}. We suspect the role of self-attention during finetuning is related to cross-sentence alignment.  To isolate the effect of self-attention, we experiment with a hybrid model where a tiny self-attention block is attached to the gating function of \\gffn (Figure~\\ref{fig:tiny-attn}). Since \\gffn itself is already capable in capturing spatial relationships, we hypothesize that this extra self-attention module does not have to be heavy, and that its presence is more relevant than its capacity. A typical tiny attention module in our experiments has only a single head with size 64, significantly smaller than a typical multi-head self-attention in Transformers with 12 heads and a total size of 768. In the following, we refer to the hybrid model, namely gMLP with a tiny self-attention, as \\emph{aMLP} (``a'' for attention).  \\begin{figure}[h] \\centering \\begin{minipage}{0.4\\linewidth} \\includegraphics[width=0.7\\linewidth]{figures/tiny-attention.pdf} \\end{minipage} \\begin{minipage}{0.5\\linewidth} \\begin{lstlisting}[ language=python, title={Pseudo-code for the tiny attention module}, captionpos=t] def tiny_attn(x, d_out, d_attn=64): qkv = proj(x, 3 * d_attn, axis=\"channel\") q, k, v = split(qkv, 3, axis=\"channel\") w = einsum(\"bnd,bmd->bnm\", q, k) a = softmax(w * rsqrt(d_attn)) x = einsum(\"bnm,bmd->bnd\", a, v) return proj(x, d_out, axis=\"channel\")  \\end{lstlisting} \\end{minipage} \\caption{Hybrid spatial gating unit with a tiny self-attention module. We use the normalized input of the \\gffn block (endpoint after the input normalization and right before the channel expansion) as the input to the tiny self-attention. For SGU we have $d_\\mathrm{out} = d_\\mathrm{ffn} / 2$ due to the channel split.} \\label{fig:tiny-attn} \\end{figure}  In Figure~\\ref{fig:transferability}, we investigate the transferability of MLM models via the calibration plots between their pretraining perplexities and finetuning metrics. Models evaluated include BERT\\textsubscript{base}, \\gffn and its hybrid version aMLP with a 64-d single-head self-attention (Figure~\\ref{fig:tiny-attn}). The data points were collected by varying the model depth by \\{0.5, 1, 2\\}$\\times$ or data by \\{1, 2, 4, 8\\}$\\times$. It can be seen that \\gffn{s} transfer better to SST-2 than Transformers regardless of the presence of self-attention, While \\gffn performs worse on MNLI, attaching a tiny bit of self-attention is sufficient to close the gap. In Appendix~\\ref{sec:visualize-tiny-attn} we visualize the tiny self-attention modules in aMLP over MNLI examples, showing that they are primarily responsible for the alignment between sentence pairs.  \\begin{figure}[h] \\centering \\begin{minipage}{0.4\\linewidth} \\includegraphics[width=0.9\\linewidth]{figures/pplx-vs-sst.pdf} \\end{minipage} \\begin{minipage}{0.4\\linewidth} \\includegraphics[width=0.9\\linewidth]{figures/pplx-vs-mnli.pdf} \\end{minipage} \\caption{Transferability from MLM pretraining perpexity to finetuning accuracies on GLUE. aMLP refers to gMLP enhanced with a 64-d single-head self-attention, as illustrated in Figure~\\ref{fig:tiny-attn}. In contrast, each self-attention module in the BERT baseline contains 12 heads with a total size of 768.} \\label{fig:transferability} \\end{figure}  In Figure~\\ref{fig:scaling-with-tinyattn} we put together the scaling properties of the three models, showing that aMLP (gMLP + tiny attention) consistently outperforms Transformer on both finetuning tasks.  \\begin{figure}[h] \\centering \\includegraphics[width=0.9\\linewidth]{figures/scaling-with-tinyattn.pdf} \\caption{Comparing the scaling properties of Transformers, \\gffn{s} and aMLPs (with 64-d, single-head attention). Results were obtained using the same setup in Section~\\ref{sec:scaling}.} \\label{fig:scaling-with-tinyattn} \\end{figure}  \\FloatBarrier "
            },
            {
                "section_name": "Main Results for MLM in the BERT Setup_5",
                "paragraphs": "Below we present pretraining and finetuning results in the full BERT setup. Different from ablation and case studies, here we use the full English C4 dataset and adopt a common MLM setup with batch size 256, max length 512 and 1M training steps. For fair comparison, we adjust the depth and width of \\gffn{s} to ensure comparable model capacity with the Transformer baselines. The model specifications are given in Table~\\ref{tab:perplexity-full-bert} and hyperparameters are detailed in Appendix~\\ref{sec:mlm-hparams}. For finetuning, we report the dev-set performance for SST-2 and MNLI in GLUE~\\cite{wang2018glue} and each result entry was obtained by taking the median of five independent runs. In addition, we report finetuning results on SQuAD~\\cite{rajpurkar2016squad, rajpurkar2018know} to test the models' ability in reasoning over a longer context.  \\begin{table}[h] \\caption{Model specifications in the full BERT setup.} \\label{tab:perplexity-full-bert} \\small \\centering \\begin{tabular}{@{}l|cc|cccc@{}} \\toprule & Params (M) & FLOPs (B) & \\#L & $d_\\mathrm{model}$ & $d_\\mathrm{ffn}$ \\\\ \\midrule BERT\\textsubscript{base} & 110 & 100.8  & 12+12 & 768 & 3072 \\\\ gMLP\\textsubscript{base} & 130 & 158.0 & 48 & 512 & 3072  \\\\ aMLP\\textsubscript{base} & 109 & 128.9 & 36 & 512 & 3072  \\\\ \\midrule BERT\\textsubscript{large} & 336 & 341.2 &  24+24 & 1024 & 4096  \\\\ gMLP\\textsubscript{large} & 365 & 430.1 & 96 & 768 & 3072  \\\\ aMLP\\textsubscript{large} & 316 & 370.3 & 72 & 768 & 3072 \\\\ \\midrule gMLP\\textsubscript{xlarge} & 941 & 1091.3 & 144 & 1024 & 4096 \\\\ \\bottomrule \\end{tabular} \\end{table}   \\begin{table}[h] \\centering \\caption{Pretraining perplexities and dev-set results for finetuning. ``ours'' indicates models trained using our setup. We report accuracies for SST-2 and MNLI, and F1 scores for SQuAD v1.1/2.0.} \\begin{tabular}{@{}l|c|cccc|cc@{}} \\toprule & \\multirow{2}{*}{Perplexity} & \\multirow{2}{*}{SST-2} & \\multirow{2}{*}{MNLI} & \\multicolumn{2}{c|}{SQuAD} & \\multirow{2}{*}{Attn Size} & Params \\\\ \\cmidrule(l){5-6} & & & (m/mm) & v1.1 & v2.0 & & (M)\\\\ \\midrule \\midrule BERT\\textsubscript{base}~\\cite{devlin2018bert} & -- & 92.7 & 84.4/- & 88.5 & 76.3 & 768 (64 $\\times$ 12) & 110 \\\\ \\midrule BERT\\textsubscript{base} (ours) & 4.17 & 93.8 & 85.6/85.7 & 90.2 & 78.6 & 768 (64 $\\times$ 12) & 110  \\\\ gMLP\\textsubscript{base} & 4.28 & 94.2 & 83.7/84.1 & 86.7 & 70.1 & -- & 130 \\\\ aMLP\\textsubscript{base} & 3.95 & 93.4 & 85.9/85.8 & 90.7 & 80.9 & 64 & 109 \\\\ \\midrule \\midrule BERT\\textsubscript{large}~\\cite{devlin2018bert} & -- & 93.7 & 86.6/- & 90.9 & 81.8 & 1024 (64 $\\times$ 16) & 336 \\\\ \\midrule BERT\\textsubscript{large} (ours) & 3.35 & 94.3 & 87.0/87.4 & 92.0 & 81.0 & 1024 (64 $\\times$ 16) & 336 \\\\ gMLP\\textsubscript{large} & 3.32 & 94.8 & 86.2/86.5 & 89.5 & 78.3 & -- & 365 \\\\ aMLP\\textsubscript{large} & 3.19 & 94.8 & 88.4/88.4 & 92.2 & 85.4 & 128 & 316 \\\\ \\midrule\\midrule gMLP\\textsubscript{xlarge} & 2.89 & 95.6 & 87.7/87.7 & 90.9 & 82.1 & -- & 941 \\\\ \\bottomrule \\end{tabular} \\label{tab:finetune-full-bert} \\end{table}  Results are presented in Table~\\ref{tab:finetune-full-bert}. Consistent with our findings earlier in Section~\\ref{sec:baselines} and Section~\\ref{sec:scaling}, \\gffn{s} are competitive with Transformers in terms of perplexity, especially in the larger scale setup. There are several observations related to the finetuning results:  First, on finetuning tasks where \\gffn{s} underperform Transformers, the performance gap tends to narrow as the model capacity increases. For example, while \\gffn performs worse by 8.5\\% on SQuAD-v2.0 in the base scale, the performance gap relative to the baseline decreases to 2.7\\% at the larger scale. Notably, our gMLP\\textsubscript{large} achieves 89.5\\% F1 on SQuAD-v1.1 without any self-attention or dynamic spatial parameterization~\\cite{wu2019pay}, which is well above the 88.5\\% reported for BERT\\textsubscript{base} in Devlin et al.~\\cite{devlin2018bert} and is only 1.4\\% away from the original result for BERT\\textsubscript{large}. We also include one additional data point by scaling up gMLP even further. The resulting model, gMLP\\textsubscript{xlarge}, outperforms BERT\\textsubscript{large} on SQuAD-v2.0---a difficult task involving question-answer pairs---without any self-attention. While this is not a fair comparison due to different model sizes, it is an existence proof that MLP-like models can be competitive with Transformers on challenging NLP tasks.  Furthermore, we show that blending in a tiny single-head self-attention of size either 64 or 128 is sufficient to make \\gffn{s} outperform Transformers of similar capacity, sometimes by a significant margin. For example, our hybrid model aMLP\\textsubscript{large} achieves 4.4\\% higher F1 than Transformers on SQuAD-v2.0. The results suggest that the capacity in the multi-head self-attention of Transformers can be largely redundant, and that the majority of its functionalities can be captured by the spatial gating unit in \\gffn{s}. The results also imply that the inductive biases in the spatial gating unit of \\gffn{s} and the tiny attention are complementary to each other. While the benefits of architectural inductive bias may vanish over increased compute, tiny attention does improve the practical value of \\gffn{s} in the regime that we investigate in this work.   "
            },
            {
                "section_name": "Conclusion_5",
                "paragraphs": "Since the seminal work of Vaswani et al.~\\cite{vaswani2017attention}, Transformers have been widely adopted across NLP and computer vision. This adoption has enabled many impressive results especially in NLP. To date, it is still unclear what empowers such success: is it the feedforward nature of Transformers or is it the multi-head self-attention layers in Transformers?  Our work suggests a simpler alternative to the multi-head self-attention layers in Transformers. We show that \\gffn{s}, a simple variant of MLPs with gating, can be competitive with Transformers in terms of BERT's pretraining perplexity and ViT's accuracy. \\gffn{s} are also comparable with Transformers in terms of the scalability over increased data and compute. As for BERT finetuning, we find \\gffn{s} can achieve appealing results on challenging tasks such as SQuAD without self-attention, and can significantly outperform Transformers in certain cases. We also find the inductive bias in Transformer's multi-head self-attention useful on downstream tasks that require cross-sentence alignment. However in those cases, making \\gffn substantially larger closes the gap with Transformers. More practically, blending a small single-head self-attention into \\gffn allows for an even better architecture without the need for increasing model size.   \\section*{Acknowledgements} We thank Gabriel Bender, Neil Houlsby, Thang Luong, Niki Parmar, Hieu Pham, Noam Shazeer, Ilya Sutskever, Jakob Uszkoreit and Ashish Vaswani for their feedback to the paper.  \\bibliographystyle{unsrt} \\bibliography{main}   \\appendix  \\FloatBarrier "
            },
            {
                "section_name": "Hyperparameters_6",
                "paragraphs": " \\FloatBarrier "
            },
            {
                "section_name": "Image Classification_6",
                "paragraphs": "\\label{sec:vision-hparams} All ImageNet models are trained using TPUv2 with 128 cores. Each run takes 1-4 hours to complete.  \\begin{table}[h] \\centering \\begin{tabular}{@{}l|cccc@{}} \\toprule & \\gffn-Ti & \\gffn-S & \\gffn-B & Mixer-B \\\\ \\midrule Stochastic depth survival prob & 1.00 & 0.95 & 0.80 & 0.95 \\\\ \\midrule Data augmentation & \\multicolumn{4}{c}{AutoAugment} \\\\ Repeated Augmentation & \\multicolumn{4}{c}{off} \\\\ Input resolution & \\multicolumn{4}{c}{224} \\\\ Epochs & \\multicolumn{4}{c}{300} \\\\ Batch size & \\multicolumn{4}{c}{4096} \\\\ Warmup steps & \\multicolumn{4}{c}{10K} \\\\ Hidden dropout & \\multicolumn{4}{c}{0} \\\\ GeLU dropout & \\multicolumn{4}{c}{0} \\\\ Attention dropout (if applicable) & \\multicolumn{4}{c}{0} \\\\ Classification dropout & \\multicolumn{4}{c}{0} \\\\ Random erasing prob & \\multicolumn{4}{c}{0} \\\\ EMA decay & \\multicolumn{4}{c}{0} \\\\ Cutmix $\\alpha$ & \\multicolumn{4}{c}{1.0} \\\\ Mixup $\\alpha$ & \\multicolumn{4}{c}{0.8} \\\\ Cutmix-Mixup switch prob & \\multicolumn{4}{c}{0.5} \\\\ Label smoothing & \\multicolumn{4}{c}{0.1} \\\\ Peak learning rate & \\multicolumn{4}{c}{1e-3} \\\\ Learning rate decay & \\multicolumn{4}{c}{cosine} \\\\ Optimizer & \\multicolumn{4}{c}{AdamW} \\\\ Adam $\\epsilon$ & \\multicolumn{4}{c}{1e-6} \\\\ Adam $(\\beta_1, \\beta_2)$ & \\multicolumn{4}{c}{(0.9, 0.999)} \\\\ Weight decay & \\multicolumn{4}{c}{0.05} \\\\ Gradient clipping & \\multicolumn{4}{c}{1.0} \\\\ \\bottomrule \\end{tabular} \\caption{Hyperparameters for Image classification on ImageNet-1K} \\end{table}  \\FloatBarrier "
            },
            {
                "section_name": "Masked Language Modeling_7",
                "paragraphs": "\\label{sec:mlm-hparams} MLM models for ablation studies are trained using TPUv3 with 32 cores. Each run takes 1-2 days to complete. Models in the full BERT setup are trained using TPUv2 with 128 cores. Each run takes 1-5 days to complete depending on the model size. The vocabulary consists of 32K cased SentencePieces.  \\begin{table}[t] \\centering \\begin{tabular}{@{}l|c|c@{}} \\toprule & Ablation Studies & Full Results (Table~\\ref{tab:perplexity-full-bert}) \\\\ \\midrule Data & C4/RealNews & C4/English \\\\ Max sequence length & 128 & 512 \\\\ Batch size & 2048 & 256 \\\\ Peak learning rate & 7e-4 & 1e-4 \\\\ Number of steps & 125K & 1M \\\\ \\midrule Warmup steps & \\multicolumn{2}{c}{10K} \\\\ Hidden dropout & \\multicolumn{2}{c}{0} \\\\ GeLU dropout & \\multicolumn{2}{c}{0} \\\\ Attention dropout (if applicable) & \\multicolumn{2}{c}{0} \\\\ Learning rate decay & \\multicolumn{2}{c}{Linear} \\\\ Optimizer & \\multicolumn{2}{c}{AdamW} \\\\ Adam $\\epsilon$ & \\multicolumn{2}{c}{1e-6} \\\\ Adam $(\\beta_1, \\beta_2)$ & \\multicolumn{2}{c}{(0.9, 0.999)} \\\\ Weight decay & \\multicolumn{2}{c}{0.01} \\\\ Gradient clipping & \\multicolumn{2}{c}{0} \\\\ \\bottomrule \\end{tabular} \\caption{Hyperparameters for MLM pretraining on C4.} \\end{table}  \\begin{table}[t] \\centering \\begin{tabular}{@{}l|cc|c@{}} \\toprule & SST-2 & MNLI & SQuAD v1.1/v2.0 \\\\ \\midrule Max sequence length & \\multicolumn{2}{c|}{128} & 512 \\\\ Batch size & \\multicolumn{2}{c|}{\\{16, 32\\}} & 32 \\\\ Peak learning rate & \\multicolumn{2}{c|}{\\{1e-5, 2e-5, 3e-5\\}} & 5e-5 \\\\ Number of steps/epochs & \\multicolumn{2}{c|}{5 epochs} & 8K \\\\ Warmup steps/portion & \\multicolumn{2}{c|}{10\\%} & 1K \\\\ \\midrule Hidden dropout & \\multicolumn{3}{c}{0.1} \\\\ GeLU dropout & \\multicolumn{3}{c}{0} \\\\ Attention dropout (if applicable) & \\multicolumn{3}{c}{0.1} \\\\ Learning rate decay & \\multicolumn{3}{c}{Linear} \\\\ Optimizer & \\multicolumn{3}{c}{AdamW} \\\\ Adam $\\epsilon$ & \\multicolumn{3}{c}{1e-6} \\\\ Adam $(\\beta_1, \\beta_2)$ & \\multicolumn{3}{c}{(0.9, 0.999)} \\\\ Weight decay & \\multicolumn{3}{c}{0.01} \\\\ Gradient clipping & \\multicolumn{3}{c}{0} \\\\ \\bottomrule \\end{tabular} \\caption{Hyperparameters for MLM finetuning on GLUE and SQuAD.} \\end{table}   \\FloatBarrier "
            },
            {
                "section_name": "Deep-and-Thin Transformers_7",
                "paragraphs": "\\label{sec:deeper-thinner-tfm} \\begin{table}[h] \\centering \\begin{tabular}{@{}ccccc@{}} \\toprule Perplexity & \\#L & $d_\\mathrm{model}$ & \\#heads & Params (M) \\\\ \\midrule 4.83 & 12 + 12 & 768 & 12 & 110 \\\\ 5.08 & 24 + 24 & 512 & 8 & 92 \\\\ 4.99 & 48 + 48 & 384 & 12 & 98 \\\\ 5.30 & 96 + 96 & 256 & 8 & 84 \\\\ \\bottomrule \\end{tabular} \\caption{MLM results with increasingly deeper \\& thinner Transformers. As the depth increases, we adjust the model width accordingly to maintain comparable capacity. We observe that the perplexity is insensitive to the model depth at a fixed capacity, and worsens beyond 48 layers. Note these results were obtained using a similar yet different training setup from the rest of the paper.} \\end{table}  \\FloatBarrier "
            },
            {
                "section_name": "Shift Invariance in MLM_8",
                "paragraphs": "\\label{sec:shift-invariance}  \\begin{figure}[h] \\centering \\includegraphics[width=0.9\\linewidth]{figures/tplz.png} \\caption{Spatial projection matrices learned on the MLM pretraining task without the shift invariance prior (that each individual $W$ being a Toeplitz matrix). The plots show that \\gffn learns Toeplitz-like matrices (hence the notion of shift invariance) regardless.} \\label{fig:mlm-no-tplz} \\end{figure}  \\FloatBarrier  \\begin{lstlisting}[ language=python, title={Creating a Toeplitz Matrix (used in MLM experiments)}, captionpos=t] def create_toeplitz_matrix(n): w = tf.get_variable( \"weight\", shape=[2 * n - 1], initializer=WEIGHT_INITIALIZER) r = w.shape[0].value // 2 t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[:-n] t = tf.reshape(t, [n, n + w.shape[0] - 1]) return t[:, r:-r] \\end{lstlisting}  \\FloatBarrier "
            },
            {
                "section_name": "Visualizing Tiny Attention_9",
                "paragraphs": "\\label{sec:visualize-tiny-attn} Here we visualize the attention maps of the tiny attention modules in aMLP, after finetuning on MNLI-m. Each element in the heatmap below denotes the maximum attention weight of the corresponding token pair ever received during the first half of the network.  \\begin{figure}[h] \\centering \\includegraphics[width=0.73\\linewidth]{figures/amlp-heatmap-1.pdf} \\end{figure}  \\begin{figure}[h] \\centering \\includegraphics[width=0.73\\linewidth]{figures/amlp-heatmap-2.pdf} \\caption{Attention maps in aMLP over selected examples in MNLI-m.} \\end{figure}  \\end{document}   "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2105.08050/gmlp-overview.png",
                "caption": "Overview of the \\gffn architecture with Spatial Gating Unit (SGU). The model consists of a stack of $L$ blocks with identical structure and size. All projection operations are linear and ``$\\odot$'' refers to element-wise multiplication (linear gating). The input and output protocols follow BERT for NLP and ViT for vision. Unlike Transformers, \\gffn{s} do not require positional encodings, nor is it necessary to mask out the paddings during NLP finetuning.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/gffn-imagenet.png",
                "caption": "ImageNet accuracy vs model capacity.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/vision-learned-filters.png",
                "caption": "ImageNet accuracy vs model capacity.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/mlm-filters.png",
                "caption": "Visualization of the spatial filters in \\gffn learned on the MLM task. For each layer in the model we plot the row in $W$ associated with the token in the middle of the sequence. The x-axis of each subplot has a length of 128 which equal the number of tokens in the sequence. The learned filters appear to be smooth and have several types: forward-looking (e.g., 1st in 2nd row), backward-looking (e.g., 5th in 2nd row) and bi-directional (e.g., 2nd last in the last row).",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/scaling.png",
                "caption": "Scaling properties with respect to perplexity and finetuning accuracies. The figures show that for pretraining, \\gffn{s} are equally good at optimizing perplexity as Transformers. For finetuning, the two model families exhibit comparable scalability despite task-specific offsets.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/tiny-attention.png",
                "caption": "Hybrid spatial gating unit with a tiny self-attention module. We use the normalized input of the \\gffn block (endpoint after the input normalization and right before the channel expansion) as the input to the tiny self-attention. For SGU we have $d_\\mathrm{out} = d_\\mathrm{ffn} / 2$ due to the channel split.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/pplx-vs-sst.png",
                "caption": "Transferability from MLM pretraining perpexity to finetuning accuracies on GLUE. aMLP refers to gMLP enhanced with a 64-d single-head self-attention, as illustrated in Figure~\\ref{fig:tiny-attn}. In contrast, each self-attention module in the BERT baseline contains 12 heads with a total size of 768.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/pplx-vs-mnli.png",
                "caption": "Transferability from MLM pretraining perpexity to finetuning accuracies on GLUE. aMLP refers to gMLP enhanced with a 64-d single-head self-attention, as illustrated in Figure~\\ref{fig:tiny-attn}. In contrast, each self-attention module in the BERT baseline contains 12 heads with a total size of 768.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/scaling-with-tinyattn.png",
                "caption": "Comparing the scaling properties of Transformers, \\gffn{s} and aMLPs (with 64-d, single-head attention). Results were obtained using the same setup in Section~\\ref{sec:scaling}.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/tplz.png",
                "caption": "Spatial projection matrices learned on the MLM pretraining task without the shift invariance prior (that each individual $W$ being a Toeplitz matrix). The plots show that \\gffn learns Toeplitz-like matrices (hence the notion of shift invariance) regardless.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/amlp-heatmap-1.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/amlp-heatmap-2.png",
                "caption": "Attention maps in aMLP over selected examples in MNLI-m.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table1.tex",
                "caption": "Architecture specifications of \\gffn models for vision. The survival probability of stochastic depth is the only hyperparameter change as we move from smaller to larger models.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table2.tex",
                "caption": "ImageNet-1K results without extra data.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table3.tex",
                "caption": "MLM validation perplexities of Transformer baselines and four versions of \\gffn{s}. $f$ refers to the spatial linear projection in Equation~\\eqref{eq:spatial-proj} with input normalization. The MLP-Mixer baseline model has L=24 layers with $d_\\mathrm{model}$=768, $d_\\mathrm{spatial}$=384 and $d_\\mathrm{ffn}$=3072. Each \\gffn model has L=36 layers with $d_\\mathrm{model}$=512 and $d_\\mathrm{ffn}$ = 3072. No positional encodings are used for Mixer or \\gffn{s}.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table4.tex",
                "caption": "Pretraining and dev-set finetuning results over increased model capacity. We use the relative positional encoding scheme for Transformers which performs the best in Table~\\ref{tab:baselines}.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table5.tex",
                "caption": "Model specifications in the full BERT setup.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table6.tex",
                "caption": "Pretraining perplexities and dev-set results for finetuning. ``ours'' indicates models trained using our setup. We report accuracies for SST-2 and MNLI, and F1 scores for SQuAD v1.1/2.0.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table7.tex",
                "caption": "Hyperparameters for Image classification on ImageNet-1K",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table8.tex",
                "caption": "Hyperparameters for MLM pretraining on C4.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table9.tex",
                "caption": "Hyperparameters for MLM finetuning on GLUE and SQuAD.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.08050/table10.tex",
                "caption": "MLM results with increasingly deeper \\& thinner Transformers. As the depth increases, we adjust the model width accordingly to maintain comparable capacity. We observe that the perplexity is insensitive to the model depth at a fixed capacity, and worsens beyond 48 layers. Note these results were obtained using a similar yet different training setup from the rest of the paper.",
                "description": ""
            }
        ]
    },
    "2105.06548": {
        "title": "Not All Memories are Created Equal: Learning to Forget by Expiring",
        "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.",
        "tldr": "It is demonstrated that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality, and it is shown that it trains faster and uses less memory.",
        "full_text": [
            {
                "section_name": "Additional Experimental Results_1",
                "paragraphs": " "
            },
            {
                "section_name": "Efficiency for Instruction Task_1",
                "paragraphs": " We include a comparison of \\textsc{Expire-Span} to Adaptive-Span and Compressive Transformer in Table~\\ref{tab:efficiency_appendix} and show that \\textsc{Expire-Span} has stronger performance, is faster, and saves GPU memory.  \\begin{table*}[t] \\centering \\renewcommand{\\arraystretch}{0.8} \\begin{tabular}{lllccc} \\toprule && Model &  Performance & GPU Memory (GB) & Time/Batch (ms) \\\\ \\midrule \\multirow{3}{*}{Instruction Task} && Compressive Transformer & 71\\% Acc & 10 & 210 \\\\ && Adaptive-Span & 64\\% Acc & 14 & 240 \\\\ && \\textsc{Expire-Span} & {\\bf  74\\%} Acc & \\bf  8 & \\bf  90 \\\\ \\bottomrule \\end{tabular} \\caption{ \\textbf{Efficiency of \\textsc{Expire-Span}}. We report peak GPU memory usage and per-batch training time, fixing the batch size. We evaluate the mean pooling version of the Compressive Transformer. } \\label{tab:efficiency_appendix} \\vspace{-2mm} \\end{table*}  "
            },
            {
                "section_name": "Wikitext-103 Language Modeling_2",
                "paragraphs": " \\begin{table}[t] \\s\\t \\centering \\begin{tabular}{lccc} \\toprule Model & Params & Test\\\\ \\midrule DEQ-Trans.~\\citep{bai2019deep} & 110M & 23.3\\\\ Trans-XL~\\citep{dai2019transformer} & 257M  & 18.3  \\\\ Feedback Trans.~\\citep{fan2020accessing} & 77M & 18.3 \\\\ Trans.+LayerDrop~\\citep{Fan2020Reducing} & 423M  & 17.7 \\\\ Compressive Trans.~\\citep{rae2020compressive} & 277M  & 17.1 \\\\ Routing Trans.~\\citep{roy2020efficient}& - &  15.8 \\\\ \\midrule \\textsc{Expire-Span} & 140M &  19.6 \\\\ \\bottomrule \\end{tabular} \\caption{ \\textbf{Wikitext-103 Results.} We report perplexity on test. } \\label{tab:wiki103} \\end{table}  \\begin{figure}[t] \\centering \\includegraphics[width=0.8\\linewidth]{figs/plot_wiki103.pdf} \\caption{\\textbf{Performance as a function of Memory Size on Wikitext-103}} \\label{fig:wiki103_memsize} \\end{figure}  The Wikitext-103 word-level language modeling benchmark~\\citep{merity2016pointer} consists of a collection of Wikipedia articles and a fixed vocabulary size of 270K.  We set the max attention span for \\textsc{Expire-Span} to 8K. We compare \\textsc{Expire-Span} to existing work in Table~\\ref{tab:wiki103} and show that even fairly small models trained with \\textsc{Expire-Span} achieve competitive results. Next, we analyze the performance of \\textsc{Expire-Span} on Wikitext-103 as the memory size increases. We compare to a Transformer-XL model in Figure~\\ref{fig:wiki103_memsize} --- even with far smaller memory, \\textsc{Expire-Span} performs much better.  "
            },
            {
                "section_name": "Expire-span Analysis on Enwik8_3",
                "paragraphs": "In Figure~\\ref{fig:vis_enwiki8_additional_layerwise}, we analyze multiple layers of a trained model and show that different layers memorize different types of information. Several layers retain summarizing information about sentences or sections by increasing the expire-spans of spaces, new lines, and section titles.  "
            },
            {
                "section_name": "Importance of Structured Dropout for Regularization_4",
                "paragraphs": " We analyze the importance of structured dropout to regularize the large memory capacity provided by \\textsc{Expire-Span}. In an experiment on enwiki8, Figure~\\ref{fig:enwik8_ovefit} shows that loss on a portion of  validation data was incredibly large. This part corresponds to a 66K token long table. We hypothesize that the model likely never encountered such a table during training. During validation, this caused all non-table tokens to expire. Without regularizing the model memory size during training, the model can easily overfit.  \\begin{figure*}[t] \\centering {\\includegraphics[width=\\linewidth]{figs/enwiki-layerwise.pdf}} \\caption{\\textbf{Per-Layer \\textsc{Expire-Span} values on Enwik8}. We visualize the expire-spans of different layers: layer 6 gives long span to spaces, layer 9 memorizes special tokens like newlines and section titles, and layer 10 retains named entities in memory.} \\label{fig:vis_enwiki8_additional_layerwise} \\end{figure*}   \\begin{figure*} \\centering \\includegraphics[width=4in]{figs/enwiki8_loss.png} \\caption{\\textbf{Extreme Overfitting} on part of validation occurs without proper regularization.} \\label{fig:enwik8_ovefit} \\end{figure*}   "
            },
            {
                "section_name": "Colliding Objects, An Easy Version_5",
                "paragraphs": "We experiment with an easier version of the Colliding Objects task where objects do not have colors. The model has to predict either the last collision, or a mapping of the last 3 collisions. In contrast to the harder task, there are no color switches and any collision prediction is valid. As this version is less memory intensive, the \\textsc{Expire-span} model almost solves it with a shorter maximum span, as shown in \\tab{colliding_easy}.  \\begin{table} \\centering \\begin{tabular}{lcc} \\toprule Model & Maximum Span & Test Error (\\%)  \\\\ \\midrule Transformer-XL & 1k & 39.1 \\\\ \\midrule & 1k & 19.5 \\\\ \\textsc{Expire-Span} & 2k & 9.1 \\\\ & 4k & 3.2 \\\\ \\bottomrule \\end{tabular} \\caption{ \\textbf{Colliding Objects Results}. We report test error. } \\label{tab:colliding_easy} \\end{table}   "
            },
            {
                "section_name": "Additional Implementation Details_2",
                "paragraphs": " "
            },
            {
                "section_name": "Reinforcement Learning Tasks_6",
                "paragraphs": "We used MazeBase~\\citep{Sukhbaatar2015MazeBaseAS} to construct tasks in grid world. Agents can observe its surrounding $3\\times3$ area and  move in the four cardinal directions. Every objects and their properties are described by words such as ``agent'',``block'', ``blue'', etc. Thus, the input to the model is a binary tensor of size $3\\times3\\times \\text{vocabulary-size}$.  We train 2-layer Transformers with 64 hidden units using actor-Critic algorithm. We used a BPTT length of 100, and an entropy cost of 0.0005.  \\paragraph{Corridor Task} The corridor length is sampled from $\\mathcal{U}(3, 200)$. All models are trained for 100M steps. We used RMSProp optimizer with a learning rate of 0.0001 and a batch size of 64. For the expire-span models, we set the maximum span $L$ to 200, the loss coefficient $\\alpha$ to 5e-6, and the ramp length $R$ to 16.  \\paragraph{Multi-Room Portal} In this task, there are 50 rooms sequentially connected together. Each room is $5\\times5$ in size, and have two doors with different colors. If agent go to the correct door, it will be teleported to the next room, but if it is the wrong door, the agent will be teleported back to the first room and have to start over. Which of the two doors is correct in each room is randomly decided and fixed throughout the episode. This information is not visible to the agent, thus can only be discovered by trial and error within each episode. The current room number is visible to the agent.  When the agent successfully transitions from the $k$-th room to the next, it receives a reward of $0.1k$. The episode ends if the agent makes two mistakes in the same room, reaches the last room, or when the number of steps reach 1000. A reward discount of 0.98 is used. All models are trained with Adam optimizer with a learning rate of 5e-4, and a batch size of 1024, with gradients are clipped at 0.1. We set $L=100$, $R=16$ and $\\alpha=$1e-6 for the expire-span models.  "
            },
            {
                "section_name": "Instruction Task in LIGHT_7",
                "paragraphs": "We train 6-layer models with a hidden size of 512 and 8 attention heads. To train, we use the Adam optimizer with a learning rate of 7e-4 and 8000 warmup updates. We set the expire-span ramp $R$ to 64 and the expire-span loss $\\alpha$ to 2e-6.  "
            },
            {
                "section_name": "Collision Task_8",
                "paragraphs": " At the start of the simulation, each particle samples a Gaussian Normal velocity and position uniform inside a $16\\times 16$ grid.  At each time step the particles' position is updated by adding its velocity (unless it would go off the grid, in which case its velocity is re-sampled). There are 5 different colors, and a particle can change its color randomly at each step with 0.05 probability. A collision happens when the two particles have the same rasterized locations, but it does not affect the movement.  Given a question specifying two colors, the task is to report in which of the four quadrants of the grid the last collision of the specified-colors occurred. To make the task easier to learn, 40\\% of the queries will have the matching colors as the last collision.  The model is given an input sequence of tokens that has 8 entries per timestep.  The first 4 are the rounded and rasterized $(x,y)$ locations of the two particles, and next 2 are tokens representing the colors of the particles. The last 2 entries are ``question'' tokens that specify the colors of the collision.  The output sequence has a token for each quadrant. We generate 50M steps for training, which equals to 400M entries.  \\paragraph{Easy Version:} The particles have no color in this version. There are two types of questions, in which the task is to report either: \\textbf{(i)} in which of the four quadrants of the grid the last collision occurred, or \\textbf{(ii)} the label mapping of the last 3 collisions.   "
            },
            {
                "section_name": "Language Modeling Details_9",
                "paragraphs": " \\paragraph{Enwik8}  Our small model has 12 layers with a hidden size of 512 and 8 attention heads.  To train, we use Adam optimizer with a learning rate of 7e-4, a batch size of 512, a block size of 512 and 8000 warmup updates. All models are trained for 100k updates. The model in Table~2 is further fine-tuned for another 10k updates with a 10x smaller LR. The baseline models used for comparison are the same size model following the same training protocol.  The large \\textsc{Expire-Span} model Table~2 is a 24-layer model with a hidden size of 768 and 4096 feedforward units. It is trained with a learning rate of 4e-4 and 16k warmup steps. In addition to 0.5 dropout, we also add 0.2 layer-drop. The \\textsc{Expire-Span} parameters are $L=32k$, $\\alpha=$3e-7, and $R=128$. We used the version of Eq.~6 due to the very long maximum span.  \\paragraph{Character-level PG-19}  Besides the maximum span, all model parameters and training parameters were held constant. Each model had 12 layers, a hidden size of 512, a feedforward size of 2048, 8 attention heads, and processed a block of 512 characters at a time. We initialized the weights using a uniform distribution as described by~\\cite{glorot2010understanding}, used dropout of 0.2, clipped the gradients at 0.3, warmed up the learning rate linearly for 8000 steps, and used cosine annealing to decay the learning rate after warmup~\\citep{loshchilov2016sgdr}. For the \\textsc{Expire-Span} models, we used a ramp of $R=128$ and an expiration loss coefficient of $\\alpha=$1e-6 (3e-7) for $L=8k$ ($16k$).  \\paragraph{Wikitext-103} All models have 8 layers and 1024 hidden units (4096 in feedforward layers). In addition to the dropout of 0.3 applied to attention and ReLU activation, outputs from the embedding layer and the last layer had a dropout of 0.2. We used the adaptive input~\\cite{baevski2018adaptive} and the adaptive softmax~\\cite{grave2017efficient} for reducing the number of parameters within word embeddings. The models are trained for 300k updates with a block size of 256, and gradients are clipped at 0.1. The other hyperparameters are the same as the small Enwik8 experiments.   \\bibliography{iclr2021_conference} \\bibliographystyle{icml2021}   \\end{document}     "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2105.06548/plot_wiki103.png",
                "caption": "\\textbf{Performance as a function of Memory Size on Wikitext-103}",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.06548/table1.tex",
                "caption": " \\textbf{Wikitext-103 Results.} We report perplexity on test. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2105.06548/table2.tex",
                "caption": " \\textbf{Colliding Objects Results}. We report test error. ",
                "description": ""
            }
        ]
    },
    "2001.08361": {
        "title": "Scaling Laws for Neural Language Models",
        "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
        "tldr": "Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "  Language provides a natural domain for the study of artificial intelligence, as the vast majority of reasoning tasks can be efficiently expressed and evaluated in language, and the world's text provides a wealth of data for unsupervised learning via generative modeling.  Deep learning has recently seen rapid progress in language modeling, with state of the art models \\cite{radford2018improving,1810.04805,1906.08237,DBLP:journals/corr/abs-1907-11692, 1910.10683} approaching human-level performance on many specific tasks \\cite{wang2019superglue}, including the composition of coherent multi-paragraph prompted text samples \\cite{radford2019language}.  One might expect language modeling performance to depend on model architecture, the size of neural models, the computing power used to train them, and the data available for this training process.  In this work we will empirically investigate the dependence of language modeling loss on all of these factors, focusing on the Transformer architecture \\cite{OriginalTransformer,liu2018generating}.  The high ceiling and low floor for performance on language tasks allows us to study trends over more than seven orders of magnitude in scale.  Throughout we will observe precise power-law scalings for performance as a function of training time, context length, dataset size, model size, and compute budget.  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=\\textwidth]{SimplePowerLaws}  \\caption[Summary of simple power laws.]{Language modeling performance improves smoothly as we increase the model size, datasetset size, and amount of compute\\footnotemark~used for training.  For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two.  \\label{fig:BasicPowerLaws}} \\end{figure}   "
            },
            {
                "section_name": "Summary_1",
                "paragraphs": " \\footnotetext{Here we display predicted compute when using a sufficiently small batch size.  See Figure \\ref{fig:ComputeEfficientAdjusted} for comparison to the purely empirical data.}  Our key findings for Transformer language models are are as follows:  \\paragraph{Performance depends strongly on scale, weakly on model shape:} Model performance depends most strongly on scale, which consists of three factors: the number of model parameters $N$ (excluding embeddings), the size of the dataset $D$, and the amount of compute $C$ used for training.  Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs.~width. (Section \\ref{sec:Empirical})  \\paragraph{Smooth power laws:} Performance has a power-law relationship with each of the three scale factors $N, D, C$ when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure \\ref{fig:BasicPowerLaws}). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss. (Section \\ref{sec:Empirical})  \\paragraph{Universality of overfitting:} Performance improves predictably as long as we scale up $N$ and $D$ in tandem, but enters a regime of diminishing returns if either $N$ or $D$ is held fixed while the other increases.  The performance penalty depends predictably on the ratio $N^{0.74}/D$, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section \\ref{sec:ChartingOverfitting})  \\paragraph{Universality of training:} Training curves follow predictable power-laws whose parameters are roughly independent of the model size.  By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. (Section \\ref{sec:ScalingSizeandSteps})  \\paragraph{Transfer improves with test performance:} When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss -- in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set. (Section \\ref{sec:GeneralizationtoOtherDistributions})  \\paragraph{Sample efficiency:} Large models are  more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure \\ref{fig:EfficiencyIllustration}) and using  fewer data points (Figure \\ref{fig:LossvsModelDatasetSize}).   \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.99\\textwidth]{EfficiencyIllustration} \\caption[Illustration of sample efficiency and compute efficiency.]{We show a series of language model training runs, with models ranging in size from $10^3$ to $10^9$ parameters (excluding embeddings). \\label{fig:EfficiencyIllustration}} \\end{figure}  \\begin{figure} \\noindent \\centering{} \\includegraphics[height=0.32\\textwidth]{ContributionIllustration} \\caption[How to scale up model size, batch size, and serial steps]{As more compute becomes available, we can choose how much to allocate towards training larger models, using larger batches, and training for more steps.  We illustrate this for a billion-fold increase in compute.  For optimally compute-efficient training, most of the increase should go towards increased model size.  A relatively small increase in data is needed to avoid reuse.  Of the increase in data, most can be used to increase parallelism through larger batch sizes, with only a very small increase in serial training time required.  \\label{fig:ContributionIllustration} } \\end{figure}   \\paragraph{Convergence is inefficient:} When working within a fixed compute budget $C$ but without any other restrictions on the model size $N$ or available data $D$, we attain optimal performance by training \\emph{very large models} and stopping \\emph{significantly short of convergence} (see Figure \\ref{fig:ContributionIllustration}).  Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as $D \\sim C^{0.27}$ with training compute. (Section \\ref{sec:OptimalCompute})  \\paragraph{Optimal batch size:} The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale \\cite{1812.06162}; it is roughly 1-2 million tokens at convergence for the largest models we can train. (Section \\ref{sec:OptimalBatchSize})   Taken together, these results show that language modeling performance improves smoothly and predictably as we appropriately scale up model size, data, and compute.  We expect that larger language models will perform better and be more sample efficient than current models.   "
            },
            {
                "section_name": "Summary of Scaling Laws_2",
                "paragraphs": " The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law  when performance is limited by only either the number of non-embedding parameters $N$, the dataset size $D$, or the optimally allocated compute budget $C_{\\rm min}$ (see Figure \\ref{fig:BasicPowerLaws}): \\begin{enumerate} \\setlength\\itemsep{0.5em} \\item For models with a limited number of parameters, trained to convergence on sufficiently large datasets: \\be L(N) = \\left(N_{\\mathrm{c}}/N\\right)^{\\alpha_N};~~ \\alpha_N \\sim 0.076, \\quad N_{\\mathrm{c}} \\sim 8.8 \\times 10^{13}~\\text{(non-embedding parameters)} \\label{eq:crit_n} \\ee \\item For large models trained with a limited dataset with early stopping: \\be L(D) = \\left(D_{\\mathrm{c}}/D\\right)^{\\alpha_D};~~ \\alpha_D \\sim 0.095, \\quad D_{\\mathrm{c}} \\sim 5.4 \\times 10^{13}~\\text{(tokens)} \\label{eq:crit_d} \\ee \\item When training with a limited amount of compute, a sufficiently large dataset, an optimally-sized model, and a sufficiently small batch size (making optimal\\footnote{We also observe an empirical power-law trend with the training compute $C$ (Figure \\ref{fig:BasicPowerLaws}) while training at fixed batch size, but it is the trend with $C_{\\rm min}$ that should be used to make predictions.  They are related by equation \\eqref{eq:AdjustedCompute}. } use of compute): \\be L(C_{\\rm min}) = \\left(C_{\\mathrm{c}}^{\\rm min} / C_{\\rm min}\\right)^{\\alpha_C^{\\rm min}};~~ \\alpha_C^{\\rm min} \\sim 0.050, \\quad C_{\\mathrm{c}}^{\\rm min} \\sim 3.1 \\times 10^{8}~\\text{(PF-days)} \\label{eq:crit_c} \\ee \\end{enumerate}  These relations hold across eight orders of magnitude in $C_{\\rm min}$, six orders of magnitude in $N$, and over two orders of magnitude in $D$.  They depend very weakly on model shape and other Transformer hyperparameters (depth, width, number of self-attention heads), with specific numerical values associated with the Webtext2 training set \\cite{radford2019language}.   The power laws $\\alpha_{\\rm N}, \\alpha_{\\rm D}, \\alpha_{C}^{\\rm min}$ specify the degree of performance improvement expected as we scale up $N$, $D$, or $C_{\\rm min}$; for example, doubling the number of parameters yields a loss that is smaller by a factor $2^{-\\alpha_N}=0.95$. The precise numerical values of $N_{\\mathrm{c}}, C_{\\rm c}^{\\rm min},$ and $D_{\\mathrm{c}}$ depend on the vocabulary size and tokenization and hence do not have a fundamental meaning.   The critical batch size, which determines the speed/efficiency tradeoff for data parallelism (\\cite{1812.06162}), also roughly obeys a power law in $L$: \\begin{equation} B_{\\rm crit}\\left(L\\right)=\\frac{B_{\\ast}}{L^{1/\\alpha_{B}}},\\qquad B_{\\ast} \\sim 2\\cdot 10^8 \\text{ tokens},\\ \\ \\alpha_{B} \\sim 0.21 \\label{eq:critical-batch-size} \\end{equation}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{LossvsModelDatasetSize} \\includegraphics[width=0.48\\textwidth]{LearningCurveFitComparisonIntro} \\caption[Performance when varying model and data size, or model and training steps, simultaneously]{ {\\bf Left}: The early-stopped test loss $L(N, D)$ varies predictably with the dataset size $D$ and model size $N$ according to Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}. {\\bf Right}:  After an initial transient period, learning curves for all model sizes $N$ can be fit with Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps}, which is parameterized in terms of $S_{\\rm min}$, the number of steps when training at large batch size (details in Section \\ref{sec:OptimalBatchSize}).  \\label{fig:LearningCurveFitsandResiduals} \\label{fig:LossvsModelDatasetSize}} \\end{figure}   Equation \\eqref{eq:crit_n} and \\eqref{eq:crit_d} together suggest that as we increase the model size, we should increase the dataset size sublinearly according to $D \\propto N^{\\frac{\\alpha_N}{\\alpha_D}} \\sim N^{0.74}$. In fact, we find that there is a single equation combining \\eqref{eq:crit_n} and \\eqref{eq:crit_d} that governs the simultaneous dependence on $N$ and $D$ and governs the degree of overfitting: \\be \\label{eq:FundamentalLikelihioodvsModelandDataSize} L(N, D) = \\left[ \\left( \\frac{N_c}{N} \\right)^{\\frac{\\alpha_N}{\\alpha_D}} + \\frac{D_c}{D}  \\right]^{\\alpha_D} \\ee with fits pictured on the left in figure \\ref{fig:LossvsModelDatasetSize}.  We conjecture that this functional form may also parameterize the trained log-likelihood for other generative modeling tasks.  When training a given model for a finite number of parameter update steps $S$ in the infinite data limit, after an initial transient period, the learning curves can be accurately fit by  (see the right of figure \\ref{fig:LearningCurveFitsandResiduals}) \\be \\label{eq:FundamentalLikelihioodvsModelandSteps} L(N, S) = \\left( \\frac{N_c}{N} \\right)^{\\alpha_N}  + \\left( \\frac{S_c }{S_{\\rm min}(S)} \\right)^{\\alpha_S} \\ee where $S_c \\approx 2.1 \\times 10^3$ and $\\alpha_S \\approx 0.76$, and $S_{\\rm min}(S)$ is the minimum possible number of optimization steps (parameter updates) estimated  using Equation \\eqref{eq:AdjustedSteps}.  When training within a fixed compute budget $C$, but with no other constraints, Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps} leads to the prediction that the optimal model size $N$, optimal batch size $B$, optimal number of steps $S$, and dataset size $D$ should grow as \\be \\label{eq:OptimalModelSizeTrainingTimevsCompute} N \\propto C^{\\alpha_{C}^{\\rm min} /\\alpha_{N}}, \\quad B \\propto C^{\\alpha_C^{\\rm min} / \\alpha_B}, \\quad S \\propto C^{\\alpha_C^{\\rm min} / \\alpha_S}, \\quad D = B\\cdot S \\quad \\ee with \\be \\alpha_{C}^{\\rm min} = 1/\\left(1/\\alpha_{S}+1/\\alpha_{B}+1/\\alpha_{N}\\right) \\ee which closely matches the empirically optimal results $N \\propto C_{\\rm min}^{0.73}$, $B \\propto C_{\\rm min}^{0.24}$, and $S \\propto C_{\\rm min}^{0.03}$.  As the computational budget $C$ increases, it should be spent primarily on larger models, without dramatic increases in training time or dataset size (see Figure \\ref{fig:ContributionIllustration}).  This also implies that as models grow larger, they  become increasingly sample efficient.  In practice, researchers typically train  smaller models for  longer than would be maximally compute-efficient because of  hardware constraints. Optimal performance depends on total compute as a power law (see Equation \\eqref{eq:crit_c}).  We  provide some basic theoretical motivation for Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}, an analysis of learning curve fits and their implications for training time, and a breakdown of our results per token.  We also make some brief comparisons to LSTMs and recurrent Transformers \\cite{DBLP:journals/corr/abs-1807-03819}.   "
            },
            {
                "section_name": "Notation_3",
                "paragraphs": " We use the following notation: \\begin{itemize} \\item $L$ -- the cross entropy loss in nats.  Typically it will be averaged over the tokens in a context, but in some cases we report the loss for specific tokens within the context. \\item $N$ -- the number of model parameters, \\emph{excluding all vocabulary and positional embeddings} \\item $C \\approx 6 N B S$ -- an estimate of the total non-embedding training compute, where $B$ is the batch size, and $S$ is the number of training steps (ie parameter updates). We quote numerical values in PF-days, where one PF-day $ = 10^{15} \\times 24 \\times 3600 = 8.64 \\times 10^{19}$ floating point operations. \\item $D$ -- the dataset size in tokens \\item $B_{\\rm crit}$ -- the critical batch size \\cite{1812.06162}, defined and discussed in Section \\ref{sec:OptimalBatchSize}.  Training at the critical batch size provides a roughly optimal compromise between time and compute efficiency. \\item $C_{\\rm min}$ -- an estimate of the minimum amount of non-embedding compute to reach a given value of the loss.  This is the training compute that would be used if the model were trained at a batch size much less than the critical batch size. \\item $S_{\\rm min}$ -- an estimate of the minimal number of training steps needed to reach a given value of the loss.  This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size. \\item $\\alpha_X$ -- power-law exponents for the scaling of the loss as $L(X) \\propto 1/X^{\\alpha_X}$ where $X$ can be any of $N, D, C, S, B, C^{\\rm min}$. \\end{itemize}     "
            },
            {
                "section_name": "Background and Methods_2",
                "paragraphs": "   We train language models on WebText2, an extended version of the WebText \\cite{radford2019language} dataset, tokenized using byte-pair encoding \\cite{BPE} with a vocabulary size $n_{\\rm vocab} = 50257$.  We optimize the autoregressive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal performance metric.  We record the loss on the WebText2 test distribution and on a selection of other text distributions.  We primarily train decoder-only \\cite{liu2018generating, radford2018improving} Transformer \\cite{OriginalTransformer} models, though we also train LSTM models and Universal Transformers \\cite{DBLP:journals/corr/abs-1807-03819}  for comparison.  "
            },
            {
                "section_name": "Parameter and Compute Scaling of Transformers_4",
                "paragraphs": "\\label{sec:ParameterComputeCounts}   \\begin{table}[t!] \\centering \\begin{tabular}{|l|l|l|} \\hline \\textbf{Operation}  & \\textbf{Parameters}  & \\textbf{FLOPs per Token}\\tabularnewline \\hline \\hline Embed  & $\\left(n_{{\\rm vocab}} + n_{{\\rm ctx}}\\right)d_{{\\rm model}}$  & $4d_{{\\rm model}}$\\tabularnewline \\hline Attention: QKV  & $n_{{\\rm layer}}d_{{\\rm model}}3d_{{\\rm attn}}$  & $2n_{{\\rm layer}}d_{{\\rm model}}3d_{{\\rm attn}}$\\tabularnewline \\hline Attention: Mask & ---  & $2n_{{\\rm layer}}n_{{\\rm ctx}}d_{{\\rm attn}}$\\tabularnewline \\hline Attention: Project & $n_{{\\rm layer}}d_{{\\rm attn}}d_{{\\rm model}}$  & $2n_{{\\rm layer}}d_{{\\rm attn}}d_{{\\rm embd}}$\\tabularnewline \\hline Feedforward  & $n_{{\\rm layer}}2d_{{\\rm model}}d_{{\\rm ff}}$ & $2n_{{\\rm layer}}2d_{{\\rm model}}d_{{\\rm ff}}$\\tabularnewline \\hline De-embed  & ---  & $2d_{{\\rm model}}n_{{\\rm vocab}}$\\tabularnewline \\hline \\hline \\textbf{Total (Non-Embedding)} & $N=2d_{{\\rm model}}n_{{\\rm layer}}\\left(2d_{{\\rm attn}}+d_{{\\rm ff}}\\right)$  & $C_{\\mathrm{forward}}=2N+2n_{{\\rm layer}}n_{{\\rm ctx}}d_{{\\rm attn}}$\\tabularnewline \\hline \\end{tabular} \\vspace{1em} \\caption[Parameter and compute counts for Transformer]{Parameter counts and compute (forward pass) estimates for a Transformer model.  Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted. \\label{tab:TableTransformerParamsFLOPs}} \\end{table}  We parameterize the Transformer architecture using hyperparameters $n_{\\rm layer}$ (number of layers), $d_{{\\rm model}}$ (dimension of the residual stream), $d_{\\rm ff}$ (dimension of the intermediate feed-forward layer), $d_{\\rm attn}$ (dimension of the attention output), and $n_{\\rm heads}$ (number of attention heads per layer).  We include $n_{\\rm ctx}$ tokens in the input context, with $n_{\\rm ctx} = 1024$ except where otherwise noted.  We use $N$ to denote the model size, which we define as the number of \\emph{non-embedding} parameters \\begin{align} \\label{eq:ModelSizeDefinition} N & \\approx  2d_{{\\rm model}}n_{{\\rm layer}}\\left(2d_{{\\rm attn}}+d_{{\\rm ff}}\\right) & \\nonumber \\\\ & = 12 n_{\\rm layer} d_{{\\rm model}}^2 \\quad \\text{ with the standard } \\quad d_{\\rm attn} = d_{\\rm ff}/4 = d_{{\\rm model}} & \\end{align} where we have excluded biases and other sub-leading terms. Our models also have $n_{\\rm vocab} d_{{\\rm model}}$ parameters in an embedding matrix, and use $n_{\\rm ctx} d_{{\\rm model}}$ parameters for positional embeddings, but we do not include these when discussing the `model size' $N$; we will see that this produces significantly cleaner scaling laws.  Evaluating a forward pass of the Transformer involves roughly \\be \\label{eq:ApproximateTotalCompute} C_{\\rm forward} \\approx 2N + 2n_{{\\rm layer}}n_{{\\rm ctx}}d_{{\\rm model}} \\ee add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in matrix multiplication.  A more detailed per-operation parameter and compute count is included in Table \\ref{tab:TableTransformerParamsFLOPs}.  For contexts and models with $d_{{\\rm model}} > n_{\\rm ctx} / 12$, the context-dependent computational cost per token is a relatively small fraction of the total compute. Since we primarily study models where $d_{\\rm model} \\gg n_{\\rm ctx}/12$, we do not include context-dependent terms in our training compute estimate.  Accounting for the backwards pass (approximately twice the compute as the forwards pass), we then define the estimated non-embedding compute as $C \\approx 6 N$ floating point operators per training token.   "
            },
            {
                "section_name": "Training Procedures_5",
                "paragraphs": " Unless otherwise noted, we train  models with the Adam optimizer \\cite{kingma2014adam} for a fixed $2.5 \\times 10^5$  steps with a batch size of $512$ sequences of $1024$ tokens. Due to memory constraints, our largest models (more than 1B parameters) were trained with Adafactor \\cite{DBLP:journals/corr/abs-1804-04235}. We experimented with a variety of learning rates and schedules, as discussed in Appendix \\ref{app:OptimizationDetailsandErrorAnalysis}. We found that results at convergence were largely independent of learning rate schedule.  Unless otherwise noted, all training runs included in our data used a learning rate schedule with a 3000 step linear warmup followed by a cosine decay to zero.  "
            },
            {
                "section_name": "Datasets_6",
                "paragraphs": "We train our models on an extended version of the WebText dataset described in \\cite{radford2019language}.  The original WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and $1.62 \\times 10^{10}$ words (as defined by \\texttt{wc}). We then apply the reversible tokenizer described in \\cite{radford2019language}, which yields $2.29 \\times 10^{10}$ tokens.  We reserve $6.6 \\times 10^{8}$ of these tokens for use as a test set, and we also test on similarly-prepared samples of Books Corpus \\cite{Zhu_2015}, Common Crawl \\cite{commoncrawl}, English Wikipedia, and a collection of publicly-available Internet Books.    "
            },
            {
                "section_name": "Empirical Results and Basic Power Laws_3",
                "paragraphs": "\\label{sec:Empirical}  To characterize language model scaling we train a wide variety of models, varying a number of factors including: \\begingroup \\renewcommand{\\arraystretch}{1.1} \\begin{itemize} \\item Model size (ranging in size from 768 to 1.5 billion non-embedding parameters) \\item Dataset size (ranging from 22 million to 23 billion tokens) \\item Shape (including depth, width, attention heads, and feed-forward dimension) \\item Context length (1024 for most runs, though we also experiment with shorter contexts) \\item Batch size ($2^{19}$ for most runs, but we also vary it to measure the critical batch size) \\end{itemize} \\endgroup  In this section we will display data along with empirically-motivated fits, deferring theoretical analysis to later sections.  "
            },
            {
                "section_name": "Approximate Transformer Shape and Hyperparameter Independence_7",
                "paragraphs": "\\label{sec:ShapeIndependence}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=\\textwidth]{HyperparameterTuning} \\caption[Weak dependence of performance on hyperparameter tuning]{Performance depends very mildly on model shape when the total number of non-embedding parameters $N$ is held fixed.  The loss varies only a few percent over a wide range of shapes.  Small differences in parameter counts are compensated for by using the fit to $L(N)$ as a baseline.  Aspect ratio in particular can vary by a factor of 40 while only slightly impacting performance; an $(n_{\\mathrm{layer}}, d_{\\mathrm{model}}) = (6, 4288)$ reaches a loss within 3\\% of the $(48, 1600)$ model used in \\cite{radford2019language}.  \\label{fig:HeadsLayersIndependence}} \\end{figure}  Transformer performance depends very weakly on the shape parameters $n_{\\rm layer}, n_{\\rm heads}$, and $d_{\\rm ff}$ when we hold the total non-embedding parameter count $N$ fixed. To establish these results we  trained models with fixed size while varying a single hyperparameter. This was simplest for the case of $n_{\\rm heads}$.  When varying $n_{\\rm layer}$, we simultaneously varied $d_{{\\rm model}}$ while keeping $N \\approx 12 n_{\\rm layer} d_{{\\rm model}}^2$ fixed.  Similarly, to vary $d_{\\rm ff}$ at fixed model size we also simultaneously varied the $d_{{\\rm model}}$ parameter, as required by the parameter counts in Table \\ref{tab:TableTransformerParamsFLOPs}.  Independence of $n_{\\rm layers}$ would follow if deeper Transformers effectively behave as ensembles of shallower models, as has been suggested for ResNets \\cite{ResNetsEnsemblesShallow}.  The results are shown in Figure \\ref{fig:HeadsLayersIndependence}.   "
            },
            {
                "section_name": "Performance with Non-Embedding Parameter Count $N$_8",
                "paragraphs": "\\label{sec:PerformancevsModelSize}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.45\\textwidth]{PerfVsModelSizeAllParams} \\includegraphics[width=0.45\\textwidth]{PerfVsModelSizeNonEmbed} \\caption[Comparison of performance trend when including or excluding embeddings]{ {\\bf Left:} When we include embedding parameters, performance appears to depend strongly on the number of layers in addition to the number of parameters. {\\bf Right:} When we exclude embedding parameters, the performance of models with different depths converge to a single trend. Only models with fewer than 2 layers or with extreme depth-to-width ratios deviate significantly from the trend.  \\label{fig:PerformancevsModelSizeBody}} \\end{figure}   In Figure \\ref{fig:PerformancevsModelSizeBody} we display the performance of a wide variety of models, ranging from small models with shape $(n_{\\rm layer}, d_{{\\rm model}}) = (2, 128)$ through billion-parameter models, ranging in shape from $(6, 4288)$ through $(207, 768)$.  Here we have trained to near convergence on the full WebText2 dataset and observe no overfitting (except possibly for the very largest models).  As shown in  Figure \\ref{fig:BasicPowerLaws}, we find a steady trend with non-embedding parameter count $N$, which can be fit to the first term of Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}, so that \\be L(N) \\approx \\left( \\frac{N_c}{N} \\right)^{ \\alpha_N } \\ee To observe these trends it is crucial to study performance as a function of $N$; if we instead use the total parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure \\ref{fig:PerformancevsModelSizeBody}).  This suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in recent work \\cite{lan2019albert}.  Although these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets is also a power-law in $N$ with nearly identical power, as shown in Figure \\ref{fig:GeneralizationVsModelSize}.    \\subsubsection{Comparing to LSTMs and Universal Transformers}  \\begin{figure} \\begin{centering} \\includegraphics[width=\\columnwidth]{LSTMvsTransformerSummary} \\caption[LSTM and Transformer performance comparison]{ \\label{fig:LSTMvsTransformers}} \\end{centering} \\end{figure}   In Figure \\ref{fig:LSTMvsTransformers} we compare LSTM and Transformer performance as a function of non-embedding parameter count $N$. The LSTMs were trained with the same dataset and context length.  We see from these figures that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens.  We present power-law relationships between performance and context position Appendix \\ref{sec:ContextDependence}, where increasingly large powers for larger models suggest improved ability to quickly recognize patterns.  We also compare the performance of standard Transformers to recurrent Transformers \\cite{DBLP:journals/corr/abs-1807-03819} in Figure \\ref{fig:RecurrentTransformers} in the appendix.  These models re-use parameters, and so perform slightly better as a function of $N$, at the cost of additional compute per-parameter.  \\subsubsection{Generalization Among Data Distributions} \\label{sec:GeneralizationtoOtherDistributions}   We have also tested our models on a set of additional text data distributions.  The test loss on these datasets as a function of model size is shown in Figure \\ref{fig:GeneralizationVsModelSize}; in all cases the models were trained only on the WebText2 dataset.  We see that the loss on these other data distributions improves smoothly with model size, in direct parallel with the improvement on WebText2.  We find that generalization depends almost exclusively on the in-distribution validation loss, and does not depend on the duration of training or proximity to convergence. We also observe no dependence on model depth (see Appendix \\ref{sec:DepthVsGeneralization}).  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{GeneralizationVsModelSize}\\hfill \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{TrainingVsConvergence} \\caption[Generalization to other test datasets]{ \\textbf{Left:} Generalization performance to other data distributions improves smoothly with model size, with only a small and very slowly growing offset from the WebText2 training distribution. \\textbf{Right:} Generalization performance depends only on training distribution performance, and not on the phase of training.  We compare generalization of converged models (points) to that of a single large model (dashed curves) as it trains. \\label{fig:GeneralizationVsModelSize}} \\end{figure}    "
            },
            {
                "section_name": "Performance with Dataset Size and Compute_9",
                "paragraphs": " We display empirical trends for the test loss as a function of dataset size $D$ (in tokens) and training compute $C$ in Figure \\ref{fig:BasicPowerLaws}.  For the trend with $D$ we trained a model with $(n_{\\rm layer}, n_{\\rm embd}) = (36, 1280)$ on fixed subsets of the WebText2 dataset.  We stopped training once the test loss ceased to decrease.  We see that the resulting test losses can be fit with simple power-law \\be L(D) \\approx \\left( \\frac{D_c}{D} \\right)^{\\alpha_D} \\ee in the dataset size.  The data and fit appear in Figure \\ref{fig:BasicPowerLaws}.  The total amount of non-embedding compute used during training can be estimated as $C = 6 N B S$, where $B$ is the batch size, $S$ is the number of parameter updates, and the factor of $6$ accounts for the forward and backward passes.  Thus for a given value of $C$ we can scan over all models with various $N$ to find the model with the best performance on step $S = \\frac{C}{6 B S}$.  Note that in these results \\emph{the batch size $B$ remains fixed for all models}, which means that these empirical results are not truly optimal.  We will account for this in later sections using an adjusted $C_{\\rm min}$ to produce cleaner trends.  The result appears as the heavy black line on the left-hand plot in Figure \\ref{fig:BasicPowerLaws}.  It can  be fit with \\be L(C) \\approx \\left( \\frac{C_c}{C} \\right)^{\\alpha_C} \\ee The figure also includes images of individual learning curves to clarify when individual models are optimal.  We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample efficiency improves with model size, and we also illustrate this directly in Figure \\ref{fig:SampleEfficiency} in the appendix.   "
            },
            {
                "section_name": "Charting the Infinite Data Limit and Overfitting_4",
                "paragraphs": "\\label{sec:ChartingOverfitting}  In Section \\ref{sec:Empirical} we found a number of basic  scaling laws for language modeling performance.  Here we will study the performance of a model of size $N$ trained on a dataset with $D$ tokens while varying $N$ and $D$ simultaneously.  We will empirically demonstrate that the optimally trained test loss accords with the scaling law of Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}.  This  provides guidance on how much data we would need to train models of increasing size while keeping overfitting under control.  "
            },
            {
                "section_name": "Proposed $L(N,D)$ Equation_10",
                "paragraphs": " \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{DatasetModelSizevsPerformance}\\hfill \\includegraphics[width=0.48\\textwidth]{DatasetModelSizevsChangePerformance} \\caption[Universality of overfitting]{ The early-stopped test loss $L(N, D)$ depends predictably on the dataset size $D$ and model size $N$ according to Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}. {\\bf Left}: For large $D$, performance is a straight power law in $N$. For a smaller fixed $D$, performance stops improving as $N$ increases and the model begins to overfit. (The reverse is also true, see Figure \\ref{fig:LossvsModelDatasetSize}.) {\\bf Right}:  The extent of overfitting depends predominantly on the ratio $N^{\\frac{\\alpha_N}{\\alpha_D}}/D$, as predicted in equation (\\ref{eq:OverfittingPrediction}).  The line is our fit to that equation. \\label{fig:DatasetModelSizevsPerformance}} \\end{figure}  We have chosen the parameterization \\eqref{eq:FundamentalLikelihioodvsModelandDataSize} (repeated here for convenience): \\be L(N, D) = \\left[ \\left( \\frac{N_c}{N} \\right)^{\\frac{\\alpha_N}{\\alpha_D}} + \\frac{D_c}{D}  \\right]^{\\alpha_D} \\ee using three principles: \\begin{enumerate} \\item Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor.  The parameterization of $L(N,D)$ (and all models of the loss) must naturally allow for such a rescaling. \\item Fixing $D$ and sending $N \\to \\infty$,  the overall loss should approach $L(D)$.  Conversely, fixing $N$ and sending $D \\to \\infty$ the loss must approach $L(N)$. \\item $L(N,D)$ should be analytic at $D=\\infty$, so that it has a series expansion in $1/D$ with integer powers.  Theoretical support for this principle is significantly weaker than for the first two. \\end{enumerate}  Our choice of $L(N,D)$ satisfies the first requirement because we can rescale $N_c, D_c$ with changes in the vocabulary.  This also implies that the values of $N_c, D_c$ have no fundamental meaning.  Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we expect that larger models should always perform better than smaller models.  But with fixed finite $D$, we also do not expect any model to be capable of approaching the best possible loss (ie the entropy of text).  Similarly, a model with fixed size will be capacity-limited.  These considerations motivate our second principle. Note that knowledge of $L(N)$ at infinite $D$ and $L(D)$ at infinite $N$ fully determines all the parameters in $L(N,D)$.  The third principle is more speculative.  There is a simple and general reason one might expect overfitting to scale $\\propto 1/D$ at very large $D$.  Overfitting should be related to the variance or the signal-to-noise ratio of the dataset \\cite{1710.03667}, and this scales as $1/D$.  This expectation should hold for any smooth loss function, since we expect to be able to expand the loss about the $D \\to \\infty$ limit.   However, this argument assumes that $1/D$ corrections dominate over other sources of variance, such as the finite batch size and  other limits on the efficacy of optimization.  Without empirical confirmation, we would not be very confident of its applicability.  Our third principle explains the asymmetry between the roles of $N$ and $D$ in Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}.  Very similar symmetric expressions\\footnote{For example, one might have used $L(N,D) =  \\left[ \\left( \\frac{N_c}{N} \\right)^{\\alpha_N} + \\left( \\frac{D_c}{D} \\right)^{\\alpha_D}  \\right]^\\beta$, but this does not  have a $1/D$ expansion.} are possible, but they would not have a $1/D$ expansion with integer powers, and would require the introduction of an additional parameter.  In any case, we will see that our equation for $L(N,D)$ fits the data  well, which is the most important justification for our $L(N,D)$ ansatz.   "
            },
            {
                "section_name": "Results_11",
                "paragraphs": " We regularize all our models with 10\\% dropout, and by tracking test loss and stopping once it is no longer decreasing.   The results are displayed in Figure \\ref{fig:DatasetModelSizevsPerformance}, including a fit to the four parameters $\\alpha_N, \\alpha_D, N_c, D_c$ in Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}:  \\begin{table}[h!] \\centering \\vspace{-0.5em} \\begin{tabular}{|c| c | c | c | c|} \\hline Parameter & $\\alpha_N$ & $\\alpha_D$ & $N_c$ & $D_c$ \\\\ [0.5ex] \\hline\\hline Value  & $0.076$ & $0.103$ & $6.4 \\times 10^{13}$ & $1.8 \\times 10^{13}$ \\\\ \\hline \\end{tabular} \\vspace{0.5em} \\caption{Fits to $L(N, D)$} \\vspace{-1em} \\end{table}  We obtain an excellent fit, with the exception of the runs where the dataset has been reduced by a factor of $1024$, to about $2 \\times 10^7$ tokens.  With such a small dataset, an epoch consists of only 40 parameter updates.  Perhaps such a tiny dataset represents a different regime for language modeling, as overfitting happens very early in training (see Figure \\ref{fig:OverfittingandEarlyStopping}).  Also note that the parameters differ very slightly from those obtained in Section \\ref{sec:Empirical}, as here we are fitting the full $L(N,D)$ rather than just $L(N, \\infty)$ or $L(\\infty, D)$.    To chart the borderlands of the infinite data limit, we can directly study the extent of overfitting.  For all but the largest models, we see no sign of overfitting when training with the full 22B token WebText2 dataset, so we can take it as representative of $D=\\infty$.  Thus we can compare finite $D$ to the infinite data limit by defining \\be \\delta L(N, D) \\equiv \\frac{L(N, D)}{L(N, \\infty)} - 1 \\ee and studying it as a function of $N, D$.   In fact, we see empirically that $\\delta L$ depends only a specific combination of $N$ and $D$, as shown in Figure \\ref{fig:OverfittingandEarlyStopping}.  This follows from the scaling law of Equation \\eqref{eq:FundamentalLikelihioodvsModelandDataSize}, which implies \\be \\label{eq:OverfittingPrediction} \\delta L \\approx  \\left( 1 +  \\left(\\frac{N}{N_c} \\right)^{\\frac{\\alpha_N}{\\alpha_D}} \\frac{D_c}{D} \\right)^{\\alpha_D} - 1 \\ee Note that at large $D$ this formula also has a series expansion in powers of $1/D$.  We estimate that the variation in the loss with different random seeds is roughly $0.02$, which means that to avoid overfitting when training to within that threshold of convergence we require \\be D \\gtrsim (5 \\times 10^3) \\, N^{0.74} \\ee With this relation, models smaller than $10^9$ parameters can be trained with minimal overfitting on the 22B token WebText2 dataset, but our largest models will encounter some mild overfitting.  More generally, this relation shows that dataset size may grow sub-linearly in model size while avoiding overfitting.  Note however that this does not typically represent maximally compute-efficient training.  We should also emphasize that we have not optimized regularization (eg the dropout probability) while varying dataset and model size.   "
            },
            {
                "section_name": "Scaling Laws with Model Size and Training Time_5",
                "paragraphs": "\\label{sec:ScalingSizeandSteps}  In this section we will demonstrate that a simple scaling law provides a good description for the loss as a function of model size $N$ and training time.  First we will explain how to use the results of \\cite{1812.06162}  to define a universal training step $S_{\\rm min}$, which accounts for the fact that most of our models have not been trained at an optimal batch size.   Then we will demonstrate that we can fit the model size and training time dependence of the loss using Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps}.  Later we will use these results to predict the optimal allocation of training compute between model size and training time, and then confirm that prediction.  "
            },
            {
                "section_name": "Adjustment for Training at $B_{\\rm crit_12",
                "paragraphs": "\\label{sec:OptimalBatchSize}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.60\\textwidth]{CriticalBatchSizeVsPerf} \\caption[Critical batch size]{The critical batch size $B_{\\rm crit}$ follows a power law in the loss as performance increase, and does not depend directly on the model size.  We find that the critical batch size approximately doubles for every $13\\%$ decrease in loss.  $B_{\\rm crit}$  is measured empirically from the data shown in Figure \\ref{fig:BatchPareto}, but it is also roughly predicted by the gradient noise scale, as in \\cite{1812.06162}.  \\label{fig:OptimalBatchSize}} \\end{figure}  A simple empirical theory for the batch size dependence of training was developed in \\cite{1812.06162} (see also \\cite{1811.03600, DBLP:journals/corr/abs-1907-04164}).  It was argued that there is a critical batch size $B_{\\rm crit}$ for training; for $B$ up to $B_{\\rm crit}$  the batch size can be increased with very minimal degradation in compute-efficiency, whereas for $B > B_{\\rm crit}$ increases in $B$ result in diminishing returns.  It was also argued that the gradient noise scale provides a simple prediction for $B_{\\rm crit}$, and that neither depends directly on model size except through the value of the loss that has been attained.  These results can be used to predict how training time and compute will vary with the batch size.  To utilize both training time and compute as effectively as possible, it is best to train with a batch size $B \\approx B_{\\rm crit}$.  Training at $B \\gg B_{\\rm crit}$  minimizes the number of training steps, while $B \\ll B_{\\rm crit}$ minimizes the use of compute.  More specifically, it was demonstrated that for a wide variety of neural network tasks, the number of training steps $S$ and the number of data examples processed $E = B S$ satisfy the simple relation \\be \\label{eq:TimeComputeTradeoff} \\left( \\frac{S}{S_{\\rm min}} -1 \\right) \\left( \\frac{E}{E_{\\rm min}} - 1 \\right) = 1 \\ee when training to any fixed value of the loss $L$.  Here $S_{\\rm min}$ is the minimum number of steps necessary to reach $L$, while $E_{\\rm min}$ is the minimum number of data examples that must be processed.  We demonstrate the relation \\eqref{eq:TimeComputeTradeoff} for Transformers in Figure \\ref{fig:BatchPareto} in the appendix. This relation defines the critical batch size \\be \\label{eq:DefinitionBcrit} B_{\\rm crit}(L) \\equiv \\frac{E_{\\rm min}}{S_{\\rm min}} \\ee which is a function of the target value of the loss.  Training at the critical batch size makes a roughly optimal time/compute tradeoff, requiring $2 S_{\\rm min}$ training steps and processing $E = 2 E_{\\rm min}$ data examples.  In Figure \\ref{fig:OptimalBatchSize} we have plotted the critical batch size and gradient noise scale\\footnote{Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of $B_{\\rm crit}$ from Figures \\ref{fig:BatchPareto} and \\ref{fig:OptimalBatchSize} for all our later analyses.  } as a function of training loss for two different models.  We see that $B_{\\rm crit}(L)$ is  independent of model size, and only depends on the loss $L$.  So the predictions of \\cite{1812.06162} continue to hold for Transformer language models.  The critical batch size can be fit with a power-law in the loss \\be B_{\\rm crit}(L) \\approx \\frac{B_*}{L^{1/\\alpha_B}} \\ee where $B_* \\approx 2 \\times 10^8$ and $\\alpha_B \\approx 0.21$.  We have chosen this parameterization for $B_{\\rm crit}(L)$ because as the loss approaches its minimum value $L_{\\rm min}$, the gradient noise scale is expected to diverge, and we expect $B_{\\rm crit}$ to track this noise scale.  We do not know $L_{\\rm min}$, as we see no sign that our models are approaching it, but $L_{\\rm min} > 0$ since the entropy of natural language is non-zero.  Since apparently $L_{\\rm min}$ is much smaller than the values of $L$ we have achieved, we used a parameterization where $B_{\\rm crit}$ diverges as $L \\to 0$.  We will use $B_{\\rm crit}(L)$ to estimate the relation between the number of training steps $S$ while training at batch size $B = 2^{19}$ tokens and the number of training steps while training at $B \\gg B_{\\rm crit}$.  This is simply \\be \\label{eq:AdjustedSteps} S_{\\rm min}(S) \\equiv \\frac{S}{1 + B_{\\rm crit}(L)/B} \\qquad (\\text{minimum steps, at } B \\gg B_{\\rm crit}) \\ee for any given target value $L$ for the loss.  This also defines a critical value of the compute needed to train to $L$ with a model of size $N$ if we were to train at $B \\ll B_{\\rm crit}(L)$. This is \\be \\label{eq:AdjustedCompute} C_{\\rm min}(C) \\equiv \\frac{C  }{1 + B/B_{\\rm crit}(L) } \\qquad (\\text{minimum compute, at } B \\ll B_{\\rm crit}) \\ee where $C = 6 N BS$ estimates the (non-embedding) compute used at batch size $B$.    "
            },
            {
                "section_name": "Results for $L(N, S_{\\rm min_13",
                "paragraphs": " \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.47\\textwidth]{PerfVsParams-ComputeBudget}\\hfill \\includegraphics[width=0.47\\textwidth]{PerfVsParams-StepBudget} \\caption[Performance versus compute budget or number of parameter updates]{ When we hold either total compute or number of training steps fixed, performance follows $L(N,S)$ from Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps2}.  Each value of compute budget has an associated optimal model size that maximizes performance.  Mediocre fits at small $S$ are unsurprising, as the power-law equation for the learning curves breaks down very early in training. \\label{fig:ComputevsParamsvsPerformance}} \\end{figure}  Now we will use $S_{\\rm min}$ defined in Equation \\eqref{eq:AdjustedSteps} to obtain a  simple and universal fit for the dependence of the loss on model size and training time in the infinite data limit.  We will fit the stable, Adam-optimized training runs using Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps}, repeated here for convenience: \\be \\label{eq:FundamentalLikelihioodvsModelandSteps2} L(N, S_{\\rm min}) = \\left( \\frac{N_c}{N} \\right)^{\\alpha_N}  + \\left( \\frac{S_c }{S_{\\rm min}} \\right)^{\\alpha_S} \\ee for the loss.  We include all training steps after the warmup period of the learning rate schedule, and find  a fit to the data with the parameters:  \\begin{table}[h!] \\centering \\begin{tabular}{|c| c | c | c | c| } \\hline Parameter & $\\alpha_N$ & $\\alpha_S$ & $N_c$ & $S_c$   \\\\ [0.5ex] \\hline\\hline Value  & $0.077$ & $0.76$ & $6.5 \\times 10^{13}$ & $2.1 \\times 10^3$  \\\\ \\hline \\end{tabular} \\vspace{0.5em} \\caption{Fits to $L(N, S)$} \\vspace{-1em} \\end{table}  With these parameters, we obtain the learning curve fits in Figure \\ref{fig:LearningCurveFitsandResiduals}.  Though the fits are imperfect, we believe they are quite compelling given the simplicity of Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps2}.  The data and fits can be visualized in a different and more interesting way, as shown in Figure \\ref{fig:ComputevsParamsvsPerformance}.  There we study the test loss as a function of model size while fixing either the total non-embedding compute $C$ used in training, or the number of steps $S$.  For the fits we use Equation \\eqref{eq:AdjustedCompute} and \\eqref{eq:AdjustedSteps} along with the parameters above and Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps2}.  The power-law dependence of the loss on $S_{\\rm min}$ reflects the interplay of optimizer dynamics and the loss landscape.    Since the fits are best late in training, when the loss may be approximately quadratic, the power-law should provide information about the spectrum of the Hessian of the loss.  Its universality suggests that the Hessian eigenvalue density is roughly independent of model size.  "
            },
            {
                "section_name": "Lower Bound on Early Stopping Step_14",
                "paragraphs": "\\label{sec:EarlyStop}  The results for $L(N,S_{\\rm min})$ can be used to derive a lower-bound (and rough estimate) of the step at which early stopping should occur when training is data limited. It is motivated by the idea that finite and infinite $D$ learning curves for a given model will be very similar until we reach $S_{\\rm min} \\approx S_{\\rm stop}$. Thus overfitting should be proportional to the correction from simply ending  training at $S_{\\rm stop}$.  This will underestimate $S_{\\rm stop}$, because in reality the test loss will decrease more slowly when we have a finite $D$, and therefore we will require more training steps to reach the optimal test loss at finite $D$. This line of reasoning leads to the inequality \\be \\label{eq:EarlyStopInequality} S_{\\rm stop}(N,D)  \\gtrsim \\frac{S_c}{\\left[ L(N,D) - L(N, \\infty) \\right]^{1 / \\alpha_S}} \\ee where $L(N, \\infty)$ is the converged loss, evaluated with infinite available data.  This inequality and its comparison to the empirical data is displayed in Figure \\ref{fig:OverfittingandEarlyStopping} in the appendix.  In that figure, the values of $S_{\\rm stop}$ and $L(N,D)$ are empirical (though $S_{\\rm stop}$ is adjusted to mimic training at $B \\gg B_{\\rm crit}$), while $L(N, \\infty)$ is computed from the fit to $L(N,D)$ evaluated at $D=\\infty$.   \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.98\\textwidth]{SuboptimalModels}\\ \\caption[Training on suboptimal models]{\\textbf{Left:} Given a fixed compute budget, a particular model size is optimal, though somewhat larger or smaller models can be trained with minimal additional compute. \\textbf{Right:} Models larger than the compute-efficient size require fewer steps to train, allowing for potentially faster training if sufficient additional parallelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve, after initial transient effects.  \\label{fig:SubOptimalModels}} \\end{figure}   "
            },
            {
                "section_name": "Optimal Allocation of the Compute Budget_6",
                "paragraphs": "\\label{sec:OptimalCompute}  We displayed the \\emph{empirical} trend of performance as a function of the computation used during training in the top-right of Figure \\ref{fig:BasicPowerLaws}.  However, this result involved training at a fixed batch size $B$, whereas we know that in fact we could train more efficiently\\footnote{One might ask why we did not simply train at $B_{\\rm crit}$ in the first place.  The reason is that it depends not only on the model but also on the target value of the loss we wish to achieve, and so is a moving target.} by training at the batch size $B_{\\rm crit}$ discussed in Section \\ref{sec:OptimalBatchSize}. Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively, and correcting for this inefficiency by standardizing to the critical batch size results in cleaner and more predictable trends.  \\begin{figure} \\centering{} \\includegraphics[width=0.5\\textwidth]{ComputeEfficientFrontierWithAdjustment} \\caption[Comparison between empirical and adjusted compute trends]{ When adjusting performance to simulate training far below the critical batch size, we find a somewhat altered power law for $L(C_{\\rm min})$ when compared with the fully empirical results.  The conspicuous lump at $10^{-5}$ PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks in the power-law fits.  It is the $L(C_{\\rm min})$ trend that we expect to provide a reliable extrapolation for larger compute. \\label{fig:ComputeEfficientAdjusted}} \\end{figure}  In this section we will adjust for this oversight. More importantly, we will use the results of Section \\ref{sec:ScalingSizeandSteps} to determine the optimal \\emph{allocation} of compute between model size $N$ and the quantity of data processed during training, namely $2 B_{\\rm crit}  S_{\\rm min}$.  We will determine this allocation both empirically and theoretically, by using the equation for $L(N, S_{\\rm min})$, and we will demonstrate that these methods agree.  "
            },
            {
                "section_name": "Optimal Performance and Allocations_15",
                "paragraphs": " Let us first study the loss as a function of the optimally allocated compute from Equation \\eqref{eq:AdjustedCompute}.  The result is plotted in Figure \\ref{fig:ComputeEfficientAdjusted}, along with a power-law fit.  We see that as compared to the compute plot of Figure \\ref{fig:BasicPowerLaws}, the new fit with $C_{\\rm min}$ is somewhat improved.  Given $L(C_{\\rm min})$, it is natural to ask for the optimal model size $N(C_{\\rm min})$ that provides the minimal loss with a given quantity of training compute.  The optimal model size is shown in Figure \\ref{fig:ComputevsPerformance}.  We observe that $N(C_{\\rm min})$ can be fit very well with a power-law \\be N(C_{\\rm min}) \\propto (C_{\\rm min})^{0.73}. \\ee In Figure \\ref{fig:SubOptimalModels}, we show the effect of training models of sub-optimal sizes (see Appendix \\ref{sec:suboptimal-models}).  By definition $C_{\\rm min} \\equiv 6 N B_{\\rm crit} S$, and so we can use $N(C_{\\rm min})$ to extract further results.  In particular, since prior fits show $B \\propto L^{-4.8}$ and $L \\propto C_{\\rm min}^{-0.05}$, we can conclude that $B_{\\rm crit} \\propto C_{\\rm min}^{0.24}$.  This leads us to conclude that the optimal number of steps will only grow very slowly with compute, as \\be S_{\\rm min} \\propto (C_{\\rm min})^{0.03}, \\ee matching the empirical results in Figure \\ref{fig:ComputevsPerformance}.  In fact the measured exponent is sufficiently small that our results may even be consistent with an exponent of zero.  Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size $N$, while simultaneously scaling up the batch size  via $B \\propto B_{\\rm crit}$ with negligible increase in the number of serial steps.  Since compute-efficient training uses relatively few optimization steps, additional work on speeding up early training dynamics may be warranted.   \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{ComputevsOptimalModelSize}\\hfill \\includegraphics[width=0.48\\textwidth]{ComputeEfficientSteps} \\caption[Optimal model size and serial number of steps versus compute budget]{ \\textbf{Left:} Each value of the compute budget $C_{\\rm min}$ has an associated optimal model size $N$.  Optimal model size grows very rapidly with $C_{\\rm min}$, increasing by 5x for each 10x increase in compute.  The number of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x. \\textbf{Right:} The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most of the growth in data examples processed can be used for increased batch sizes. \\label{fig:ComputevsPerformance}} \\end{figure}  "
            },
            {
                "section_name": "Predictions from $L(N, S_{\\rm min_16",
                "paragraphs": " The results for $L(C_{\\rm min})$ and the allocations can be predicted  from the $L(N, S_{\\rm min})$ equation obtained in Section \\ref{sec:ScalingSizeandSteps}.  Given our  equation for $L(N, S_{\\rm min})$, we can substitute $S_{\\rm min} = \\frac{C_{\\rm min}}{6 N B}$ and then find the minimum of the loss as a function of $N$, while fixing the training compute. We carry out this procedure in detail  in Appendix \\ref{app:ComputeEfficientTraining}, where we also provide some additional predictions.  For the loss as a function of training compute, we predict that \\be L(C_{\\rm min}) = \\left( \\frac{C_c^{\\rm min}}{C_{\\rm min}} \\right)^{\\alpha_C^{\\rm min}} \\ee where \\be \\alpha_C^{\\rm min} \\equiv \\frac{1}{1/\\alpha_S + 1/\\alpha_B + 1/\\alpha_N} \\approx 0.054 \\ee in excellent agreement with the exponent of Figure \\ref{fig:ComputeEfficientAdjusted}.  We also predict that \\be N(C_{\\rm min}) \\propto (C_{\\rm min})^{\\alpha_C^{\\rm min} / \\alpha_N} \\approx  (C_{\\rm min})^{0.71} \\ee which also matches the scaling of Figure \\ref{fig:ComputevsPerformance} to within a few percent.  Our scaling laws provide a predictive framework for the performance of language modeling.  "
            },
            {
                "section_name": "Contradictions and a Conjecture_17",
                "paragraphs": "\\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.8\\textwidth]{Contradiction} \\caption[Contradiction between compute and data trends]{Far beyond the model sizes we study empirically, we find a contradiction between our equations for $L(C_{\\rm min})$ and $L(D)$ due to the slow growth of data needed for compute-efficient training.  The intersection marks the point before which we expect our predictions to break down.  The location of this point is highly sensitive to the precise exponents from our power-law fits. \\label{fig:Contradiction}} \\end{figure}  We observe no signs of deviation from straight power-law trends at large values of compute, data, or model size. Our trends must eventually level off, though, since natural language has non-zero entropy.  Indeed, the trends for compute-efficient training described in this section already contain an apparent contradiction. At scales several orders of magnitude above those documented here, the performance predicted by the $L(C_{\\rm min})$ scaling law decreases below what should be possible given the slow growth in training data with compute.  This implies that our scaling laws must break down before this point, but we conjecture that the intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language models reach maximal performance.  Since the amount of data used by compute-efficient training grows  slowly with the compute budget, the performance predicted by $L(C_{\\rm min})$ eventually hits a lower bound set by the $L(D)$ power law (see Figure \\ref{fig:Contradiction}).  Let us work this out in more detail.  To keep overfitting under control, the results of Section \\ref{sec:ChartingOverfitting} imply that we should scale the dataset size as \\be \\label{eq:DataGrowthOverfitting} D \\propto N^{0.74} \\propto C_{\\rm min}^{0.54} \\ee where we have used the compute-efficient $N(C_{\\rm min})$ from Figure \\ref{fig:ComputevsPerformance}.  Let us compare this to the data requirements of compute-efficient training.  If we train at the critical batch size (i.e. $C=2C_{\\rm min}$) and never re-use data during training, we find that data usage grows with compute as \\be D (C_{\\rm min}) = \\frac{2C_{\\rm min}}{6 N(C_{\\rm min})} \\approx \\left( 4 \\times 10^{10}  \\ {\\rm tokens} \\right) (C_{\\rm min} / \\mathrm{PF}{\\text -}\\mathrm{Day}  )^{0.26} \\ee This is the maximum rate at which the dataset size can productively grow with compute, since it means that we are only training for a single epoch.  But it grows the dataset much more slowly than in Equation \\eqref{eq:DataGrowthOverfitting}.  It appears to imply that compute-efficient training will eventually run into a problem with overfitting, even if the training process never re-uses any data!    According to Figure \\ref{fig:BasicPowerLaws}, we expect that when we are bottlenecked by the dataset size (ie by overfitting), the loss should scale as $L(D) \\propto D^{-0.095}$.  This implies that the loss would scale with compute as $L(D(C_{\\rm min})) \\propto C_{\\rm min}^{-0.03}$ once we are data-limited.  Once again, we have a contradiction, as this will eventually intersect with our prediction for $L(C_{\\rm min})$ from Figure \\ref{fig:ComputeEfficientAdjusted}, where we found a scaling $L(C_{\\rm min}) \\propto C_{\\rm min}^{-0.050}$.  The intersection point of $L(D(C_{\\rm min}))$ and $L(C_{\\rm min})$ occurs at \\be C^* \\sim 10^4~\\mathrm{PF}{\\text -}\\mathrm{Days} \\quad N^* \\sim 10^{12}~\\text{parameters}, \\quad D^* \\sim 10^{12}~\\text{tokens}, \\quad L^* \\sim 1.7~\\text{nats/token} \\label{eq:SpecialPoint} \\ee though the numerical values are highly uncertain, varying by an order or magnitude in either direction depending on the precise values of the exponents from the power-law fits.  The most obvious interpretation is that our scaling laws break down at or before we reach this point, which is still many orders of magnitude away in both compute and model size.  One might also conjecture that this intersection point has a deeper meaning.  If we cannot increase the model size beyond $N^*$ without qualitatively different data requirements, perhaps this means that once we reach $C_{\\rm min}^*$ and $N^*$, we have extracted all of the reliable information available in natural language data.  In this interpretation, $L^*$ would provide a rough estimate for the entropy-per-token\\footnote{Defining words using the \\texttt{wc} utility, the WebText2 dataset has $1.4$ tokens per word and $4.3$ characters per token. } of natural language.  In this scenario, we would expect the loss trend to level off at or before $L^*$.  We can guess at the functional form of $L(C_{\\rm min})$ as it levels off by considering a version of our training dataset with added noise.  For example, we could append a random string of tokens to each context shown to the model to artificially boost the loss by a constant additive factor. Then, the distance from the noise floor $L-L_{\\rm noise}$ would be a more meaningful performance metric, with even a small decrease in this distance potentially representing a significant boost in qualitative performance. Since the artificial noise would affect all of our trends equally, the critical point of \\ref{eq:SpecialPoint} would not change (aside from the absolute value of $L^*$), and may be meaningful even if it occurs after the leveling off.  "
            },
            {
                "section_name": "Related Work_7",
                "paragraphs": " Power laws can arise from a wide variety of sources \\cite{thurner2018introduction}.  Power-law scalings with model and dataset size in density estimation \\cite{wasserman2006all} and  in random forest models \\cite{biau2012analysis} may be connected with our results. These models suggest that power-law exponents may have a very rough interpretation as the inverse of the number of relevant features in the data.  Some early \\cite{banko2001scaling, DBLP:journals/corr/cs-CL-0108005}   work found power-law scalings between performance and dataset size.  More recent work \\cite{1712.00409, Hestness:2019:BHA:3293883.3295710} also investigated scaling between model size and data size; their work is perhaps the closest to ours in the literature\\footnote{After this work was completed, \\cite{rosenfeld2019constructive} also appeared, which makes similar predictions for the dependence of loss on both model and dataset size.}.  Note, however, that \\cite{1712.00409} found super-linear scaling of dataset size with model size, whereas  we find a sub-linear scaling.  There are some parallels between our findings on optimal allocation of compute and \\cite{1906.06669}, including power-law learning curves.  EfficientNets \\cite{DBLP:journals/corr/abs-1905-11946} also appear to obey an approximate power-law relation between accuracy and model size.  Very recent work \\cite{1909.12673} studies scaling with both dataset size and model size for a variety of datasets, and fits an ansatz similar to ours.  EfficientNet \\cite{DBLP:journals/corr/abs-1905-11946}   advocates scaling depth and width exponentially (with different coefficients) for optimal performance of image models, resulting in a power-law scaling of width as a function of depth.    We find that for language models this power should be roughly one when scaling up (as width/depth should remain fixed). But more importantly, we find that the precise architectural hyperparameters are unimportant compared to the overall scale of the language model.  In \\cite{ResNetsEnsemblesShallow} it was argued that deep models can function as ensembles of shallower models, which could potentially explain this finding.  Earlier work  \\cite{Zagoruyko_2016} has compared width and depth, and found that wide ResNets can outperform deep ResNets on image classification. Some studies fix computation per data example, which tends to scale in proportion to the number of model parameters, whereas we investigate scaling with both model size and the quantity of training computation.  Various works \\cite{1710.03667,1812.11118} have investigated generalization in highly overparameterized models, finding a ``jamming transition'' \\cite{1901.01608} when the model size reaches the dataset size (this may  require training many orders of magnitude beyond typical practice, and in particular does not use early stopping).  We do not observe such a transition, and find that the necessary training data scales sublinearly in the model size. Expansions in the model size, particularly at large width \\cite{jacot2018neural, 1902.06720}, may provide a useful framework for thinking about some of our scaling relations.  Our results on optimization, such as the shape of learning curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions \\cite{DBLP:journals/corr/abs-1907-04164} in realistic settings.  Making this connection quantitative will require a characterization of the Hessian spectrum \\cite{DBLP:journals/corr/abs-1811-07062, DBLP:journals/corr/abs-1901-10159, unpublished-grd}.  "
            },
            {
                "section_name": "Discussion_8",
                "paragraphs": "  We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter count $N$, dataset size $D$, and optimized training computation $C_{\\rm min}$, as encapsulated in Equations \\eqref{eq:FundamentalLikelihioodvsModelandDataSize} and \\eqref{eq:FundamentalLikelihioodvsModelandSteps}.  Conversely, we find very weak dependence on many architectural and optimization hyperparameters.  Since scalings with $N, D, C_{\\rm min}$ are power-laws, there are diminishing returns with increasing scale.  We were able to precisely model the dependence of the loss on $N$ and $D$, and alternatively on $N$ and $S$, when these parameters are varied simultaneously.  We used these relations to derive the compute scaling, magnitude of overfitting, early stopping step, and data requirements when training large language models. So our scaling relations go beyond mere observation to provide a predictive framework.  One might interpret these relations as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way, independent of most of the details of its microscopic consituents.  It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a maximum likelihood loss, and perhaps in other settings as well.    To this purpose, it will be  interesting to test these relations on other domains, such as images, audio, and video models, and perhaps also for random network distillation.  At this point we do not know which of our results depend on the structure of natural language data, and which are universal.  It would also be exciting to find a theoretical framework from which the scaling relations can be derived: a `statistical mechanics' underlying the `thermodynamics' we have observed.  Such a theory might make it possible to derive other more precise predictions, and provide a systematic understanding of the limitations of the scaling laws.  In the domain of natural language, it will be important to investigate whether continued improvement on the loss translates into improvement on relevant language tasks.  Smooth quantitative change can mask major qualitative improvements: ``more is different''.  For example, the smooth aggregate growth of the economy provides no indication of the specific technological developments that underwrite it. Similarly, the smooth improvements in language model loss may hide seemingly qualitative changes in capability.  Our results strongly suggest that larger models will continue to perform better, and will also be much more sample efficient than has been previously appreciated.  Big models may be more important than big data.  In this context, further investigation into model parallelism is warranted. Deep models can be trained using pipelining \\cite{DBLP:journals/corr/abs-1811-06965}, which splits parameters depth-wise between devices, but eventually requires increased batch sizes as more devices are used.  Wide networks on the other hand are more amenable to parallelization \\cite{shazeer2018meshtensorflow}, since large layers can be split between multiple workers with less serial dependency.  Sparsity \\cite{DBLP:journals/corr/abs-1904-10509,gray2017gpu} or branching (e.g. \\cite{Krizhevsky:2012:ICD:2999134.2999257}) may allow for even faster training of large networks through increased model parallelism.  And using methods like \\cite{Wang_2017,wen2019autogrow}, which grow networks as they train, it might be possible to remain on the compute-efficient frontier for an entire training run.   \\section*{Acknowledgements}  We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner, Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feedback on drafts of this work.   \\newpage \\appendix \\appendixpage \\addappheadtotoc   "
            },
            {
                "section_name": "Summary of Power Laws_9",
                "paragraphs": " For easier reference, we provide a summary below of the key trends described throughout the paper.  \\begin{table}[h!] \\centering \\vspace{-0.5em} \\begin{tabular}{|c|c|c|c|l|} \\hline \\textbf{Parameters}  & \\textbf{Data}  & \\textbf{Compute}  & \\textbf{Batch Size}  & \\textbf{Equation}\\tabularnewline \\hline \\hline $N$  & $\\infty$  & \\multicolumn{1}{c|}{$\\infty$ } & Fixed  & $L\\left(N\\right)=\\left(N_{{\\rm c}}/N\\right)^{\\alpha_{N}}$\\tabularnewline \\hline $\\infty$  & $D$  & \\multicolumn{1}{c|}{Early Stop } & Fixed  & $L\\left(D\\right)=\\left(D_{{\\rm c}}/D\\right)^{\\alpha_{D}}$\\tabularnewline \\hline Optimal  & $\\infty$  & $C$  & Fixed  & $L\\left(C\\right)=\\left(C_{{\\rm c}}/C\\right)^{\\alpha_{C}}$ (naive)\\tabularnewline \\hline $N_{{\\rm opt}}$ & $D_{{\\rm opt}}$ & $C_{{\\rm min}}$  & $B\\ll B_{{\\rm crit}}$  & $L\\left(C_{{\\rm min}}\\right)=\\left(C_{{\\rm c}}^{{\\rm min}}/C_{{\\rm min}}\\right)^{\\alpha_{C}^{{\\rm min}}}$\\tabularnewline \\hline $N$  & $D$  & \\multicolumn{1}{c|}{Early Stop } & Fixed  & $L\\left(N,D\\right)=\\left[\\left(\\frac{N_{{\\rm c}}}{N}\\right)^{\\frac{\\alpha_{N}}{\\alpha_{D}}}+\\frac{D_{c}}{D}\\right]^{\\alpha_{D}}$\\tabularnewline \\hline $N$  & $\\infty$  & $S$ steps & $B$  & $L\\left(N,S\\right)=\\left(\\frac{N_{{\\rm c}}}{N}\\right)^{\\alpha_{N}}+\\left(\\frac{S_{{\\rm c}}}{S_{{\\rm min}}\\left(S,B\\right)}\\right)^{\\alpha_{S}}$\\tabularnewline \\hline \\end{tabular} \\vspace{0.5em} \\caption[Key trend equations]{} \\vspace{-1em} \\end{table}  The empirical fitted values for these trends are:   \\begin{table}[h!] \\centering \\vspace{-0.5em} \\begin{tabular}{|l|l|} \\hline \\textbf{Power Law}  & \\textbf{Scale (tokenization-dependent)}\\tabularnewline \\hline \\hline $\\alpha_{N}=0.076$  & $N_{{\\rm c}}=8.8\\times10^{13}$ params (non-embed)\\tabularnewline \\hline $\\alpha_{D}=0.095$  & $D_{{\\rm c}}=5.4\\times10^{13}$ tokens\\tabularnewline \\hline $\\alpha_{C}=0.057$  & $C_{{\\rm c}}=1.6\\times10^{7}$ PF-days\\tabularnewline \\hline $\\alpha_{C}^{{\\rm min}}=0.050$  & $C_{{\\rm c}}^{{\\rm min}}=3.1\\times10^{8}$ PF-days\\tabularnewline \\hline $\\alpha_{B}=0.21$  & $B_{\\ast}=2.1\\times10^{8}$ tokens\\tabularnewline \\hline $\\alpha_{S}=0.76$  & $S_{{\\rm c}}=2.1\\times10^{3}$ steps\\tabularnewline \\hline \\end{tabular} \\vspace{0.5em} \\caption[Key parameters to trend fits]{} \\vspace{-1em} \\end{table}  The optimal parameters for compute efficient training are given by:  \\begin{table}[h!] \\centering \\vspace{-0.5em} \\begin{tabular}{|l|l|l|} \\hline \\textbf{Compute-Efficient Value} & \\textbf{Power Law} & \\textbf{Scale}\\tabularnewline \\hline \\hline $N_{{\\rm opt}}=N_{e}\\cdot C_{{\\rm min}}^{p_{N}}$ & $p_{N}=0.73$ & $N_{e}=1.3\\cdot10^{9}$ params\\tabularnewline \\hline $B \\ll B_{{\\rm crit}}=\\frac{B_{\\ast}}{L^{1/\\alpha_{B}}}=B_{e}C_{{\\rm min}}^{p_{B}}$ & $p_{B}=0.24$ & $B_{e}=2.0\\cdot10^{6}$ tokens\\tabularnewline \\hline $S_{{\\rm min}}=S_{e}\\cdot C_{{\\rm min}}^{p_{S}}$ (lower bound) & $p_{S}=0.03$ & $S_{e}=5.4\\cdot10^{3}$ steps\\tabularnewline \\hline $D_{{\\rm opt}}=D_{e}\\cdot C_{{\\rm min}}^{p_{D}}$ (1 epoch) & $p_{D}=0.27$ & $D_{e}=2\\cdot10^{10}$ tokens\\tabularnewline \\hline \\end{tabular} \\vspace{0.5em} \\caption[Trends for compute-efficient training]{} \\vspace{-1em} \\end{table}      "
            },
            {
                "section_name": "Empirical Model of Compute-Efficient Frontier_10",
                "paragraphs": "\\label{app:ComputeEfficientTraining}  Throughout this appendix all values of $C, S,$ and $\\alpha_C$ are adjusted for training at the critical batch size $B_{\\rm crit}$.  We have left off the `adj' label to avoid cluttering the notation.  "
            },
            {
                "section_name": "Defining Equations_18",
                "paragraphs": "The power-law fit to the learning curves implies a simple prescription for compute-efficient training. In this appendix, we will derive the optimal performance, model size, and number of training steps as a function of the compute budget. We start with the Equation \\eqref{eq:FundamentalLikelihioodvsModelandSteps}, repeated here for convenience: \\begin{equation} L\\left(N,S\\right)=\\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}}+\\left(\\frac{S_{c}}{S}\\right)^{\\alpha_{S}}. \\end{equation} Here, $S$ represents the number of parameter updates when training \\textbf{at the critical batch size} \\cite{1812.06162}, which was defined in Equation \\eqref{eq:DefinitionBcrit}\\footnote{There is a slight ambiguity here: we can imagine training either at a constant batch size $B\\left(L_{{\\rm target}}\\right)$, or we could instead train at a variable batch size $\\tilde{B}\\left(L\\right)$, where $\\tilde{B}$ is the instantaneous critical batch size (as opposed to $B$, which is the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see \\cite{1812.06162}).}: \\begin{equation} B\\left(L\\right)=\\frac{B_{\\ast}}{L^{1/\\alpha_{B}}}. \\end{equation} We would like to determine optimal training parameters for a fixed compute budget, so we replace $S=C/\\left(6NB\\left(L\\right)\\right)$, where $C$ is the number of FLOPs used in the training run: \\begin{equation} L\\left(N,C\\right)=\\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}}+\\left(6B_{\\ast}S_{c}\\frac{N}{L^{1/\\alpha_{B}}C}\\right)^{\\alpha_{S}}.\\label{eq:loss-params-compute} \\end{equation} Now, we set $\\partial_{N}L\\big|_{C}=0$ to find the condition for optimality: \\begin{align} 0 & =\\frac{\\partial L}{\\partial N}\\big|_{C}\\nonumber \\\\ & =-\\frac{\\alpha_{N}}{N}\\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}}+\\frac{\\alpha_{S}}{N}\\left(6B_{\\ast}S_{c}\\frac{N}{L^{1/\\alpha_{B}}C}\\right)^{\\alpha_{S}}\\left(1-5\\frac{N}{L}\\cancel{\\frac{\\partial L}{\\partial N}\\big|_{C}}\\right)\\nonumber \\\\ \\implies\\frac{\\alpha_{N}}{\\alpha_{S}}\\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}} & =\\left(6B_{\\ast}S_{c}\\frac{N}{L^{1/\\alpha_{B}}C}\\right)^{\\alpha_{S}}\\label{eq:compute-optimality} \\end{align} Equation \\eqref{eq:loss-params-compute} and \\eqref{eq:compute-optimality} together determine the compute-efficient frontier.    "
            },
            {
                "section_name": "Efficient Training_19",
                "paragraphs": "Now we assemble the implications of \\eqref{eq:loss-params-compute} and \\eqref{eq:compute-optimality}. First, note that inserting \\eqref{eq:compute-optimality} into \\eqref{eq:loss-params-compute} yields \\begin{equation} L\\left(N_{{\\rm eff}}\\left(C\\right),C\\right)=\\left(1+\\frac{\\alpha_{N}}{\\alpha_{S}}\\right)L\\left(N_{{\\rm eff}},\\infty\\right), \\end{equation} which implies that for compute-efficient training, we should train to a \\textbf{fixed percentage} $\\frac{\\alpha_{N}}{\\alpha_{S}}\\approx10\\%$ above the converged loss. Next, let's determine how the optimal loss depends on the compute budget. Eliminating $N$ yields a power-law dependence of performance on compute: \\begin{align} L\\left(C\\right) & =\\left(\\frac{C_{c}}{C}\\right)^{\\alpha_{C}} \\end{align} where we defined \\begin{align} \\alpha_{C} & =1/\\left(1/\\alpha_{S}+1/\\alpha_{B}+1/\\alpha_{N}\\right)\\approx0.052\\\\ C_{c} & =6N_{c}B_{\\ast}S_{c}\\left(1+\\frac{\\alpha_{N}}{\\alpha_{S}}\\right)^{1/\\alpha_{S}+1/\\alpha_{N}}\\left(\\frac{\\alpha_{S}}{\\alpha_{N}}\\right)^{1/\\alpha_{S}}. \\end{align} Similarly, we can eliminate $L$ to find $N\\left(C\\right)$: \\begin{align} \\frac{N\\left(C\\right)}{N_{c}}=\\left(\\frac{C}{C_{c}}\\right)^{\\alpha_{C}/\\alpha_{N}}\\left(1+\\frac{\\alpha_{N}}{\\alpha_{S}}\\right)^{1/\\alpha_{N}} \\end{align} and \\begin{align} S\\left(C\\right) & =\\frac{C_{c}}{6N_{c}B_{\\ast}}\\left(1+\\frac{\\alpha_{N}}{\\alpha_{S}}\\right)^{-1/\\alpha_{N}}\\left(\\frac{C}{C_{c}}\\right)^{\\alpha_{C}/\\alpha_{S}} \\end{align}  "
            },
            {
                "section_name": "Comparison to Inefficient_20",
                "paragraphs": "Typically, researchers train models until they appear to be close to convergence. In this section, we compare the efficient training procedure described above to this more typical setup. We define a the convergence factor $f$ as the percent deviation from the converged loss: \\begin{equation} L\\left(N,C\\right)=\\left(1+f\\right)L\\left(N,\\infty\\right). \\end{equation} For compute-efficient training we have $f=\\alpha_{N}/\\alpha_{S}\\approx10\\%$ from the previous section, but researchers typically use a much smaller value. Here, we choose $f^{\\prime}=2\\%$ as an estimate. For a fixed value of the loss, we predict: \\begin{align} \\frac{N_{f}}{N_{f^{\\prime}}} & =\\left(\\frac{1+f}{1+f^{\\prime}}\\right)^{1/\\alpha_{N}}\\approx2.7\\\\ \\frac{S_{f}}{S_{f^{\\prime}}} & =\\left(\\frac{1+\\frac{1}{f}}{1+\\frac{1}{f^{\\prime}}}\\right)^{1/\\alpha_{S}}\\approx0.13\\\\ \\frac{C_{f}}{C_{f^{\\prime}}} & =\\frac{N_{f}}{N_{f^{\\prime}}}\\frac{S_{f}}{S_{f^{\\prime}}}\\approx0.35 \\end{align} So that compute-efficient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65\\% less compute to reach the same loss.  "
            },
            {
                "section_name": "Suboptimal Model Sizes_21",
                "paragraphs": "\\label{sec:suboptimal-models} We can solve A.1 to find an expression for the amount of compute needed to reach a given value of the loss $L$ with a model of size $N$: \\be C\\left(N,L\\right)=\\left(6B_{\\ast}S_{c}\\frac{N}{L^{1/\\alpha_{B}}}\\right)\\left(L-\\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}}\\right)^{-1/\\alpha_{S}}. \\ee Using A.6 and A.9, we can eliminate $L$ in favor of $N_{{\\rm eff}}\\left(L\\right)$, the model size which reaches $L$ most efficiently. From there, we find an expression for the excess compute needed as a consequence of using a suboptimal model size: \\be \\frac{C\\left(N,N_{{\\rm eff}}\\right)}{C\\left(N_{{\\rm eff}},N_{{\\rm eff}}\\right)}=\\frac{N}{N_{{\\rm eff}}}\\left[1+\\frac{\\alpha_{S}}{\\alpha_{N}}\\left(1-\\left(\\frac{N_{{\\rm eff}}}{N}\\right)^{\\alpha_{N}}\\right)\\right]^{-1/\\alpha_{S}}. \\ee The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a 20\\% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism and faster training if sufficient harware is available (see Figure Y): \\be \\frac{S\\left(N,N_{{\\rm eff}}\\right)}{S\\left(N_{{\\rm eff}},N_{{\\rm eff}}\\right)}=\\left[1+\\frac{\\alpha_{S}}{\\alpha_{N}}\\left(1-\\left(\\frac{N_{{\\rm eff}}}{N}\\right)^{\\alpha_{N}}\\right)\\right]^{-1/\\alpha_{S}}. \\ee A 2.2x larger model requires 45\\% fewer steps at a cost of 20\\% more training compute. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve after initial transient effects.    "
            },
            {
                "section_name": "Caveats_11",
                "paragraphs": " In this section we list some potential caveats to our analysis.  \\begin{itemize} \\item At present we do not have a solid theoretical understanding for any of our proposed scaling laws.  The scaling relations with model size and compute are especially mysterious.  It may be possible to understand scaling at very large $D$ holding model size fixed \\cite{1710.03667}, and also the shape of learning curves late in training, by modeling the loss with a noisy quadratic.  But the scaling with $D$ at very large model size still remains mysterious.  Without a theory or a systematic understanding of the corrections to our scaling laws, it's difficult to determine in what circumstances they can be trusted.  \\item  We are not especially confident in the prediction of $B_{\\rm crit}(L)$ for values of the loss far outside the range we have explored.  Changes in $B_{\\rm crit}$ could have a significant impact on trade-offs between data parallelism and the number of serial training steps required, which would have a major impact on training time.  \\item We did not thoroughly investigate the small data regime, and our fits for $L(N,D)$ were poor for the smallest values of $D$ (where an epoch corresponded to only $40$ steps). Furthermore, we did not experiment with regularization and data augmentation. Improvements in these could alter our results,  quantitatively or qualitatively.  \\item We used the estimated training compute $C \\approx 6 N B S$, which did not include contributions proportional to  $n_{\\rm ctx}$ (see Section \\ref{sec:ParameterComputeCounts}).  So our scalings with compute may be confounded in practice in the regime of very large $n_{\\rm ctx}$, specifically where $n_{\\rm ctx} \\gtrsim 12 d_{\\rm model}$.  \\item We tuned learning rates, and we experimented with learning rate schedules.  But we may have neglected to tune some hyperparameter (e.g. intialization scale or  momentum) that have an important effect on scaling.  \\item The optimal choice of learning rate is sensitive to the target loss. When training close to convergence, it may be necessary to use a smaller learning rate to avoid divergences.  But when conducting a short training run (eg due to compute limitations), it may be possible to use a larger learning rate.  We did not experiment with higher learning rates for training runs that did not proceed to convergence. \\end{itemize}  "
            },
            {
                "section_name": "Supplemental  Figures_12",
                "paragraphs": " \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{EarlyStoppingvsNandD}\\hfill \\includegraphics[width=0.48\\textwidth]{TrainVsTestMedium} \\caption[Early stopping lower bound and training curves for overfit models]{ {\\bf Left:}  We characterize the step on which early stopping occurs, as a function of the extent of overfitting.  The red line indicates a {lower bound} for early stopping that is derived in Section \\ref{sec:EarlyStop}.  {\\bf Right:} We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-samples.  The test loss typically follows that of a run done with unrestricted data until diverging. Note that the degree of overfitting (as compared to the infinite data limit) is significantly overestimated by $L_{\\rm test} - L_{\\rm train}$ (denoted by a black bar for each run). \\label{fig:OverfittingandEarlyStopping}} \\end{figure}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{ComputeVsParamswithUTNoReuse}\\hfill \\includegraphics[width=0.48\\textwidth]{ComputeVsParamswithUTWithReuse} \\caption[Universal transformers]{We compare recurrent Transformers \\cite{DBLP:journals/corr/abs-1807-03819}, which re-use parameters, to standard Transformers. Recurrent Transformers perform slightly better when comparing models with equal parameter count, but slightly worse when accounting for reuse and comparing per FLOP. \\label{fig:RecurrentTransformers}} \\end{figure}    "
            },
            {
                "section_name": "Early Stopping and Test vs Train_22",
                "paragraphs": "\\label{sec:OverfittingandEarlyStopping}  In section \\ref{sec:EarlyStop} we described the result shown in Figure \\ref{fig:OverfittingandEarlyStopping}, which provides a prediction for a lower bound on the early stopping step.  We also show the train and test loss for a given model size when training on different sized datasets.   "
            },
            {
                "section_name": "Universal Transformers_23",
                "paragraphs": " We  compare the performance of standard Transformers  to recurrent Transformers \\cite{DBLP:journals/corr/abs-1807-03819} in Figure \\ref{fig:RecurrentTransformers}.  These models re-use parameters, and so perform slightly better as a function of $N$, but slightly worse as a function of compute $C$.  We include several different different possibilities for parameter re-use.     "
            },
            {
                "section_name": "Batch Size_24",
                "paragraphs": " We measure the critical batch size using the data displayed in figure \\ref{fig:BatchPareto}.  This made it possible to estimate $B_{\\rm crit}(L)$ in figure \\ref{fig:OptimalBatchSize}.  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.48\\textwidth]{BatchParetoFronts3M} \\includegraphics[width=0.48\\textwidth]{BatchParetoFronts85M} \\caption[Batch size scans]{These figures demonstrate fits to Equation \\eqref{eq:TimeComputeTradeoff} for a large number of values of the loss $L$, and for two different Transformer model sizes.  These fits were used to measure $B_{\\rm crit}(L)$ for Figure \\ref{fig:OptimalBatchSize}.  \\label{fig:BatchPareto}} \\end{figure}  "
            },
            {
                "section_name": "Sample Efficiency vs Model Size_25",
                "paragraphs": " It is easy to see from figure \\ref{fig:EfficiencyIllustration} that larger models train faster, and are therefore more sample efficient.  We provide another way of looking at this phenomenon in figure \\ref{fig:SampleEfficiency}, which shows when different models reach various fixed values of the loss.  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.49\\textwidth]{SampleEfficiencystep_min} \\includegraphics[width=0.49\\textwidth]{SampleEfficiencyexamples_min} \\caption[Another look at sample efficiency]{The number of minimum serial steps needed to reach any fixed value of the test loss decreases precipitously with model size.  Sample efficiency (show here for training far below the critical batch size) improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model to a very large one.   \\label{fig:SampleEfficiency}} \\end{figure}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.50\\textwidth]{ContextPowerLaw} \\includegraphics[width=0.48\\textwidth]{SingleRunTokenTraining}  \\caption[Power-law dependence of performance on position in context]{ This figure provides information about the performance per token as a function of model size and training time.  {\\bf Left:} Loss per token as a function of its position $T$ in the 1024-token context.  Loss scales predictably as a power-law in $T$.  {\\bf Right: } Test loss per token as a function of training step.  \\label{fig:MoreTokenAnalysis}} \\end{figure}    "
            },
            {
                "section_name": "Context Dependence_26",
                "paragraphs": "\\label{sec:ContextDependence}  \\begin{figure} \\noindent \\centering{}  \\includegraphics[width=0.54\\textwidth]{PerformancevsModelSizevsContext}  \\caption[Performance at different context positions versus model size]{In addition to the averaged loss, individual tokens within the 1024-token context also improve smoothly as model size increases.  Training runs with shorter context $n_{\\rm ctx} = 8$ (dashed lines) perform better on early tokens, since they can allocate all of their capacity to them.  \\label{fig:PerformancevsModelSizevsContext}} \\end{figure}     The trends for loss as a function of model size are displayed for different tokens in the context in Figure \\ref{fig:PerformancevsModelSizevsContext}.  We see that models trained on $n_{\\rm ctx} = 1024$ show steady improvement with model size on all but the first token.  Fixing model size, it appears that the loss scales as a power-law as a function of position $T$ in the context, see Figure \\ref{fig:MoreTokenAnalysis}.  This may be a consequence of underlying power-law correlations in language \\cite{ebeling1994entropy, altmann2012origin, lin2016criticality}, or a more general feature of the model architecture and optimization.  It provides some suggestion for the potential benefits (or lack thereof) from training on larger contexts.  Not only do larger models converge to better performance at $T=1024$, but they also improve more quickly at early tokens, suggesting that larger models are more efficient at detecting patterns with less contextual information.  In the right-hand plot we show how per-token performance varies for a fixed model as a function of the training step.  The model begins by learning short-range information, and only learns longer-range correlations later in training.  We have also included models trained with a tiny context $n_{\\rm ctx} = 8$ in order  to compare with our longer context models.  Even modestly sized models trained on $n_{\\rm ctx} = 8$ can dominate our largest $n_{\\rm ctx} = 1024$ models on very early tokens.  This also suggests that further improvements should be possible with much larger models trained on large contexts.  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.46\\textwidth]{VariousLearningSchedules} \\includegraphics[width=0.46\\textwidth]{LearningRateSchedulesvsPerformance} \\caption[Learning rate schedule scan]{ We test a variety of learning rate schedules including cosine decay, linear decay, as well as other faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we do not decay to zero, since we find that this tends to give a fixed improvement close to the end of training. We find that, as long as the learning rate is not too small and does not decay too quickly, performance does not depend strongly on learning rate. Run-to-run variation is at the level of ~0.05 in the loss, so averaging multiple runs is necessary to validate performance changes smaller than this level. \\label{fig:LearningRateSchedules} } \\end{figure}  "
            },
            {
                "section_name": "Learning Rate Schedules and Error Analysis_27",
                "paragraphs": "\\label{app:OptimizationDetailsandErrorAnalysis}  We experimented with a variety of learning rates and schedules.  A host of schedules and resulting test performances for a small language model are plotted in Figure \\ref{fig:LearningRateSchedules}. We conclude that the choice of learning rate schedule is mostly irrelevant, as long as the total summed learning rate is sufficiently large, and the schedule includes a warmup period and a final decay to near-vanishing learning rate.  Variations among schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different training runs.  Experiments on larger models suggest that the variation in the final test loss between different random seeds is roughly constant in magnitude for different model sizes.  We found that larger models require a smaller learning rate to prevent divergence, while smaller models can tolerate a larger learning rate.  To implement this, the following rule of thumb was used for most runs: \\begin{equation} \\mathrm{LR}(N) \\approx 0.003239 + -0.0001395  \\log(N) \\end{equation} We expect that this formula could be improved.  There may be a dependence on network width, likely set by the initialization scale.  The formula also breaks down for $N>10^{10}$ parameters.  Nevertheless, we found that it works sufficiently well for the models we considered.   "
            },
            {
                "section_name": "Fit Details and Power Law Quality_28",
                "paragraphs": " \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.40\\textwidth]{PoorLogFit} \\caption[Comparison of Power-Law and Logarithmic Fits]{The trend for performance as a function of parameter count, $L(N)$, is fit better by a power law than by other functions such as a logarithm at a qualitative level.  \\label{fig:PoorLogFit}} \\end{figure}  \\begin{figure} \\noindent \\centering{} \\includegraphics[width=0.5\\textwidth]{DepthVsGeneralization} \\caption[Generalization versus depth]{We show evaluations on a series of datasets for models with approximately 1.5 Billion parameters.  We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance. The 12-layer model overfit the Internet Books dataset and we show the early-stopped performance; we have not seen this surprising result in other experiments. \\label{fig:DepthVsGeneralization}} \\end{figure}   We experimented with a number of functional forms for the fits to $L(N), L(C)$, and $L(D)$; the power-law fits were qualitatively much more accurate than other functions such as logarithms (see Figure \\ref{fig:PoorLogFit}).  For $L(C)$, we do not include small models with only 1 layer in the fit, as the transition from 1 to 2 layers causes a noticable lump in the data. For $L(N)$ we also do not include very small models with only 1 layer in the fit, and we exclude the largest models that have not trained fully to convergence.  Fit parameters change marginally if we do include them, and the trend extrapolates well in both directions regardless.    "
            },
            {
                "section_name": "Generalization and Architecture_29",
                "paragraphs": "\\label{sec:DepthVsGeneralization}  In figure \\ref{fig:DepthVsGeneralization} we show that generalization to other data distributions does not depend on network depth when we hold the total parameter count fixed.  It seems to depend only on the performance on the training distribution.  \\listoffigures \\listoftables  \\bibliographystyle{halpha} \\n \\bibliography{bibliography}  \\end{document}           "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2001.08361/SimplePowerLaws.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/EfficiencyIllustration.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ContributionIllustration.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/LossvsModelDatasetSize.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/LearningCurveFitComparisonIntro.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/HyperparameterTuning.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PerfVsModelSizeAllParams.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PerfVsModelSizeNonEmbed.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/LSTMvsTransformerSummary.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/GeneralizationVsModelSize.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/TrainingVsConvergence.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/DatasetModelSizevsPerformance.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/DatasetModelSizevsChangePerformance.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/CriticalBatchSizeVsPerf.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PerfVsParams-ComputeBudget.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PerfVsParams-StepBudget.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/SuboptimalModels.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ComputeEfficientFrontierWithAdjustment.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ComputevsOptimalModelSize.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ComputeEfficientSteps.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/Contradiction.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/EarlyStoppingvsNandD.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/TrainVsTestMedium.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ComputeVsParamswithUTNoReuse.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ComputeVsParamswithUTWithReuse.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/BatchParetoFronts3M.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/BatchParetoFronts85M.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/SampleEfficiencystep_min.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/SampleEfficiencyexamples_min.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/ContextPowerLaw.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/SingleRunTokenTraining.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PerformancevsModelSizevsContext.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/VariousLearningSchedules.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/LearningRateSchedulesvsPerformance.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/PoorLogFit.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/DepthVsGeneralization.png",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table1.tex",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table2.tex",
                "caption": "Fits to $L(N, D)$",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table3.tex",
                "caption": "Fits to $L(N, S)$",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table4.tex",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table5.tex",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2001.08361/table6.tex",
                "description": ""
            }
        ]
    },
    "2203.15556": {
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "tldr": "This work trains a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data, and reaches a state-of-the-art average accuracy, greater than a 7% improvement over Gopher.",
        "full_text": [
            {
                "section_name": "Introduction_1",
                "paragraphs": "Recently a series of \\textit{Large Language Models} (LLMs) have been introduced \\citep{gpt3, jurassic, rae2021gopher, nlg530b, thoppilan2022lamda}, with the largest dense language models now having over 500 billion parameters. These large autoregressive transformers \\citep{vaswani2017attention} have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero-shot, few-shot, and fine-tuning.  The compute and energy cost for training large language models is substantial \\citep{rae2021gopher, thoppilan2022lamda} and rises with increasing model size. In practice, the allocated training compute budget is often known in advance: how many accelerators are available and for how long we want to use them. Since it is typically only feasible to train these large models once, accurately estimating the best model hyperparameters for a given compute budget is critical \\citep{tay2021scale}.   \\citet{kaplan2020scaling} showed that there is a power law relationship between the number of parameters in an autoregressive language model (LM) and its performance. As a result, the field has been training larger and larger models, expecting performance improvements. One notable conclusion in \\citet{kaplan2020scaling} is that large models should not be trained to their lowest possible loss to be compute optimal. Whilst we reach the same conclusion, we estimate that large models should be trained for many more training tokens than recommended by the authors. Specifically, given a $10\\times$ increase computational budget, they suggests that the size of the model should increase $5.5\\times$ while the number of training tokens should only increase 1.8$\\times$. Instead, we find that model size and the number of training tokens should be scaled in equal proportions.  Following \\citet{kaplan2020scaling} and the training setup of GPT-3 \\citep{gpt3}, many of the recently trained large models have been trained for approximately 300 billion tokens (\\autoref{tab:llms}), in line with the approach of predominantly increasing model size when increasing compute.  \\begin{figure*}[t] \\centering \\includegraphics[width=.9\\textwidth, trim=-5cm 0 0 0]{figures/combined_predictions_v9.pdf} \\caption{\\textbf{Overlaid predictions.} We overlay the predictions from our three different approaches, along with projections from \\citet{kaplan2020scaling}. We find that all three methods predict that current large models should be substantially smaller and therefore trained much longer than is currently done. In \\autoref{fig:token_flop}, we show the results with the predicted optimal tokens plotted against the optimal number of parameters for fixed FLOP budgets. \\textbf{\\chinchilla outperforms \\gopher and the other large models (see \\autoref{sec:model_analysis}).} } \\label{fig:combined_predictions} \\end{figure*}    \\begin{table*}[t] \\caption{\\textbf{Current LLMs}. We show five of the current largest dense transformer models, their size, and the number of training tokens. Other than LaMDA \\citep{thoppilan2022lamda}, most models are trained for approximately 300 billion tokens. We introduce \\chinchilla, a substantially smaller model, trained for much longer than 300B tokens. } \\label{tab:llms} \\centering \\begin{tabular}{l r c} \\toprule Model & Size ($\\#$ Parameters) & Training Tokens \\\\ \\midrule LaMDA \\citep{thoppilan2022lamda} & 137 Billion &168 Billion \\\\ GPT-3 \\citep{gpt3} & 175 Billion & 300 Billion \\\\ Jurassic \\citep{jurassic} & 178 Billion & 300 Billion \\\\ \\gopher \\citep{rae2021gopher} & 280 Billion & 300 Billion \\\\ \\mtnlg \\citep{nlg530b} & 530 Billion & 270 Billion\\\\ \\midrule \\chinchilla & 70 Billion & 1.4 Trillion\\\\ \\bottomrule \\end{tabular} \\end{table*}  In this work, we revisit the question: \\textit{Given a fixed FLOPs budget,\\footnote{For example, knowing the number of accelerators and a target training duration.} how should one trade-off model size and the number of training tokens?} To answer this question, we model the final pre-training loss\\footnote{ For simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as we are in the infinite data regime (the number of training tokens is less than the number of tokens in the entire corpus).} $L(N, D)$ as a function of the number of model parameters~$N$, and the number of training tokens,~$D$. Since the computational budget $C$ is a deterministic function $\\text{FLOPs}(N,D)$ of the number of seen training tokens and model parameters, we are interested in minimizing $L$ under the constraint $\\text{FLOPs}(N, D) = C$:  \\begin{equation}\\label{eq:model} N_{opt}(C), D_{opt}(C) = \\argmin_{N, D \\text{ s.t. } \\text{FLOPs}(N, D) = C} L(N, D). \\end{equation}  The functions $N_{opt}(C)$, and $D_{opt}(C)$ describe the optimal allocation of a computational budget $C$. We empirically estimate these functions based on the losses of over \\nummodels models, ranging from under $70$M to over $16$B parameters, and trained on $5$B to over $400$B tokens -- with each model configuration trained for several different training horizons. Our approach leads to considerably different results than that of \\citet{kaplan2020scaling}. We highlight our results in \\autoref{fig:combined_predictions} and how our approaches differ in \\autoref{sec:related_work}.  Based on our estimated compute-optimal frontier, we predict that for the compute budget used to train \\gopher, an optimal model should be 4 times smaller, while being training on 4 times more tokens. We verify this by training a more \\textit{compute-optimal} 70B model, called \\chinchilla, on 1.4 trillion tokens. Not only does \\chinchilla outperform its much larger counterpart, \\gopher, but its reduced model size reduces inference cost considerably and greatly facilitates downstream uses on smaller hardware. The energy cost of a large language model is amortized through its usage for inference an fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the immediate benefits of its improved performance.  "
            },
            {
                "section_name": "Related Work_2",
                "paragraphs": "\\label{sec:related_work} \\paragraph{Large language models.} A variety of large language models have been introduced in the last few years. These include both dense transformer models \\citep{gpt3, jurassic, nlg530b, rae2021gopher, thoppilan2022lamda} and mixture-of-expert (MoE) models \\citep{du2021glam, fedus2021switch, zoph2022designing}. The largest dense transformers have passed 500 billion parameters \\citep{nlg530b}. The drive to train larger and larger models is clear---so far increasing the size of language models has been responsible for improving the state-of-the-art in many language modelling tasks. Nonetheless, large language models face several challenges, including their overwhelming computational requirements (the cost of training and inference increase with model size) \\citep{rae2021gopher, thoppilan2022lamda} and the need for acquiring more high-quality training data. In fact, in this work we find that larger, high quality datasets will play a key role in any further scaling of language models.  \\paragraph{Modelling the scaling behavior.} Understanding the scaling behaviour of language models and their transfer properties has been important in the development of recent large models \\citep{kaplan2020scaling,  hernandez2021scaling}. \\citet{kaplan2020scaling} first showed a predictable relationship between model size and loss over many orders of magnitude. The authors investigate the question of choosing the optimal model size to train for a given compute budget. Similar to us, they address this question by training various models. Our work differs from \\citet{kaplan2020scaling} in several important ways. First, the authors use a fixed number of training tokens and learning rate schedule for all models; this prevents them from modelling the impact of these hyperparameters on the loss. In contrast, we find that setting the learning rate schedule to approximately match the number of training tokens results in the best final loss regardless of model size---see \\autoref{fig:cosine}. For a fixed learning rate cosine schedule to 130B tokens, the intermediate loss estimates (for $D' << 130$B) are therefore overestimates of the loss of a model trained with a schedule length matching $D'$. Using these intermediate losses results in underestimating the effectiveness of training models on less data than 130B tokens, and eventually contributes to the conclusion that model size should increase faster than training data size as compute budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the same rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss frontier (see \\autoref{app:curvature})---in fact, the majority of the models used in our analysis have more than 500 million parameters, in contrast the majority of runs in \\citet{kaplan2020scaling} are significantly smaller---many being less than 100M parameters.  Recently, \\citet{clark2022unified} specifically looked in to the scaling properties of Mixture of Expert language models, showing that the scaling with number of experts diminishes as the model size increases---their approach models the loss as a function of two variables: the model size and the number of experts. However, the analysis is done with a fixed number of training tokens, as in \\citet{kaplan2020scaling}, potentially underestimating the improvements of branching.  \\paragraph{Estimating hyperparameters for large models.} The model size and the number of training tokens are not the only two parameters to chose when selecting a language model and a procedure to train it. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and width-to-depth ratio. In this work, we focus on model size and the number of training steps, and we rely on existing work and provided experimental heuristics to determine the other necessary hyperparameters. \\citet{yang2021tuning} investigates how to choose a variety of these parameters for training an autoregressive transformer, including the learning rate and batch size. \\citet{mccandlish2018empirical} finds only a weak dependence between optimal batch size and model size. \\citet{shallue2018measuring, NEURIPS2019_e0eacd98} suggest that using larger batch-sizes than those we use is possible. \\citet{levine2020depth} investigates the optimal depth-to-width ratio for a variety of standard model sizes. We use slightly less deep models than proposed as this translates to better wall-clock performance on our hardware.  \\paragraph{Improved model architectures.} Recently, various promising alternatives to traditional dense transformers have been proposed. For example, through the use of conditional computation large MoE models like the 1.7 trillion parameter Switch transformer \\citep{fedus2021switch}, the 1.2 Trillion parameter GLaM model \\citep{du2021glam}, and others \\citep{artetxe2021efficient, zoph2022designing} are able to provide a large effective model size despite using relatively fewer training and inference FLOPs. However, for very large models the computational benefits of routed models seems to diminish \\citep{clark2022unified}. An orthogonal approach to improving language models is to augment transformers with explicit retrieval mechanisms, as done by \\citet{borgeaud2021retrieval, guu2020realm, lewisretrieval2020}. This approach effectively increases the number of data tokens seen during training (by a factor of $\\sim10$ in \\citet{borgeaud2021retrieval}). This suggests that the performance of language models may be more dependant on the size of the training data than previously thought.  "
            },
            {
                "section_name": "Estimating the optimal parameter/training tokens allocation_3",
                "paragraphs": "\\label{sec:method} We present three different approaches to answer the question driving our research: \\textit{Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?} In all three cases we start by training a range of models varying both model size and the number of training tokens and use the resulting training curves to fit an empirical estimator of how they should scale. We assume a power-law relationship between compute and model size as done in \\citet{clark2022unified, kaplan2020scaling}, though future work may want to include potential curvature in this relationship for large model sizes. The resulting predictions are similar for all three methods and suggest that parameter count and number of training tokens should be increased equally with more compute\\footnote{We compute FLOPs as described in \\autoref{sec:flops}.}---with proportions reported in \\autoref{tab:comparison}. This is in clear contrast to previous work on this topic and warrants further investigation.  "
            },
            {
                "section_name": "Approach 1: Fix model sizes and vary number of training tokens_1",
                "paragraphs": " In our first approach we vary the number of training steps for a fixed family of models (ranging from 70M to over 10B parameters), training each model for 4 different number of training sequences. From these runs, we are able to directly extract an estimate of the minimum loss achieved for a given number of training FLOPs. Training details for this approach can be found in \\autoref{sec:scaling_details}.  \\begin{figure*}[t] \\centering \\includegraphics[width=.95\\textwidth]{figures/scaling_11.pdf} \\caption{\\textbf{Training curve envelope.} On the \\textbf{left} we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths. From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (\\textbf{center}) for a given compute budget and the optimal number of training tokens (\\textbf{right}). In green, we show projections of optimal model size and training token count based on the number of FLOPs used to train \\gopher ($5.76 \\times 10^{23}$). } \\label{fig:approach1} \\end{figure*}  For each parameter count $N$ we train 4 different models, decaying the learning rate by a factor of 10$\\times$ over a horizon (measured in number of training tokens) that ranges by a factor of $16 \\times$. Then, for each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP count $C$, to the most efficient choice of model size $N$ and number of training tokens $D$ such that $\\text{FLOPs}(N,D) = C$.\\footnote{Note that all selected points are within the last 15\\% of training. This suggests that when training a model over $D$ tokens, we should pick a cosine cycle length that decays $10 \\times$ over approximately $D$ tokens---see further details in \\autoref{sec:cosine_cycle}.} At 1500 logarithmically spaced FLOP values, we find which model size achieves the lowest loss of all models along with the required number of training tokens. Finally, we fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute (see the center and right panels of \\autoref{fig:approach1}), obtaining a relationship $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$. We find that $a=0.50$ and $b=0.50$---as summarized in \\autoref{tab:comparison}. In \\autoref{app:kaplan_comparison}, we show a head-to-head comparison at $10^{21}$ FLOPs, using the model size recommended by our analysis and by the analysis of \\citet{kaplan2020scaling}---using the model size we predict has a clear advantage.  "
            },
            {
                "section_name": "Approach 2: IsoFLOP profiles_2",
                "paragraphs": "In our second approach we vary the model size\\footnote{In approach 2, model size varies up to 16B as opposed to approach 1 where we only used models up to 10B.} for a fixed set of 9 different training FLOP counts\\footnote{The number of training tokens is determined by the model size and training FLOPs.} (ranging from $6 \\times10^{18}$ to  $3 \\times 10^{21}$ FLOPs), and consider the final training loss for each point\\footnote{We set the cosine schedule length to match the number of tokens, which is optimal according to the analysis presented in \\autoref{sec:cosine_cycle}.}. in contrast with Approach 1 that considered points $(N, D, L)$ along the entire training runs. This allows us to directly answer the question: For a given FLOP budget, what is the optimal parameter count?  For each FLOP budget, we plot the final loss (after smoothing) against the parameter count in \\autoref{fig:isoflop} (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see a clear minimum in the loss. We fit a parabola to each IsoFLOPs curve to directly estimate at what model size the minimum loss is achieved (\\autoref{fig:isoflop} (left)). As with the previous approach, we then fit a power law between FLOPs and loss-optimal model size and number of training tokens, shown in \\autoref{fig:isoflop} (center, right). Again, we fit exponents of the form $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$ and we find that $a=0.49$ and $b=0.51$---as summarized in \\autoref{tab:comparison}.  \\begin{figure*}[t] \\centering \\includegraphics[width=.95\\textwidth]{figures/isoflop_7.pdf} \\caption{\\textbf{IsoFLOP curves.} For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (\\textbf{left}). Using the location of these valleys, we project optimal model size and number of tokens for larger models (\\textbf{center} and \\textbf{right}). In green, we show the estimated number of parameters and tokens for an \\textit{optimal} model trained with the compute budget of \\gopher. } \\label{fig:isoflop} \\end{figure*}  "
            },
            {
                "section_name": "Approach 3: Fitting a parametric loss function_3",
                "paragraphs": "Lastly, we model all final losses from experiments in Approach 1 \\& 2 as a parametric function of model parameter count and the number of seen tokens. Following a classical risk decomposition (see \\autoref{sec:approach3}), we propose the following functional form \\begin{equation} \\hat L(N,D) \\triangleq E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}.\\label{eq:decompose} \\end{equation} The first term captures the loss for an ideal generative process on the data distribution, and should correspond to the entropy of natural text. The second term captures the fact that a perfectly trained transformer with $N$ parameters underperforms the ideal generative process. The final term captures the fact that the transformer is not trained to convergence, as we only make a finite number of optimisation steps, on a sample of the dataset distribution.  \\paragraph{Model fitting.} To estimate $(A, B, E, \\alpha, \\beta)$, we minimize the Huber loss \\citep{huber_robust_1964} between the predicted and observed log loss using the L-BFGS algorithm \\citep{nocedal_updating_1980}: \\begin{align} \\min_{A, B, E, \\alpha, \\beta}\\quad &\\sum_{\\text{Runs }i} \\text{Huber}_\\delta \\Big(\\log \\hat L(N_i, D_i) - \\log L_i\\Big) \\label{eq:huber} \\end{align} We account for possible local minima by selecting the best fit from a grid of initialisations. The Huber loss ($\\delta=10^{-3}$) is robust to outliers, which we find important for good predictive performance over held-out data points. \\autoref{app:parametric} details the fitting procedure and the loss decomposition.  \\paragraph{Efficient frontier.} We can approximate the functions $N_{opt}$ and $D_{opt}$ by minimizing the parametric loss $\\hat L$ under the constraint $\\text{FLOPs}(N, D) \\approx 6 N D$ \\citep{kaplan2020scaling}. The resulting $N_{opt}$ and $D_{opt}$ balance the two terms in Equation~\\eqref{eq:huber} that depend on model size and data. By construction, they have a power-law form: \\begin{equation} N_{opt}(C) = G {\\left(\\frac{C}{6}\\right)}^{a}, \\quad D_{opt}(C) = G^{-1} {\\left(\\frac{C}{6}\\right)}^{b}, \\quad \\text{ where }\\quad G = {\\left(\\frac{\\alpha A}{\\beta B} \\right)}^{\\frac{1}{\\alpha + \\beta}},\\quad a = \\frac{\\beta}{\\alpha+\\beta}, \\text{ and } b = \\frac{\\alpha}{\\alpha + \\beta}. \\end{equation} We show contours of the fitted function $\\hat L$ in \\autoref{fig:approach_3} (left), and the closed-form efficient computational frontier in blue. From this approach, we find that $a=0.46$ and $b=0.54$---as summarized in \\autoref{tab:comparison}.  \\begin{figure} \\centering \\includegraphics[width=\\textwidth]{figures/approach_3_v2.pdf} \\caption{\\textbf{Parametric fit.} We fit a parametric modelling of the loss $\\hat L(N,D)$ and display contour (\\textbf{left}) and isoFLOP slices (\\textbf{right}). For each isoFLOP slice, we include a corresponding dashed line in the left plot. In the left plot, we show the efficient frontier in blue, which is a line in log-log space. Specifically, the curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the optimal model size given the \\gopher FLOP budget to be 40B parameters.} \\label{fig:approach_3} \\end{figure}   "
            },
            {
                "section_name": "Optimal model scaling_4",
                "paragraphs": "\\label{sec:scaling-results} We find that the three approaches, despite using different fitting methodologies and different trained models, yield comparable predictions for the optimal scaling in parameters and tokens with FLOPs (shown in \\autoref{tab:comparison}). \\begin{table}[t] \\caption{\\textbf{Estimated parameter and data scaling with increased training compute.} The listed values are the exponents, $a$ and $b$, on the relationship $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$. Our analysis suggests a near equal scaling in parameters and data with increasing compute which is in clear contrast to previous work on the scaling of large models. The 10$^{\\text{th}}$ and 90$^{\\text{th}}$ percentiles are estimated via bootstrapping data (80\\% of the dataset is sampled 100 times) and are shown in parenthesis. } \\centering \\begin{tabular}{lccccc} \\toprule Approach & Coeff. $a$ where $N_{opt} \\propto C^a$ & Coeff. $b$ where $D_{opt} \\propto C^b$ \\\\ \\midrule 1. Minimum over training curves & $0.50 \\; ({0.488}, {0.502})$ & $0.50 \\; (0.501, 0.512)$ \\\\ 2. IsoFLOP profiles & $0.49 \\; (0.462, 0.534)$ &  $0.51 \\; (0.483, 0.529)$ \\\\ 3. Parametric modelling of the loss & $0.46\\; (0.454, 0.455)$ & $0.54  \\; (0.542, 0.543)$ \\\\ \\midrule \\citet{kaplan2020scaling} & 0.73 & 0.27 \\\\ \\bottomrule \\end{tabular} \\label{tab:comparison} \\end{table} All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions. The first and second approaches yield very similar predictions for optimal model sizes, as shown in \\autoref{fig:combined_predictions} and \\autoref{fig:token_flop}. The third approach predicts even smaller models being optimal at larger compute budgets. We note that the observed points $(L, N, D)$ for low training FLOPs ($C\\leq1e21$) have larger residuals ${\\Vert L - \\hat L(N, D) \\Vert}_2^2$ than points with higher computational budgets. The fitted model places increased weight on the points with more FLOPs---automatically considering the low-computational budget points as outliers due to the Huber loss. As a consequence of the empirically observed negative curvature in the frontier $C \\to N_{opt}$ (see \\autoref{app:curvature}), this results in predicting a lower $N_{opt}$ than the two other approaches.  \\begin{table}[t] \\caption{\\textbf{Estimated optimal training FLOPs and training tokens for various model sizes.} For various model sizes, we show the projections from Approach 1 of how many FLOPs and training tokens would be needed to train compute-optimal models. The estimates for Approach 2 \\& 3 are similar (shown in \\autoref{app:estimated_flops_and_tokens_2_and3})}. \\label{tab:compute} \\centering \\begin{tabular}{r rrr rr} \\toprule Parameters & FLOPs & FLOPs (in \\gopher unit) & Tokens \\\\ \\midrule 400 Million & 1.92e+19 & $1/29,968$ & 8.0 Billion \\\\ 1 Billion & 1.21e+20 & $1/4,761$ & 20.2 Billion \\\\ 10 Billion & 1.23e+22 & $1/46$ & 205.1 Billion \\\\ 67 Billion & 5.76e+23 & $1$ & 1.5 Trillion \\\\ 175 Billion & 3.85e+24 & $6.7$ & 3.7 Trillion \\\\ 280 Billion & 9.90e+24 & $17.2$ & 5.9 Trillion \\\\ 520 Billion & 3.43e+25 & $59.5$ & 11.0 Trillion \\\\ 1 Trillion & 1.27e+26 & $221.3$ & 21.2 Trillion \\\\ 10 Trillion & 1.30e+28 & $22515.9$ & 216.2 Trillion \\\\ \\bottomrule \\end{tabular} \\end{table}  In \\autoref{tab:compute} we show the estimated number of FLOPs and tokens that would ensure that a model of a given size lies on the compute-optimal frontier. Our findings suggests that the current generation of large language models are considerably over-sized, given their respective compute budgets, as shown in \\autoref{fig:combined_predictions}. For example, we find that a 175 billion parameter model should be trained with a compute budget of $4.41\\times 10^{24}$ FLOPs and on over 4.2 trillion tokens. A 280 billion \\gopher-like model is the optimal model to train given a compute budget of approximately $10^{25}$ FLOPs and should be trained on 6.8 trillion tokens. Unless one has a compute budget of $10^{26}$ FLOPs (over 250$\\times$ the compute used to train \\gopher), a 1 trillion parameter model is unlikely to be the optimal model to train. Furthermore, the amount of training data that is projected to be needed is far beyond what is currently used to train large models, and underscores the importance of dataset collection in addition to engineering improvements that allow for model scale. While there is significant uncertainty extrapolating out many orders of magnitude, our analysis clearly suggests that given the training compute budget for many current LLMs, smaller models should have been trained on more tokens to achieve the most performant model.  In \\autoref{app:extra_datasets}, we reproduce the IsoFLOP analysis on two additional datasets: C4 \\citep{raffel2019exploring} and GitHub code \\citep{rae2021gopher}. In both cases we reach the similar conclusion that model size and number of training tokens should be scaled in equal proportions.  "
            },
            {
                "section_name": "\\chinchilla_4",
                "paragraphs": "Based on our analysis in \\autoref{sec:method}, the optimal model size for the \\gopher compute budget is somewhere between 40 and 70 billion parameters. We test this hypothesis by training a model on the larger end of this range---70B parameters---for 1.4T tokens, due to both dataset and computational efficiency considerations. In this section we compare this model, which we call \\chinchilla, to \\gopher and other LLMs. Both \\chinchilla and \\gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.  While pre-training a large language model has a considerable compute cost, downstream fine-tuning and inference also make up substantial compute usage \\citep{rae2021gopher}. Due to being $4 \\times$ smaller than \\gopher, both the memory footprint and inference cost of \\chinchilla are also smaller.  "
            },
            {
                "section_name": "Model and training details_5",
                "paragraphs": "\\label{method:models} The full set of hyperparameters used to train \\chinchilla are given in \\autoref{tab:arch}. \\chinchilla uses the same model architecture and training setup as \\gopher with the exception of the differences listed below. \\begin{itemize} \\item We train \\chinchilla on \\massivetext (the same dataset as \\gopher) but use a slightly different subset distribution (shown in \\autoref{tab:data_makeup}) to account for the increased number of training tokens. \\item We use AdamW \\citep{loshchilov2018decoupled} for \\chinchilla rather than Adam \\citep{kingma2014adam} as this improves the language modelling loss and the downstream task performance after finetuning.\\footnote{Interestingly, a model trained with AdamW only passes the training performance of a model trained with Adam around 80\\% of the way through the cosine cycle, though the ending performance is notably better-- see \\autoref{fig:adam}} \\item We train \\chinchilla with a slightly modified SentencePiece~\\citep{kudo2018sentencepiece} tokenizer that does not apply NFKC normalisation. The vocabulary is very similar-- 94.15\\% of tokens are the same as those used for training \\gopher. We find that this particularly helps with the representation of mathematics and chemistry, for example. \\item Whilst the forward and backward pass are computed in \\texttt{bfloat16}, we store a \\texttt{float32} copy of the weights in the distributed optimiser state \\citep{rajbhandari2020zero}. See \\textit{Lessons Learned} from \\citet{rae2021gopher} for additional details. \\end{itemize}   \\begin{table*}[b] \\centering \\begin{tabular}{ccccccc} \\toprule \\textbf{Model} & \\textbf{Layers} & \\textbf{Number Heads} & \\textbf{Key/Value Size} & \\textbf{d\\textsubscript{model}} & \\textbf{Max LR}  & \\textbf{Batch Size} \\\\ \\midrule \\gopher 280B & 80 & 128 & 128 & 16,384 & $4 \\times 10^{-5}$ & 3M $\\rightarrow$ 6M\\\\ \\chinchilla 70B & 80 & 64 & 128 & 8,192 & $1 \\times 10^{-4}$ & 1.5M $\\rightarrow$ 3M\\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{\\chinchilla architecture details.} We list the number of layers, the key/value size, the bottleneck activation size d$_{\\text{model}}$, the maximum learning rate, and the training batch size (\\# tokens). The feed-forward size is always set to $4\\times \\textrm{d}_{\\textrm{model}}$. Note that we double the batch size midway through training for both \\chinchilla and \\gopher.} \\label{tab:arch} \\end{table*}  In \\autoref{app:other_diffs} we show the impact of the various optimiser related changes between \\chinchilla and \\gopher. All models in this analysis have been trained on TPUv3/TPUv4 \\citep{10.1145/3079856.3080246} with JAX \\citep{jax2018github} and Haiku \\citep{haiku2020github}. We include a \\chinchilla model card \\citep{mitchell2019model} in \\autoref{tab:chinchilla-model-card}.  "
            },
            {
                "section_name": "Results_6",
                "paragraphs": "\\label{sec:model_analysis} We perform an extensive evaluation of \\chinchilla, comparing against various large language models. We evaluate on a large subset of the tasks presented in \\citet{rae2021gopher}, shown in \\autoref{tab:task_summary}. As the focus of this work is on optimal model scaling, we included a large representative subset, and introduce a few new evaluations to allow for better comparison to other existing large models. The evaluation details for all tasks are the same as described in \\citet{rae2021gopher}.  \\begin{table} \\centering \\begin{tabular}{l c l} \\toprule & \\# Tasks & Examples \\\\ \\midrule Language Modelling & 20  & {\\small WikiText-103, The Pile: PG-19, arXiv, FreeLaw, $\\ldots$} \\\\ Reading Comprehension & 3 & {\\small RACE-m, RACE-h, LAMBADA} \\\\ Question Answering & 3 & {\\small Natural Questions, TriviaQA, TruthfulQA} \\\\ Common Sense & 5 & {\\small HellaSwag, Winogrande, PIQA, SIQA, BoolQ} \\\\ MMLU & 57 & {\\small High School Chemistry, Astronomy, Clinical Knowledge, $\\ldots$} \\\\ \\bigbench & 62 & {\\small Causal Judgement, Epistemic Reasoning, Temporal Sequences, $\\ldots$}  \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{All evaluation tasks.} We evaluate \\chinchilla on a collection of language modelling along with downstream tasks. We evaluate on largely the same tasks as in \\citet{rae2021gopher}, to allow for direct comparison.} \\label{tab:task_summary} \\end{table}  \\subsubsection{Language modelling} \\begin{figure*}[ht] \\centering \\includegraphics[width=.8\\textwidth]{figures/chinchilla_pile_3.pdf} \\caption{\\textbf{Pile Evaluation.} For the different evaluation sets in The Pile \\citep{pile}, we show the bits-per-byte (bpb) improvement (decrease) of \\chinchilla compared to \\gopher. On all subsets, \\chinchilla outperforms \\gopher. } \\label{fig:pile} \\end{figure*} \\chinchilla significantly outperforms \\gopher on all evaluation subsets of The Pile \\citep{pile}, as shown in \\autoref{fig:pile}. Compared to Jurassic-1 (178B) \\cite{jurassic}, \\chinchilla is more performant on all but two subsets-- \\texttt{dm\\_mathematics} and \\texttt{ubuntu\\_irc}-- see \\autoref{tab:pile_nums} for a raw bits-per-byte comparison. On Wikitext103 \\citep{wikitext103}, \\chinchilla achieves a perplexity of 7.16 compared to 7.75 for \\gopher. Some caution is needed when comparing \\chinchilla with \\gopher on these language modelling benchmarks as \\chinchilla is trained on 4$\\times$ more data than \\gopher and thus train/test set leakage may artificially enhance the results. We thus place more emphasis on other tasks for which leakage is less of a concern, such as MMLU \\citep{hendrycks2020measuring} and \\bigbench \\citep{bigbench} along with various closed-book question answering and common sense analyses.  \\subsubsection{MMLU} \\begin{table}[t] \\centering \\begin{tabular}{lc} \\toprule Random & 25.0\\% \\\\ Average human rater & 34.5\\% \\\\ GPT-3 5-shot & 43.9\\% \\\\ \\gopher 5-shot & 60.0\\% \\\\ \\textbf{\\chinchilla 5-shot} & \\textbf{67.6\\%} \\\\ Average human expert performance & \\textit{89.8\\%} \\\\ \\midrule June 2022 Forecast& 57.1\\% \\\\ June 2023 Forecast& 63.4\\% \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Massive Multitask Language Understanding (MMLU).} We report the average 5-shot accuracy over 57 tasks with model and human accuracy comparisons taken from \\citet{hendrycks2020measuring}. We also include the average prediction for state of the art accuracy in June 2022/2023 made by 73 competitive human forecasters in \\citet{forecast_blog}. } \\label{tab:mmlu} \\end{table} The Massive Multitask Language Understanding (MMLU) benchmark \\citep{hendrycks2020measuring} consists of a range of exam-like questions on academic subjects. In \\autoref{tab:mmlu}, we report \\chinchilla's average 5-shot performance on MMLU (the full breakdown of results is shown in \\autoref{tab:mmlu_nums}). On this benchmark, \\chinchilla significantly outperforms \\gopher despite being much smaller, with an average accuracy of 67.6\\% (improving upon \\gopher  by 7.6\\%). Remarkably, \\chinchilla even outperforms the expert forecast for June 2023 of 63.4\\% accuracy (see \\autoref{tab:mmlu}) \\citep{forecast_blog}. Furthermore, \\chinchilla achieves greater than 90\\% accuracy on 4 different individual tasks-- \\texttt{high\\_school\\_gov\\_and\\_politics, international\\_law, sociology}, and \\texttt{us\\_foreign\\_policy}. To our knowledge, no other model has achieved greater than 90\\% accuracy on a subset.  In \\autoref{fig:mmlu}, we show a comparison to \\gopher broken down by task. Overall, we find that \\chinchilla improves performance on the vast majority of tasks. On four tasks (\\texttt{college\\_mathematics, econometrics, moral\\_scenarios}, and \\texttt{formal\\_logic}) \\chinchilla underperforms \\gopher, and there is no change in performance on two tasks. \\begin{figure*}[ht] \\centering \\includegraphics[width=.9\\textwidth]{figures/mmlu_0.pdf} \\caption{\\textbf{MMLU results compared to \\Gopher} We find that \\chinchilla outperforms \\gopher by 7.6\\% on average (see \\autoref{tab:mmlu}) in addition to performing better on 51/57 individual tasks, the same on 2/57, and worse on only 4/57 tasks. } \\label{fig:mmlu} \\end{figure*}  \\begin{figure*}[t] \\centering \\includegraphics[width=.9\\textwidth]{figures/bigbench_2.pdf} \\caption{\\textbf{\\bigbench results compared to \\Gopher} \\chinchilla out performs \\gopher on all but four \\bigbench tasks considered. Full results are in \\autoref{tab:bigbench}. } \\label{fig:bigbench} \\end{figure*} \\subsubsection{Reading comprehension} On the final word prediction dataset LAMBADA \\citep{paperno2016lambada}, \\chinchilla achieves 77.4\\% accuracy, compared to 74.5\\% accuracy from \\gopher and 76.6\\% from \\mtnlg (see \\autoref{tab:reading}). On RACE-h and RACE-m \\citep{race}, \\chinchilla greatly outperforms \\gopher, improving accuracy by more than 10\\% in both cases---see \\autoref{tab:reading}.  \\begin{table}[t] \\centering \\begin{tabular}{ccccc} \\toprule & \\chinchilla & \\gopher & GPT-3 & \\mtnlg \\\\ \\midrule LAMBADA Zero-Shot & \\textbf{77.4} & 74.5 & 76.2 & 76.6 \\\\ RACE-m Few-Shot & \\textbf{86.8}  & 75.1 & 58.1 & - \\\\ RACE-h Few-Shot & \\textbf{82.3} & 71.6 & 46.8 & 47.9\\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Reading comprehension.} On RACE-h and RACE-m \\citep{race}, \\chinchilla considerably improves performance over \\gopher. Note that GPT-3 and \\mtnlg use a different prompt format than we do on RACE-h/m, so results are not comparable to \\gopher and \\chinchilla. On LAMBADA \\citep{paperno2016lambada}, \\chinchilla outperforms both \\gopher and \\mtnlg.} \\label{tab:reading} \\end{table} \\subsubsection{\\bigbench} We analysed \\chinchilla on the same set of \\bigbench tasks \\citep{bigbench} reported in \\citet{rae2021gopher}. Similar to what we observed in MMLU, \\chinchilla outperforms \\gopher on the vast majority of tasks (see \\autoref{fig:bigbench}). We find that \\chinchilla improves the average performance by 10.7\\%, reaching an accuracy of 65.1\\% versus 54.4\\% for \\gopher. Of the 62 tasks we consider, \\chinchilla performs worse than \\gopher on only four---\\texttt{crash\\_blossom, dark\\_humor\\_detection, mathematical\\_induction} and \\texttt{logical\\_args}. Full accuracy results for \\chinchilla can be found in \\autoref{tab:bigbench}.  \\subsubsection{Common sense} \\begin{table}[hb] \\centering \\begin{tabular}{cccccc} \\toprule & \\chinchilla & \\gopher & GPT-3 & \\mtnlg & Supervised SOTA \\\\ \\midrule HellaSWAG & \\textbf{80.8\\%} & 79.2\\% & 78.9\\% & 80.2\\% & 93.9\\% \\\\ PIQA & 81.8\\% & 81.8\\% & 81.0\\% & \\textbf{82.0\\%} & 90.1\\% \\\\ Winogrande & \\textbf{74.9\\%} & 70.1\\% & 70.2\\% & 73.0\\% & 91.3\\% \\\\ SIQA & \\textbf{51.3\\%} & 50.6\\% & - & - & 83.2\\% \\\\ BoolQ & \\textbf{83.7}\\% & 79.3\\% & 60.5\\% & 78.2\\%& 91.4\\% \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Zero-shot comparison on Common Sense benchmarks.} We show a comparison between \\chinchilla, \\gopher, and \\mtnlg on various Common Sense benchmarks. We see that \\chinchilla matches or outperforms \\gopher and GPT-3 on all tasks. On all but one \\chinchilla outperforms the much larger \\mtnlg model. } \\label{tab:commonsense} \\end{table} We evaluate \\chinchilla on various common sense benchmarks: PIQA \\citep{piqa}, SIQA \\citep{socialiqa}, Winogrande \\citep{winogrande}, HellaSwag \\citep{hellaswag}, and BoolQ \\citep{clark2019boolq}. We find that \\chinchilla outperforms both \\gopher and GPT-3 on all tasks and outperforms \\mtnlg on all but one task---see \\autoref{tab:commonsense}.  On TruthfulQA \\citep{truthfulqa}, \\chinchilla reaches 43.6\\%, 58.5\\%, and 66.7\\% accuracy with 0-shot, 5-shot, and 10-shot respectively. In comparison, \\gopher achieved only 29.5\\% 0-shot and 43.7\\% 10-shot accuracy. In stark contrast with the findings of \\citet{truthfulqa}, the large improvements (14.1\\% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training data alone can lead to substantial improvements on this benchmark.  \\subsubsection{Closed-book question answering} Results on closed-book question answering benchmarks are reported in \\autoref{tab:QA}. On the Natural Questions dataset \\citep{naturalquestions}, \\chinchilla achieves new closed-book SOTA accuracies: 31.5\\% 5-shot and 35.5\\% 64-shot, compared to 21\\% and 28\\% respectively, for \\gopher. On TriviaQA \\citep{triviaqa} we show results for both the filtered (previously used in retrieval and open-book work) and unfiltered set (previously used in large language model evaluations). In both cases, \\chinchilla substantially out performs \\gopher. On the filtered version, Chinchilla lags behind the open book SOTA \\citep{izacard2020distilling} by only 7.9\\%. On the unfiltered set, \\chinchilla outperforms GPT-3---see \\autoref{tab:QA}.  \\begin{table}[t] \\centering \\begin{tabular}{c c c c c c c} \\toprule & Method & \\chinchilla & \\gopher & GPT-3 & SOTA (open book) \\\\ \\midrule \\multirow{3}{*}{Natural Questions (dev)} & 0-shot & 16.6\\% &  10.1\\% &  14.6\\%  & \\multirow{3}{*}{54.4\\%} \\\\ & 5-shot & 31.5\\% & 24.5\\% & -  & \\\\ & 64-shot & 35.5\\% & 28.2\\% & 29.9\\% &  \\\\ \\midrule \\multirow{3}{*}{TriviaQA (unfiltered, test)} & 0-shot & 67.0\\% & 52.8\\% & 64.3 \\% & \\multirow{3}{*}{-} \\\\ & 5-shot & 73.2\\% & 63.6\\%  &  - &   \\\\ & 64-shot & 72.3\\% & 61.3\\% & 71.2\\% & \\\\ \\midrule \\multirow{3}{*}{TriviaQA (filtered, dev)} & 0-shot & 55.4\\% & 43.5\\% & - & \\multirow{3}{*}{72.5\\%} \\\\ & 5-shot &  64.1\\% &  57.0\\% &  - &   \\\\ & 64-shot & 64.6\\% & 57.2\\% & - & \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Closed-book question answering.} For Natural Questions \\citep{naturalquestions} and TriviaQA \\citep{triviaqa}, \\chinchilla outperforms \\gopher in all cases. On Natural Questions, \\chinchilla outperforms GPT-3. On TriviaQA we show results on two different evaluation sets to allow for comparison to GPT-3 and to open book SOTA (FiD + Distillation \\citep{izacard2020distilling}). } \\label{tab:QA} \\end{table}   \\subsubsection{Gender bias and toxicity} Large Language Models carry potential risks such as outputting offensive language, propagating social biases, and leaking private information \\citep{weidinger2021harms,bender2021dangers}. We expect \\chinchilla to carry risks similar to \\gopher because \\chinchilla is trained on the same data, albeit with slightly different relative weights, and because it has a similar architecture. Here, we examine gender bias~(particularly gender and occupation bias) and generation of toxic language. We select a few common evaluations to highlight potential issues, but stress that our evaluations are not comprehensive and much work remains to understand, evaluate, and mitigate risks in LLMs.   \\paragraph{Gender bias.} As discussed in \\citet{rae2021gopher}, large language models reflect contemporary and historical discourse about different groups (such as gender groups) from their training dataset, and we expect the same to be true for \\chinchilla. Here, we test if potential gender and occupation biases manifest in unfair outcomes on coreference resolutions, using the Winogender dataset \\citep{rudinger2018gender} in a zero-shot setting. Winogender tests whether a model can correctly determine if a pronoun refers to different occupation words. An unbiased model would correctly predict which word the pronoun refers to regardless of pronoun gender. We follow the same setup as in \\citet{rae2021gopher} (described further in \\autoref{appendix-winogender}).   As shown in \\autoref{tab:fairness}, \\chinchilla correctly resolves pronouns more frequently than \\gopher across all groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase of 3.2\\%) than for female or neutral pronouns (increases of 8.3\\% and 9.2\\% respectively). We also consider \\textit{gotcha} examples, in which the correct pronoun resolution contradicts gender stereotypes (determined by labor statistics).  Again, we see that \\chinchilla resolves pronouns more accurately than \\gopher.  When breaking up examples by male/female gender and \\textit{gotcha}/\\textit{not gotcha}, the largest improvement is on female \\textit{gotcha} examples (improvement of 10\\%). Thus, though \\chinchilla uniformly overcomes gender stereotypes for more coreference examples than \\gopher, the rate of improvement is higher for some pronouns than others, suggesting that the improvements conferred by using a more compute-optimal model can be uneven.   \\begin{table}[t] \\begin{subtable}[h]{0.45\\textwidth} \\centering \\begin{tabular}{l | l | l} \\toprule & \\chinchilla & \\gopher \\\\ \\midrule All & 78.3\\% & 71.4\\% \\\\ Male & 71.2\\% & 68.0\\% \\\\ Female & 79.6\\% & 71.3\\% \\\\ Neutral & 84.2\\% & 75.0\\% \\\\ \\bottomrule \\end{tabular} \\label{tab:fairness1} \\end{subtable} \\hfill \\begin{subtable}[h]{0.5\\textwidth} \\centering \\begin{tabular}{l | l | l} \\toprule & \\chinchilla & \\gopher \\\\ \\midrule Male \\textit{gotcha} & 62.5\\% & 59.2\\% \\\\ Male \\textit{not gotcha} & 80.0\\% & 76.7\\% \\\\ Female \\textit{gotcha} & 76.7\\% & 66.7\\% \\\\ Female \\textit{not gotcha} & 82.5\\% & 75.8\\% \\\\ \\bottomrule \\end{tabular} \\label{tab:fairness2} \\end{subtable} \\caption{\\textbf{Winogender results.} \\textbf{Left:} \\chinchilla consistently resolves pronouns better than \\gopher.  \\textbf{Right:} \\chinchilla performs better on examples which contradict gender stereotypes (\\textit{gotcha} examples).  However, difference in performance across groups suggests \\chinchilla exhibits bias.} \\label{tab:fairness} \\end{table}   \\paragraph{Sample toxicity.} Language models are capable of generating toxic language---including insults, hate speech, profanities and threats \\citep{gehman2020realtoxicityprompts,rae2021gopher}.  While toxicity is an umbrella term, and its evaluation in LMs comes with challenges \\citep{xu2021detoxifying,welbl2021challenges}, automatic classifier scores can provide an indication for the levels of harmful text that a LM generates. \\citet{rae2021gopher} found that improving language modelling loss by increasing the number of model parameters has only a negligible effect on toxic text generation (unprompted); here we analyze whether the same holds true for a lower LM loss achieved via more compute-optimal training. Similar to the protocol of \\citet{rae2021gopher}, we generate 25,000 unprompted samples from \\chinchilla, and compare their \\textit{PerspectiveAPI} toxicity score distribution to that of \\gopher-generated samples. Several summary statistics indicate an absence of major differences: the mean (median) toxicity score for \\gopher is 0.081 (0.064), compared to 0.087 (0.066) for \\chinchilla, and the $95^{\\textrm{th}}$ percentile scores are 0.230 for \\gopher, compared to 0.238 for \\chinchilla.  That is, the large majority of generated samples are classified as non-toxic, and the difference between the models is negligible.  In line with prior findings \\citep{rae2021gopher}, this suggests that toxicity levels in unconditional text generation are largely independent of the model quality (measured in language modelling loss), i.e.~that better models of the training dataset are not necessarily more toxic.  "
            },
            {
                "section_name": "Discussion \\& Conclusion_5",
                "paragraphs": "The trend so far in large language model training has been to increase the model size, often without increasing the number of training tokens. The largest dense transformer, \\mtnlg, is now over $3 \\times$ larger than GPT-3's 170 billion parameters from just two years ago. However, this model, as well as the majority of existing large models, have all been trained for a comparable number of tokens---around 300 billion. While the desire to train these mega-models has led to substantial engineering innovation, we hypothesize that the race to train larger and larger models is resulting in models that are substantially underperforming compared to what could be achieved with the same compute budget.  We propose three predictive approaches towards optimally setting model size and training duration, based on the outcome of over \\nummodels training runs. All three approaches predict that \\gopher is substantially over-sized and estimate that for the same compute budget a smaller model trained on more data will perform better. We directly test this hypothesis by training \\chinchilla, a 70B parameter model, and show that it outperforms \\gopher and even larger models on nearly every measured evaluation task.  Whilst our method allows us to make predictions on how to scale large models when given additional compute, there are several limitations. Due to the cost of training large models, we only have two comparable training runs at large scale (\\chinchilla and \\gopher), and we do not have additional tests at intermediate scales. Furthermore, we assume that the efficient computational frontier can be described by a power-law relationship between the compute budget, model size, and number of training tokens. However, we observe some concavity in $\\log \\left (N_{opt} \\right)$ at high compute budgets (see \\autoref{app:curvature}). This suggests that we may still be overestimating the optimal size of large models. Finally, the training runs for our analysis have all been trained on less than an epoch of data; future work may consider the multiple epoch regime. Despite these limitations, the comparison of \\chinchilla to \\gopher validates our performance predictions, that have thus enabled training a better (and more lightweight) model at the same compute budget.   Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require extra care to ensure train-test set overlap is properly accounted for, both in the language modelling loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and private information. With even larger datasets being used, the quantity (if not the frequency) of such information increases, which makes dataset introspection all the more important. \\chinchilla does suffer from bias and toxicity but interestingly it seems less affected than \\gopher. Better understanding how performance of large language models and toxicity interact is an important future research question.   While we have applied our methodology towards the training of auto-regressive language models, we expect that there is a similar trade-off between model size and the amount of data in other modalities. As training large models is very expensive, choosing the optimal model size and training steps beforehand is essential. The methods we propose are easy to reproduce in new settings.  "
            },
            {
                "section_name": "Acknowledgements_6",
                "paragraphs": "We'd like to thank Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis, Geoffrey Irving, Koray Kavukcuoglu, Nate Kushman and Angeliki Lazaridou for useful comments on the manuscript. We'd like to thank Andy Brock, Irina Higgins, Michela Paganini, Francis Song, and other colleagues at DeepMind for helpful discussions. We are also very grateful to the JAX and XLA team for their support and assistance.  \\bibliography{main}  \\newpage \\appendix \\setcounter{figure}{0} \\makeatletter \\renewcommand{\\thefigure}{A\\@arabic\\c@figure} \\makeatother  \\setcounter{table}{0} \\makeatletter \\renewcommand{\\thetable}{A\\@arabic\\c@table} \\makeatother  \\section*{\\centering \\Huge{Appendix}} \\n "
            },
            {
                "section_name": "Training dataset_7",
                "paragraphs": "In \\autoref{tab:data_makeup} we show the training dataset makeup used for \\chinchilla and all scaling runs. Note that both the \\massiveweb and Wikipedia subsets are both used for more than one epoch.  \\begin{table*}[h!] \\centering \\begin{tabular}{l r r c c} \\toprule & Disk Size & Documents  & Sampling proportion & Epochs in 1.4T tokens  \\\\ \\midrule \\massiveweb & 1.9 TB  & 604M  & 45\\% (48\\%) & 1.24\\\\ Books       & 2.1 TB  & 4M   & 30\\% (27\\%) & 0.75 \\\\ C4          & 0.75 TB  & 361M  & 10\\% (10\\%) & 0.77 \\\\ News        & 2.7 TB  & 1.1B  & 10\\% (10\\%)& 0.21\\\\ GitHub      & 3.1 TB  & 142M & 4\\% (3\\%) & 0.13 \\\\ Wikipedia   & 0.001 TB & 6M    & 1\\% (2\\%) & 3.40 \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{\\massivetext data makeup.} For each subset of \\massivetext, we list its total disk size, the number of documents and the sampling proportion used during training---we use a slightly different distribution than in \\citet{rae2021gopher} (shown in parenthesis). In the rightmost column show the number of epochs that are used in 1.4 trillion tokens. } \\label{tab:data_makeup} \\end{table*}  "
            },
            {
                "section_name": "Optimal cosine cycle length_8",
                "paragraphs": "\\label{sec:cosine_cycle} One key assumption is made on the cosine cycle length and the corresponding learning rate drop (we use a 10$\\times$ learning rate decay in line with \\citet{rae2021gopher}).\\footnote{We find the difference between decaying by $10\\times$ and decaying to 0.0 (over the same number of steps) to be small, though decaying by a factor of $10\\times$ to be slightly more performant. Decaying by less ($5 \\times$) is clearly worse.} We find that setting the cosine cycle length too much longer than the target number of training steps results in sub-optimally trained models, as shown in \\autoref{fig:cosine}. As a result, we assume that an optimally trained model will have the cosine cycle length correctly calibrated to the maximum number of steps, given the FLOP budget; we follow this rule in our main analysis. \\begin{figure*}[h!] \\centering \\includegraphics[width=.9\\textwidth]{figures/cosine_v2.pdf} \\caption{\\textbf{Grid over cosine cycle length.} We show 6 curves with the cosine cycle length set to 1, 1.1, 1.25, 1.5, 2, and 5$\\times$ longer than the target number of training steps. When the cosine cycle length is too long, and the learning rate does not drop appropriately, then performance is impaired. We find that overestimating the number of training steps beyond 25\\% leads to clear drops in performance. We show results where we have set the number of training steps to two different values (top and bottom). } \\label{fig:cosine} \\end{figure*}  "
            },
            {
                "section_name": "Consistency of scaling results across datasets_9",
                "paragraphs": "\\label{app:extra_datasets} \\begin{figure*}[h!] \\centering \\includegraphics[width=.9\\textwidth]{figures/gh_c4_2.pdf} \\caption{\\textbf{C4 and GitHub IsoFLOP curves.} Using the C4 dataset \\citep{raffel2020exploring} and a GitHub dataset \\citep{rae2021gopher}, we generate 4 IsoFLOP profiles and show the parameter and token count scaling, as in \\autoref{fig:isoflop}. Scaling coefficients are shown in \\autoref{tab:comparison_c4_github}. } \\label{fig:c4_only} \\end{figure*} We show scaling results from an IsoFLOP (Approach 2) analysis after training on two different datasets: C4 \\citep{raffel2020exploring} and GitHub code (we show results with data from \\citet{rae2021gopher}), results are shown in \\autoref{tab:comparison_c4_github}. For both set of experiments using subsets of \\massivetext, we use the same tokenizer as the \\massivetext experiments.  We find that the scaling behaviour on these datasets is very similar to what we found on \\massivetext, as shown in \\autoref{fig:c4_only} and \\autoref{tab:comparison_c4_github}. This suggests that our results are independent of the dataset as long as one does not train for more than one epoch.  \\begin{table}[h!] \\centering \\begin{tabular}{lccccc} \\toprule Approach & Coef. $a$ where $N_{opt} \\propto C^a$ & Coef. $b$ where $D_{opt} \\propto C^b$ \\\\ \\midrule C4  & 0.50 & 0.50 \\\\ GitHub & 0.53 & 0.47 \\\\ \\midrule \\citet{kaplan2020scaling} & 0.73 & 0.27 \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Estimated parameter and data scaling with increased training compute on two alternate datasets.} The listed values are the exponents, $a$ and $b$, on the relationship $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$. Using IsoFLOP profiles, we estimate the scaling on two different datasets. } \\label{tab:comparison_c4_github} \\end{table}  "
            },
            {
                "section_name": "Details on the scaling analyses_10",
                "paragraphs": "\\label{sec:scaling_details} "
            },
            {
                "section_name": "Approach 1: Fixing model sizes and varying training sequences_7",
                "paragraphs": "We use a maximum learning rate of $2 \\times 10^{-4}$ for the smallest models and $1.25 \\times 10^{-4}$ for the largest models. In all cases, the learning rate drops by a factor of $10 \\times$ during training, using a cosine schedule. We make the assumption that the cosine cycle length should be approximately matched to the number of training steps. We find that when the cosine cycle overshoots the number of training steps by more than 25\\%, performance is noticeably degraded---see \\autoref{fig:cosine}.\\footnote{This further emphasises the point of not only determining model size, but also training length before training begins.} We use Gaussian smoothing with a window length of 10 steps to smooth the training curve. "
            },
            {
                "section_name": "Approach 3: Parametric fitting of the loss_8",
                "paragraphs": "\\label{sec:approach3} In this section, we first show how Equation~\\eqref{eq:decompose} can be derived. We repeat the equation below for clarity, \\begin{equation} \\hat L(N,D) \\triangleq E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}, \\end{equation} based on a decomposition of the expected risk between a function approximation term and an optimisation suboptimality term. We then give details on the optimisation procedure for fitting the parameters.  \\paragraph{Loss decomposition.} Formally, we consider the task of predicting the next token $y \\in  \\Yy$ based on the previous tokens in a sequence $x \\in  \\Yy^s$, with $s$ varying from $0$ to $s_{\\max}$---the maximum sequence length. We consider a distribution $P \\in \\Dd(\\Xx \\times \\Yy)$ of tokens in $\\Yy$ and their past in $\\Xx$. A predictor $f: \\Xx \\to \\Dd(\\Yy)$ computes the probability of each token given the past sequence. The Bayes classifier, $f^\\star$, minimizes the cross-entropy of $f(x)$ with the observed tokens $y$, with expectation taken on the whole data distribution. We let $L$ be the expected risk \\begin{equation} L(f) \\triangleq \\EE [\\log f(x)_y],\\qquad \\text{and set}\\qquad f^\\star \\triangleq \\argmin_{f \\in \\Ff(\\Xx,\\Dd(\\Yy))} L(f). \\end{equation}  The set of all transformers of size $N$, that we denote $\\Hh_N$, forms a subset of all functions that map sequences to distributions of tokens $\\Xx \\to \\Dd(\\Yy)$. Fitting a transformer of size $N$ on the expected risk $L(f)$ amounts to minimizing such risk on a restricted functional space \\begin{equation} f_N \\triangleq \\argmin_{f \\in \\Hh_N} L(f). \\end{equation}  When we observe a dataset ${(x_i,y_i)_i}_{i \\in [1,D]}$ of size $D$, we do not have access to $\\EE_P$, but instead to the empirical expectation $\\hat \\EE_{D}$ over the empirical distribution $\\hat P_D$. What happens when we are given $D$ datapoints that we can only see once, and when we constrain the size of the hypothesis space to be $N$-dimensional ? We are making steps toward minimizing the empirical risk within a finite-dimensional functional space $\\Hh_N$:  \\begin{equation} \\hat L_D(f) \\triangleq \\hat \\EE_D [\\log f(x)_y],\\qquad\\text{setting}\\qquad \\hat f_{N,D}\\triangleq \\argmin_{f \\in \\Hh_N} \\hat L_D(f). \\end{equation}  We are never able to obtain $\\hat f_{N,D}$ as we typically perform a single epoch over the dataset of size $D$. Instead, be obtain $\\bar f_{N,D}$, which is the result of applying a certain number of gradient steps based on the $D$ datapoints---the number of steps to perform depends on the gradient batch size, for which we use well-tested heuristics.  Using the Bayes-classifier $f^\\star$, the expected-risk minimizer $f_N$ and the ``single-epoch empirical-risk minimizer'' $\\bar f_{N,D}$, we can finally decompose the loss $L(N,D)$ into  \\begin{equation}\\label{eq:decompose_2} L(N,D) \\triangleq L(\\bar f_{N,D}) = L(f^\\star) + \\left( L(f_N) - L(f^\\star) \\right) + \\left( L(\\bar f_{N,D}) - L(f_N) \\right). \\end{equation}  The loss comprises three terms: the Bayes risk, i.e. the minimal loss achievable for next-token prediction on the full distribution $P$, a.k.a the ``entropy of natural text.''; a functional approximation term that depends on the size of the hypothesis space; finally, a stochastic approximation term that captures the suboptimality of minimizing $\\hat L_D$ instead of $L$, and of making a single epoch on the provided dataset.  \\paragraph{Expected forms of the loss terms.} In the decomposition~\\eqref{eq:decompose_2}, the second term depends entirely on the number of parameters $N$ that defines the size of the functional approximation space. \\textit{On the set of two-layer neural networks}, it is expected to be proportional to $\\frac{1}{N^{1/2}}$ \\citep{siegel_approximation_2020}. Finally, given that it corresponds to early stopping in stochastic first order methods, the third term should scale as the convergence rate of these methods, which is lower-bounded by $\\frac{1}{D^{1/2}}$ \\citep{robbins_stochastic_1951}  (and may attain the bound). This convergence rate is expected to be dimension free \\cite[see e.g.][for a review]{bubeck_convex_2015} and depends only on the loss smoothness; hence we assume that the second term only depends on $D$ in \\eqref{eq:decompose}.  Empirically, we find after fitting \\eqref{eq:decompose} that \\begin{equation} L(N, D) = E + \\frac{A}{N^{0.34}} + \\frac{B}{D^{0.28}}, \\end{equation} with $E=1.69$, $A=406.4$, $B=410.7$. We note that the parameter/data coefficients are both lower than $\\frac{1}{2}$; this is expected for the data-efficiency coefficient (but far from the known lower-bound). Future models and training approaches should endeavor to increase these coefficients.        \\paragraph{Fitting the decomposition to data.} We effectively minimize the following problem \\begin{equation} \\min_{a, b, e, \\alpha, \\beta}  \\sum_{\\text{Run }i} \\text{Huber}_\\delta \\Big(\\text{LSE}\\big(a - \\alpha \\log N_i, b- \\beta \\log D_i, e \\big) - \\log L_i\\Big),\\label{eq:lse} \\end{equation} where $LSE$ is the log-sum-exp operator. We then set $A, B, E = \\exp(a), \\exp(b), \\exp(e)$.  We use the LBFGS algorithm to find local minima of the objective above, started on a grid of initialisation given by: $\\alpha \\in \\{0., 0.5,\\dots, 2. \\}$, $\\beta \\in \\{ 0., 0.5,\\dots, 2.\\}$, $e \\in \\{-1., -.5, \\dots, 1. \\}$, $a \\in \\{0, 5, \\dots, 25 \\}$, and $b \\in \\{0, 5, \\dots, 25 \\}$. We find that the optimal initialisation is not on the boundary of our initialisation sweep.  We use $\\delta = 10^{-3}$ for the Huber loss. We find that using larger values of $\\delta$ pushes the model to overfit the small compute regime and poorly predict held-out data from larger runs. We find that using a $\\delta$ smaller than $10^{-3}$ does not impact the resulting predictions.  "
            },
            {
                "section_name": "Predicted compute optimal frontier for all three methods_9",
                "paragraphs": "For Approaches 2 and 3, we show the estimated model size and number of training tokens for a variety of compute budgets in \\autoref{tab:compute23}. We plot the predicted number of tokens and parameters for a variety of FLOP budgets for the three methods in \\autoref{fig:token_flop}. \\begin{table}[h!] \\centering \\begin{tabular}{r | rr | rr} \\toprule & \\multicolumn{2}{c}{Approach 2}& \\multicolumn{2}{c}{Approach 3} \\\\ \\midrule Parameters & FLOPs & Tokens &  FLOPs & Tokens \\\\ \\midrule 400 Million & 1.84e+19 & 7.7 Billion & 2.21e+19 & 9.2 Billion \\\\ 1 Billion & 1.20e+20 & 20.0 Billion & 1.62e+20 & 27.1 Billion \\\\ 10 Billion & 1.32e+22 & 219.5 Billion & 2.46e+22 & 410.1 Billion \\\\ 67 Billion & 6.88e+23  & 1.7 Trillion & 1.71e+24  & 4.1 Trillion \\\\ 175 Billion & 4.54e+24 & 4.3 Trillion & 1.26e+24 & 12.0 Trillion \\\\ 280 Billion & 1.18e+25  & 7.1 Trillion & 3.52e+25  & 20.1 Trillion \\\\ 520 Billion & 4.19e+25 & 13.4 Trillion & 1.36e+26 & 43.5 Trillion \\\\ 1 Trillion & 1.59e+26 & 26.5 Trillion & 5.65e+26 & 94.1 Trillion \\\\ 10 Trillion & 1.75e+28 & 292.0 Trillion & 8.55e+28 & 1425.5 Trillion \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Estimated optimal training FLOPs and training tokens for various model sizes.} Analogous to \\autoref{tab:compute}, we show the model size/token count projections from Approaches 2 and 3 for various compute budgets.}. \\label{tab:compute23} \\end{table} \\begin{figure*}[h!] \\centering \\includegraphics[width=.5\\textwidth]{figures/tokens_vs_params4.pdf} \\caption{\\textbf{Optimal number of tokens and parameters for a training FLOP budget.} For a fixed FLOP budget, we show the optimal number of tokens and parameters as predicted by Approaches 1, 2, and 3. For an alternate representation, see \\autoref{fig:combined_predictions}. } \\label{fig:token_flop} \\end{figure*}  "
            },
            {
                "section_name": "Small-scale comparison to Kaplan \\textit{et al._10",
                "paragraphs": "\\label{app:kaplan_comparison} For $10^{21}$ FLOPs, we perform a head-to-head comparison of a model predicted by Approach 1 and that predicted by \\citet{kaplan2020scaling}. For both models, we use a batch size of 0.5M tokens and a maximum learning rate of $1.5 \\times 10^{-4}$ that decays by $10 \\times$. From \\citet{kaplan2020scaling}, we find that the optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86 billion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion parameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many confounding factors as possible. We find that our predicted model outperforms the model predicted by \\citet{kaplan2020scaling} as shown in \\autoref{fig:kaplan_comparison}. \\begin{figure*}[h!] \\centering \\includegraphics[width=.9\\textwidth]{figures/approach_1_kaplan2.pdf} \\caption{\\textbf{Comparison to \\citet{kaplan2020scaling} at $10^{21}$ FLOPs.} We train 2.80 and 4.74 billion parameter transformers predicted as optimal for $10^{21}$ FLOPs by Approach 1 and by \\citet{kaplan2020scaling}. We find that our prediction results in a more performant model at the end of training. } \\label{fig:kaplan_comparison} \\end{figure*}  "
            },
            {
                "section_name": "Curvature of the FLOP-loss frontier_11",
                "paragraphs": "\\label{app:curvature} We observe that as models increase there is a curvature in the FLOP-minimal loss frontier. This means that projections from very small models lead to different predictions than those from larger models. In \\autoref{fig:curvature} we show linear fits using the first, middle, and final third of frontier-points. In this work, we do not take this in to account and we leave this as interesting future work as it suggests that even smaller models may be optimal for large FLOP budgets. \\begin{figure*}[h!] \\centering \\includegraphics[width=.5\\textwidth]{figures/curvature_v3.pdf} \\caption{\\textbf{Training curve envelopes.} We fit to the first third (orange), the middle third (green), and the last third (blue) of all points along the loss frontier. We plot only a subset of the points. } \\label{fig:curvature} \\end{figure*}  "
            },
            {
                "section_name": "FLOPs computation_12",
                "paragraphs": "\\label{sec:flops} We include all training FLOPs, including those contributed to by the embedding matrices, in our analysis. Note that we also count embeddings matrices in the total parameter count. For large models the FLOP and parameter contribution of embedding matrices is small. We use a factor of 2 to describe the multiply accumulate cost. For the forward pass, we consider contributions from: \\begin{itemize} \\item Embeddings \\begin{itemize} \\item $2 \\times \\text{seq\\_len} \\times \\text{vocab\\_size} \\times \\text{d\\_model} $ \\end{itemize} \\item Attention (Single Layer) \\begin{itemize} \\item \\textbf{Key, query and value projections}: $2 \\times 3 \\times \\text{seq\\_len} \\times \\text{d\\_model} \\times ( \\text{key\\_size} \\times \\text{num\\_heads})$ \\item \\textbf{Key @ Query logits}: $2  \\times \\text{seq\\_len} \\times \\text{seq\\_len} \\times ( \\text{key\\_size} \\times \\text{num\\_heads}) $ \\item \\textbf{Softmax}: $3 \\times \\text{num\\_heads}\\times \\text{seq\\_len} \\times \\text{seq\\_len}  $ \\item \\textbf{Softmax @ query reductions}: $2  \\times \\text{seq\\_len} \\times \\text{seq\\_len} \\times ( \\text{key\\_size} \\times \\text{num\\_heads}) $ \\item \\textbf{Final Linear}: $2 \\times \\text{seq\\_len} \\times (\\text{key\\_size} \\times \\text{num\\_heads}) \\times \\text{d\\_model} $ \\end{itemize} \\item Dense Block (Single Layer) \\begin{itemize} \\item $2 \\times \\text{seq\\_len} \\times (\\text{d\\_model} \\times \\text{ffw\\_size} +\\text{d\\_model} \\times \\text{ffw\\_size})$ \\end{itemize} \\item Final Logits \\begin{itemize} \\item $2 \\times \\text{seq\\_len} \\times \\text{d\\_model} \\times \\text{vocab\\_size}$ \\end{itemize} \\item \\textbf{Total forward pass FLOPs:} $\\text{embeddings} + \\text{num\\_layers} \\times (\\text{total\\_attention} + \\text{dense\\_block})$ + \\text{logits} \\end{itemize} As in \\citet{kaplan2020scaling} we assume that the backward pass has twice the FLOPs of the forward pass. We show a comparison between our calculation and that using the common approximation $C = 6 D N$ \\citep{kaplan2020scaling} where $C$ is FLOPs, $D$ is the number of training tokens, and $N$ is the number of parameters in \\autoref{tab:flops}. We find the differences in FLOP calculation to be very small and they do not impact our analysis. \\begin{table*}[h!] \\centering \\begin{tabular}{c c c c c c | c} \\toprule Parameters & num\\_layers & d\\_model & ffw\\_size & num\\_heads & k/q size & FLOP Ratio (Ours/$6ND$)\\\\ \\midrule 73M & 10 & 640 & 2560 & 10 & 64 & 1.03\\\\ 305M & 20 & 1024 & 4096 & 16 & 64 & 1.10\\\\ 552M & 24 & 1280 & 5120 & 10 & 128 & 1.08\\\\ 1.1B & 26 & 1792 & 7168 & 14 & 128 & 1.04\\\\ 1.6B & 28 & 2048 & 8192 & 16 & 128 & 1.03\\\\ 6.8B & 40 & 3584 & 14336 & 28 & 128 & 0.99\\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{FLOP comparison.} For a variety of different model sizes, we show the ratio of the FLOPs that we compute per sequence to that using the $6ND$ approximation. } \\label{tab:flops} \\end{table*} Compared to the results presented in \\citet{rae2021gopher}, we use a slightly more accurate calculation giving a slightly different value ($6.3 \\times 10^{23}$ compared to $5.76 \\times 10^{23}$).  "
            },
            {
                "section_name": "Other differences between \\chinchilla and \\gopher_13",
                "paragraphs": "\\label{app:other_diffs} Beyond differences in model size and number of training tokens, there are some additional minor differences between \\chinchilla and \\gopher. Specifically, \\gopher was trained with Adam \\citep{kingma2014adam} whereas \\chinchilla was trained with AdamW \\citep{loshchilov2018decoupled}. Furthermore, as discussed in \\textit{Lessons Learned} in \\citet{rae2021gopher}, \\chinchilla stored a higher-precision copy of the weights in the sharded optimiser state.   We show comparisons of models trained with Adam and AdamW in \\autoref{fig:ablate} and \\autoref{fig:adam}. We find that, independent of the learning rate schedule, AdamW trained models outperform models trained with Adam. \\begin{figure*}[h!] \\centering \\includegraphics[width=.9\\textwidth]{figures/ablate_v1.pdf} \\caption{\\textbf{Comparison of other differences.} Using an 680 million parameter model, we show a comparison between the setup used to train \\gopher and \\chinchilla--- the change in optimiser and using a higher precision copy of the weights in the optimiser state. The setup used for \\chinchilla (orange) clearly outperforms the setup used to train \\gopher (green). } \\label{fig:ablate} \\end{figure*} \\begin{figure*}[h!] \\centering \\includegraphics[width=.9\\textwidth]{figures/Adam_AdamW.pdf} \\caption{\\textbf{Adam vs AdamW.} For a 417M (blue) and 1.4B model (green), we find that training with AdamW improves performance over training with Adam. } \\label{fig:adam} \\end{figure*} In \\autoref{fig:ablate} we show a comparison of an 680 million parameter model trained with and without the higher precision copy of the weights and with Adam/AdamW for comparison.  "
            },
            {
                "section_name": "Results_14",
                "paragraphs": ""
            },
            {
                "section_name": "The Pile_11",
                "paragraphs": "In \\autoref{tab:pile_nums} we show the bits-per-byte (bpb) on The Pile \\citep{pile} of \\chinchilla, \\gopher, and Jurassic-1. \\chinchilla outperforms \\gopher on all subsets. Jurassic-1 outperforms \\chinchilla on 2 subsets--- \\texttt{dm\\_mathematics} and \\texttt{ubuntu\\_irc}.   \\begin{table}[h!] \\centering \\begin{tabular}{lrrr} \\toprule Subset &         \\chinchilla (70B) &         \\gopher (280B) &         Jurassic-1 (170B) \\\\ \\midrule pile\\_cc            & \\textbf{0.667} & 0.691 & 0.669 \\\\ pubmed\\_abstracts   & \\textbf{0.559} & 0.578 & 0.587 \\\\ stackexchange      & \\textbf{0.614} & 0.641 & 0.655 \\\\ github             & \\textbf{0.337} & 0.377 & 0.358 \\\\ openwebtext2       & \\textbf{0.647} & 0.677 &   -  \\\\ arxiv              & \\textbf{0.627} & 0.662 & 0.680 \\\\ uspto\\_backgrounds  & \\textbf{0.526} & 0.546 & 0.537 \\\\ freelaw            & \\textbf{0.476} & 0.513 & 0.514 \\\\ pubmed\\_central     & \\textbf{0.504} & 0.525 & 0.579 \\\\ dm\\_mathematics     & 1.111 & 1.142 & \\textbf{1.037} \\\\ hackernews         & \\textbf{0.859} & 0.890 & 0.869 \\\\ nih\\_exporter       & \\textbf{0.572} & 0.590 & 0.590 \\\\ opensubtitles      & \\textbf{0.871} & 0.900 & 0.879 \\\\ europarl           & \\textbf{0.833} & 0.938 & - \\\\ books3             & \\textbf{0.675} & 0.712 & 0.835 \\\\ philpapers         & \\textbf{0.656} & 0.695 & 0.742 \\\\ gutenberg\\_pg\\_19    & \\textbf{0.548} & 0.656 & 0.890 \\\\ bookcorpus2        & \\textbf{0.714} & 0.741 &   -  \\\\ ubuntu\\_irc         & 1.026 & 1.090 & \\textbf{0.857} \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{Bits-per-Byte on The Pile.} We show the bpb on The Pile for \\chinchilla compared to \\gopher and Jurassic-1.} \\label{tab:pile_nums} \\end{table}   "
            },
            {
                "section_name": "MMLU_12",
                "paragraphs": "In \\autoref{tab:mmlu_nums} we show the performance of \\chinchilla and \\gopher on each subset of MMLU.  \\begin{table}[h!] \\centering \\small \\begin{tabular}{lll|lll} \\toprule Task &     \\chinchilla &     \\gopher &                                    Task &     \\chinchilla &     \\gopher \\\\ \\midrule abstract\\_algebra &  31.0 &  25.0 &                              anatomy &  70.4 &  56.3 \\\\ astronomy &  73.0 &  65.8 &                      business\\_ethics &  72.0 &  70.0 \\\\ clinical\\_knowledge &  75.1 &  67.2 &                      college\\_biology &  79.9 &  70.8 \\\\ college\\_chemistry &  51.0 &  45.0 &             college\\_computer\\_science &  51.0 &  49.0 \\\\ college\\_mathematics &  32.0 &  37.0 &                     college\\_medicine &  66.5 &  60.1 \\\\ college\\_physics &  46.1 &  34.3 &                    computer\\_security &  76.0 &  65.0 \\\\ conceptual\\_physics &  67.2 &  49.4 &                         econometrics &  38.6 &  43.0 \\\\ electrical\\_engineering &  62.1 &  60.0 &               elementary\\_mathematics &  41.5 &  33.6 \\\\ formal\\_logic &  33.3 &  35.7 &                         global\\_facts &  39.0 &  38.0 \\\\ high\\_school\\_biology &  80.3 &  71.3 &                high\\_school\\_chemistry &  58.1 &  47.8 \\\\ high\\_school\\_computer\\_science &  58.0 &  54.0 &         high\\_school\\_european\\_history &  78.8 &  72.1 \\\\ high\\_school\\_geography &  86.4 &  76.8 &  high\\_school\\_gov\\_and\\_politics &  91.2 &  83.9 \\\\ high\\_school\\_macroeconomics &  70.5 &  65.1 &              high\\_school\\_mathematics &  31.9 &  23.7 \\\\ high\\_school\\_microeconomics &  77.7 &  66.4 &                  high\\_school\\_physics &  36.4 &  33.8 \\\\ high\\_school\\_psychology &  86.6 &  81.8 &               high\\_school\\_statistics &  58.8 &  50.0 \\\\ high\\_school\\_us\\_history &  83.3 &  78.9 &            high\\_school\\_world\\_history &  85.2 &  75.1 \\\\ human\\_aging &  77.6 &  66.4 &                      human\\_sexuality &  86.3 &  67.2 \\\\ international\\_law &  90.9 &  77.7 &                        jurisprudence &  79.6 &  71.3 \\\\ logical\\_fallacies &  80.4 &  72.4 &                     machine\\_learning &  41.1 &  41.1 \\\\ management &  82.5 &  77.7 &                            marketing &  89.7 &  83.3 \\\\ medical\\_genetics &  69.0 &  69.0 &                        miscellaneous &  84.5 &  75.7 \\\\ moral\\_disputes &  77.5 &  66.8 &                      moral\\_scenarios &  36.5 &  40.2 \\\\ nutrition &  77.1 &  69.9 &                           philosophy &  79.4 &  68.8 \\\\ prehistory &  81.2 &  67.6 &              professional\\_accounting &  52.1 &  44.3 \\\\ professional\\_law &  56.5 &  44.5 &                professional\\_medicine &  75.4 &  64.0 \\\\ professional\\_psychology &  75.7 &  68.1 &                     public\\_relations &  73.6 &  71.8 \\\\ security\\_studies &  75.9 &  64.9 &                            sociology &  91.0 &  84.1 \\\\ us\\_foreign\\_policy &  92.0 &  81.0 &                             virology &  53.6 &  47.0 \\\\ world\\_religions &  87.7 &  84.2 &                                     &    &    \\\\ \\bottomrule \\end{tabular} \\caption{\\textbf{\\chinchilla MMLU results.} For each subset of MMLU \\citep{hendrycks2020measuring}, we show \\chinchilla's accuracy compared to \\gopher. } \\label{tab:mmlu_nums} \\end{table}  "
            },
            {
                "section_name": "Winogender Setup_13",
                "paragraphs": "\\label{appendix-winogender} We follow the same setup as in \\citet{rae2021gopher}.  To test coreference resolution in \\chinchilla, we input a sentence which includes a pronoun reference (e.g., \u201cThe librarian helped the child pick out a book because \\{pronoun\\} liked to encourage reading.\u201d), then measure the probability of the model completing the sentence \u201c\u2018\\{Pronoun\\}\u2019 refers to the\u201d with different sentence roles (\u201clibrarian\u201d and \u201cchild\u201d in this example). Each example is annotated with the correct pronoun resolution (the pronoun corresponds to the librarian in this example).  Each sentence is tested with a female, male, and gender-neutral pronoun. An unbiased model would correctly predict which word the pronoun refers to regardless of pronoun gender.  "
            },
            {
                "section_name": "\\bigbench_14",
                "paragraphs": "In \\autoref{tab:bigbench} we show \\chinchilla and \\gopher performance on each subset of \\bigbench that we consider.  \\begin{table}[ht] \\centering \\small \\begin{tabular}{lll|lll} \\toprule Task &     \\chinchilla &     \\gopher &                                    Task &     \\chinchilla &     \\gopher \\\\ \\midrule hyperbaton &  54.2 &  51.7 &       movie\\_dialog\\_same\\_or\\_diff &  54.5 &  50.7 \\\\ causal\\_judgment &  57.4 &  50.8 &                              winowhy &  62.5 &  56.7 \\\\ formal\\_fallacies\\_syllogisms\\_neg &  52.1 &  50.7 &                 movie\\_recommendation &  75.6 &  50.5 \\\\ crash\\_blossom &  47.6 &  63.6 &                 moral\\_permissibility &  57.3 &  55.1 \\\\ discourse\\_marker\\_prediction &  13.1 &  11.7 &                           strategyqa &  68.3 &  61.0 \\\\ general\\_knowledge\\_json &  94.3 &  93.9 &               nonsense\\_words\\_grammar &  78.0 &  61.4 \\\\ sports\\_understanding &  71.0 &  54.9 &                     metaphor\\_boolean &  93.1 &  59.3 \\\\ implicit\\_relations &  49.4 &  36.4 &                             navigate &  52.6 &  51.1 \\\\ penguins\\_in\\_a\\_table &  48.7 &  40.6 &               presuppositions\\_as\\_nli &  49.9 &  34.0 \\\\ intent\\_recognition &  92.8 &  88.7 &                   temporal\\_sequences &  32.0 &  19.0 \\\\ reasoning\\_about\\_colored\\_objects &  59.7 &  49.2 &                   question\\_selection &  52.6 &  41.4 \\\\ logic\\_grid\\_puzzle &  44.0 &  35.1 &            logical\\_fallacy\\_detection &  72.1 &  58.9 \\\\ timedial &  68.8 &  50.9 &                   physical\\_intuition &  79.0 &  59.7 \\\\ epistemic\\_reasoning &  60.6 &  56.4 &                           physics\\_mc &  65.5 &  50.9 \\\\ ruin\\_names &  47.1 &  38.6 &                identify\\_odd\\_metaphor &  68.8 &  38.6 \\\\ hindu\\_knowledge &  91.4 &  80.0 &                 understanding\\_fables &  60.3 &  39.6 \\\\ misconceptions &  65.3 &  61.7 &                     logical\\_sequence &  64.1 &  36.4 \\\\ implicatures &  75.0 &  62.0 &               mathematical\\_induction &  47.3 &  57.6 \\\\ disambiguation\\_q &  54.7 &  45.5 &                    fantasy\\_reasoning &  69.0 &  64.1 \\\\ known\\_unknowns &  65.2 &  63.6 &                               SNARKS &  58.6 &  48.3 \\\\ dark\\_humor\\_detection &  66.2 &  83.1 &                             crass\\_ai &  75.0 &  56.8 \\\\ analogical\\_similarity &  38.1 &  17.2 &                    entailed\\_polarity &  94.0 &  89.5 \\\\ sentence\\_ambiguity &  71.7 &  69.1 &                 irony\\_identification &  73.0 &  69.7 \\\\ riddle\\_sense &  85.7 &  68.2 &  evaluating\\_info\\_essentiality &  17.6 &  16.7 \\\\ date\\_understanding &  52.3 &  44.1 &                   phrase\\_relatedness &  94.0 &  81.8 \\\\ analytic\\_entailment &  67.1 &  53.0 &                       novel\\_concepts &  65.6 &  59.1 \\\\ odd\\_one\\_out &  70.9 &  32.5 &                  empirical\\_judgments &  67.7 &  52.5 \\\\ logical\\_args &  56.2 &  59.1 &           figure\\_of\\_speech\\_detection &  63.3 &  52.7 \\\\ alignment\\_questionnaire &  91.3 &  79.2 &                     english\\_proverbs &  82.4 &  57.6 \\\\ similarities\\_abstraction &  87.0 &  81.8 &  Human\\_organs\\_senses\\_mcc &  85.7 &  84.8 \\\\ anachronisms &  69.1 &  56.4 &            gre\\_reading\\_comprehension &  53.1 &  27.3 \\\\ \\bottomrule \\end{tabular}  \\caption{\\textbf{\\chinchilla \\bigbench results.} For each subset of \\bigbench \\citep{bigbench}, we show \\chinchilla and \\gopher's accuracy. } \\label{tab:bigbench} \\end{table}  "
            },
            {
                "section_name": "Model Card_15",
                "paragraphs": "\\label{appendix:gopher-model-card} We present the \\chinchilla model card in \\autoref{tab:chinchilla-model-card}, following the framework presented by \\citet{mitchell2019model}.  \\begin{center} \\begin{longtable}[ht]{p{0.35\\linewidth} | p{0.6\\linewidth}} \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Model Details}} \\vspace{2mm}\\\\ \\toprule Organization Developing the Model & DeepMind  \\\\ \\midrule Model Date & March 2022 \\\\ \\midrule Model Type & Autoregressive Transformer Language Model  (\\autoref{method:models} for details)  \\\\ \\midrule Feedback on the Model & \\texttt{\\{jordanhoffmann, sborgeaud, amensch,sifre\\}@deepmind.com}\\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Intended Uses}} \\vspace{2mm} \\\\ \\toprule Primary Intended Uses & The primary use is research on language models, including: research on the scaling behaviour of language models along with those listed in \\citet{rae2021gopher}. \\\\ \\midrule Primary Intended Users & DeepMind researchers. We will not make this model available publicly. \\\\ \\midrule Out-of-Scope Uses & Uses of the language model for language generation in harmful or deceitful settings. More generally, the model should not be used for downstream applications without further safety and fairness mitigations. \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Factors}} \\vspace{2mm} \\\\ \\toprule Card Prompts -- Relevant Factor & Relevant factors include which language is used.  Our model is trained on English data. Furthermore, in the analysis of models trained on the same corpus in \\citet{rae2021gopher}, we found it has unequal performance when modelling some dialects (e.g., African American English).  Our model is designed for research. The model should not be used for downstream applications without further analysis on factors in the proposed downstream application. \\\\ \\midrule Card Prompts -- Evaluation Factors & See the results in \\citet{rae2021gopher} which analyzes models trained on the same text corpus. \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Metrics}} \\vspace{2mm} \\\\ \\toprule Model Performance Measures & \\begin{itemize} \\item Perplexity and bits per byte on language modelling datasets \\item Accuracy on completion tasks, reading comprehension, MMLU, \\bigbench and fact checking. \\item Exact match accuracy for question answering. \\item Generation toxicity from Real Toxicity Prompts (RTP) alongside toxicity classification accuracy. \\T \\item Gender and occupation bias.  Test include comparing the probability of generating different gender terms and the Winogender coreference resolution task. \\T \\i\\J \\end{itemize} \\vspace*{\\baselineskip} We principally focus on \\chinchilla's performance compared to \\gopher on text likelihood prediction. \\\\ \\midrule Decision thresholds & N/A \\\\ \\midrule Approaches to Uncertainty and Variability & Due to the costs of training large language models, we did not train \\chinchilla multiple times. However, the breadth of our evaluation on a range of different task types gives a reasonable estimate of the overall performance of the model. Furthermore, the existence of another large model trained on the same dataset (\\gopher) provides a clear point of comparison. \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Evaluation Data}} \\vspace{2mm} \\\\ \\toprule Datasets & \\begin{itemize} \\item Language modelling on LAMBADA, Wikitext103~\\citep{wikitext103}, C4~\\citep{raffel2019exploring}, PG-19~\\citep{rae2020compressive} and the Pile~\\citep{pile}. \\item  Language understanding, real world knowledge, mathematical and logical reasoning on the Massive Multitask Language Understanding (MMLU) benchmark~\\citep{hendrycks2020measuring} and on the \u201cBeyond the Imitation Game Benchmark\u201d (\\bigbench)~\\citep{bigbench}. \\item Question answering (closed book) on Natural Questions~\\citep{naturalquestions} and TriviaQA~\\citep{triviaqa}. \\item Reading comprehension on RACE~\\citep{race} \\item Common sense understanding on HellaSwag~\\citep{hellaswag}, PIQA~\\citep{piqa}, Winogrande~\\citep{winogrande}, SIQA~\\citep{socialiqa}, BoolQ~\\citep{clark2019boolq}, and TruthfulQA~\\citep{truthfulqa}. \\end{itemize} \\\\ \\midrule Motivation & We chose evaluations from \\citet{rae2021gopher} to allow us to most directly compare to \\gopher.\\\\ \\midrule Preprocessing & Input text is tokenized using a SentencePiece tokenizer with a vocabulary of size 32,000. Unlike the tokenizer used for \\gopher, the tokenizer used for \\chinchilla does not perform NFKC normalization. \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Training Data}} \\vspace{2mm} \\\\ \\toprule \\multicolumn{2}{c}{The same dataset is used as in \\citet{rae2021gopher}. Differences in sampling are shown in \\autoref{tab:data_makeup}. } \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Quantitative Analyses}} \\vspace{2mm}\\\\ \\toprule Unitary Results & \\autoref{sec:model_analysis} gives a detailed description of our analysis.  Main take-aways include: \\begin{itemize} \\item  Our model is capable of outputting toxic language as measured by the PerspectiveAPI.  This is particularly true when the model is prompted with toxic prompts. \\item Gender:  Our model emulates stereotypes found in our dataset, with occupations such as ``dietician\u201d and \u201creceptionist\u201d being more associated with women and \u201ccarpenter\u201d and \u201csheriff\u201d being more associated with men. \\item Race/religion/country sentiment:  Prompting our model to discuss some groups leads to sentences with lower or higher sentiment, likely reflecting text in our dataset. \\end{itemize} \\\\ \\midrule Intersectional Results & We did not investigate intersectional biases. \\vspace{1mm} \\\\  \\toprule \\noalign{\\vskip 2mm} \\multicolumn{2}{c}{\\textbf{Ethical Considerations}} \\vspace{2mm}\\\\ \\toprule Data & The data is the same as described in \\citet{rae2021gopher}. \\\\ \\midrule Human Life & The model is not intended to inform decisions about matters central to human life or flourishing. \\\\ \\midrule Mitigations & We considered filtering the dataset to remove toxic content but decided against it due to the observation that this can introduce new biases as studied by \\citet{welbl2021challenges}. More work is needed on mitigation approaches to toxic content and other types of risks associated with language models, such as those discussed in \\citet{weidinger2021harms}. \\\\ \\midrule Risks and Harms & The data is collected from the internet, and thus undoubtedly there is toxic/biased content in our training dataset. Furthermore, it is likely that personal information is also in the dataset that has been used to train our models. We defer to the more detailed discussion in \\citet{weidinger2021harms}. \\\\ \\midrule Use Cases & Especially fraught use cases include the generation of factually incorrect information with the intent of distributing it or using the model to generate racist, sexist or otherwise toxic text with harmful intent. Many more use cases that could cause harm exist. Such applications to malicious use are discussed in detail in \\citet{weidinger2021harms}.\\\\  \\bottomrule \\caption{\\textbf{\\chinchilla model card.} We follow the framework presented in \\citet{mitchell2019model}. } \\label{tab:chinchilla-model-card} \\end{longtable} \\end{center}   "
            },
            {
                "section_name": "List of trained models_16",
                "paragraphs": "In \\autoref{tab:all_models} we list the model size and configuration of all models used in this study. Many models have been trained multiple times, for a different number of training steps. \\clearpage \\footnotesize \\begin{longtable}[h!] {r | rrrrr } \\toprule Parameters (million) &  d\\_model &  ffw\\_size &  kv\\_size &  n\\_heads &  n\\_layers \\\\ \\midrule 44 &                   512 &                          2048 &                     64 &                       8 &                        8 \\\\ 57 &                   576 &                          2304 &                     64 &                       9 &                        9 \\\\ 74 &                   640 &                          2560 &                     64 &                      10 &                       10 \\\\ 90 &                   640 &                          2560 &                     64 &                      10 &                       13 \\\\ 106 &                   640 &                          2560 &                     64 &                      10 &                       16 \\\\ 117 &                   768 &                          3072 &                     64 &                      12 &                       12 \\\\ 140 &                   768 &                          3072 &                     64 &                      12 &                       15 \\\\ 163 &                   768 &                          3072 &                     64 &                      12 &                       18 \\\\ 175 &                   896 &                          3584 &                     64 &                      14 &                       14 \\\\ 196 &                   896 &                          3584 &                     64 &                      14 &                       16 \\\\ 217 &                   896 &                          3584 &                     64 &                      14 &                       18 \\\\ 251 &                  1024 &                          4096 &                     64 &                      16 &                       16 \\\\ 278 &                  1024 &                          4096 &                     64 &                      16 &                       18 \\\\ 306 &                  1024 &                          4096 &                     64 &                      16 &                       20 \\\\ 425 &                  1280 &                          5120 &                    128 &                      10 &                       18 \\\\ 489 &                  1280 &                          5120 &                    128 &                      10 &                       21 \\\\ 509 &                  1408 &                          5632 &                    128 &                      11 &                       18 \\\\ 552 &                  1280 &                          5120 &                    128 &                      10 &                       24 \\cr 587 &                  1408 &                          5632 &                    128 &                      11 &                       21 \\cr 632 &                  1536 &                          6144 &                    128 &                      12 &                       19\\cr 664 &                  1408 &                          5632 &                    128 &                      11 &                       24\\cr 724 &                  1536 &                          6144 &                    128 &                      12 &                       22 \\cr 816 &                  1536 &                          6144 &                    128 &                      12 &                       25\\cr 893 &                  1792 &                          7168 &                    128 &                      14 &                       20\\cr 1,018 &                  1792 &                          7168 &                    128 &                      14 &                       23 \\cr 1,143 &                  1792 &                          7168 &                    128 &                      14 &                       26 \\cr 1,266 &                  2048 &                          8192 &                    128 &                      16 &                       22\\cr 1,424 &                  2176 &                          8704 &                    128 &                      17 &                       22 \\cr 1,429 &                  2048 &                          8192 &                    128 &                      16 &                       25  \\cr 1,593 &                  2048 &                          8192 &                    128 &                      16 &                       28  \\cr 1,609 &                  2176 &                          8704 &                    128 &                      17 &                       25 \\cr 1,731 &                  2304 &                          9216 &                    128 &                      18 &                       24 \\cr 1,794 &                  2176 &                          8704 &                    128 &                      17 &                       28\\cr 2,007 &                  2304 &                          9216 &                    128 &                      18 &                       28  \\cr 2,283 &                  2304 &                          9216 &                    128 &                      18 &                       32  \\cr 2,298 &                  2560 &                         10240 &                    128 &                      20 &                       26  \\cr 2,639 &                  2560 &                         10240 &                    128 &                      20 &                       30 \\cr 2,980 &                  2560 &                         10240 &                    128 &                      20 &                       34  \\cr 3,530 &                  2688 &                         10752 &                    128 &                      22 &                       36  \\cr 3,802 &                  2816 &                         11264 &                    128 &                      22 &                       36 \\cr 4,084 &                  2944 &                         11776 &                    128 &                      22 &                       36 \\cr 4,516 &                  3072 &                         12288 &                    128 &                      24 &                       36 \\cr 6,796 &                  3584 &                         14336 &                    128 &                      28 &                       40 \\cr 9,293 &                  4096 &                         16384 &                    128 &                      32 &                       42  \\cr 11,452 &                  4352 &                         17408 &                    128 &                      32 &                       47  \\cr 12,295 &                  4608 &                         18432 &                    128 &                      36 &                       44\\cr 12,569 &                  4608 &                         18432 &                    128 &                      32 &                       47 \\cr 13,735 &                  4864 &                         19456 &                    128 &                      32 &                       47 \\cr 14,940 &                  4992 &                         19968 &                    128 &                      32 &                       49  \\cr 16,183 &                  5120 &                         20480 &                    128 &                      40 &                       47 \\cr \\bottomrule \\caption{\\textbf{All models.} We list the hyperparameters and size of all models trained as part of this work. Many shown models have been trained with multiple learning rate schedules/number of training tokens.} \\label{tab:all_models} \\end{longtable}  \\end{document}  "
            }
        ],
        "figures_and_tables": [
            {
                "file": "./data_postprocessing/2203.15556/approach_3_v2.png",
                "caption": "\\textbf{Parametric fit.} We fit a parametric modelling of the loss $\\hat L(N,D)$ and display contour (\\textbf{left}) and isoFLOP slices (\\textbf{right}). For each isoFLOP slice, we include a corresponding dashed line in the left plot. In the left plot, we show the efficient frontier in blue, which is a line in log-log space. Specifically, the curve goes through each iso-loss contour at the point with the fewest FLOPs. We project the optimal model size given the \\gopher FLOP budget to be 40B parameters.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table1.tex",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table2.tex",
                "caption": "\\textbf{Estimated optimal training FLOPs and training tokens for various model sizes.} For various model sizes, we show the projections from Approach 1 of how many FLOPs and training tokens would be needed to train compute-optimal models. The estimates for Approach 2 \\& 3 are similar (shown in \\autoref{app:estimated_flops_and_tokens_2_and3})",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table3.tex",
                "caption": "\\textbf{All evaluation tasks.} We evaluate \\chinchilla on a collection of language modelling along with downstream tasks. We evaluate on largely the same tasks as in \\citet{rae2021gopher}, to allow for direct comparison.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table4.tex",
                "caption": "\\textbf{Massive Multitask Language Understanding (MMLU).} We report the average 5-shot accuracy over 57 tasks with model and human accuracy comparisons taken from \\citet{hendrycks2020measuring}. We also include the average prediction for state of the art accuracy in June 2022/2023 made by 73 competitive human forecasters in \\citet{forecast_blog}. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table5.tex",
                "caption": "\\textbf{Reading comprehension.} On RACE-h and RACE-m \\citep{race}, \\chinchilla considerably improves performance over \\gopher. Note that GPT-3 and \\mtnlg use a different prompt format than we do on RACE-h/m, so results are not comparable to \\gopher and \\chinchilla. On LAMBADA \\citep{paperno2016lambada}, \\chinchilla outperforms both \\gopher and \\mtnlg.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table6.tex",
                "caption": "\\textbf{Zero-shot comparison on Common Sense benchmarks.} We show a comparison between \\chinchilla, \\gopher, and \\mtnlg on various Common Sense benchmarks. We see that \\chinchilla matches or outperforms \\gopher and GPT-3 on all tasks. On all but one \\chinchilla outperforms the much larger \\mtnlg model. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table7.tex",
                "caption": "\\textbf{Closed-book question answering.} For Natural Questions \\citep{naturalquestions} and TriviaQA \\citep{triviaqa}, \\chinchilla outperforms \\gopher in all cases. On Natural Questions, \\chinchilla outperforms GPT-3. On TriviaQA we show results on two different evaluation sets to allow for comparison to GPT-3 and to open book SOTA (FiD + Distillation \\citep{izacard2020distilling}). ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table8.tex",
                "caption": "\\textbf{Winogender results.} \\textbf{Left:} \\chinchilla consistently resolves pronouns better than \\gopher.  \\textbf{Right:} \\chinchilla performs better on examples which contradict gender stereotypes (\\textit{gotcha} examples).  However, difference in performance across groups suggests \\chinchilla exhibits bias.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table9.tex",
                "caption": "\\textbf{Estimated parameter and data scaling with increased training compute on two alternate datasets.} The listed values are the exponents, $a$ and $b$, on the relationship $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$. Using IsoFLOP profiles, we estimate the scaling on two different datasets. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table10.tex",
                "caption": "\\textbf{Estimated optimal training FLOPs and training tokens for various model sizes.} Analogous to \\autoref{tab:compute}, we show the model size/token count projections from Approaches 2 and 3 for various compute budgets.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table11.tex",
                "caption": "\\textbf{Bits-per-Byte on The Pile.} We show the bpb on The Pile for \\chinchilla compared to \\gopher and Jurassic-1.",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table12.tex",
                "caption": "\\textbf{\\chinchilla MMLU results.} For each subset of MMLU \\citep{hendrycks2020measuring}, we show \\chinchilla's accuracy compared to \\gopher. ",
                "description": ""
            },
            {
                "file": "./data_postprocessing/2203.15556/table13.tex",
                "caption": "\\textbf{\\chinchilla \\bigbench results.} For each subset of \\bigbench \\citep{bigbench}, we show \\chinchilla and \\gopher's accuracy. ",
                "description": ""
            }
        ]
    }
}